<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Hadoop升级 | Hexo</title><meta name="author" content="CJ"><meta name="copyright" content="CJ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="五、部分框架升级5.1 Hadoop 3.1.35.1.1 安装安装方式同旧版本 5.1.2 配置文件core-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;configuration&gt;    &lt;!-- 指定NameNode地址 --&gt;">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop升级">
<meta property="og:url" content="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/Hadoop%E5%8D%87%E7%BA%A7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="五、部分框架升级5.1 Hadoop 3.1.35.1.1 安装安装方式同旧版本 5.1.2 配置文件core-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;configuration&gt;    &lt;!-- 指定NameNode地址 --&gt;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-05-06T05:31:21.063Z">
<meta property="article:modified_time" content="2023-05-06T05:31:21.063Z">
<meta property="article:author" content="CJ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/Hadoop%E5%8D%87%E7%BA%A7/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop升级',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-06 13:31:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Hexo"><span class="site-name">Hexo</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop升级</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-06T05:31:21.063Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-06T05:31:21.063Z" title="更新于 2023-05-06 13:31:21">2023-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/">大数据离线</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop升级"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="五、部分框架升级"><a href="#五、部分框架升级" class="headerlink" title="五、部分框架升级"></a>五、部分框架升级</h1><h2 id="5-1-Hadoop-3-1-3"><a href="#5-1-Hadoop-3-1-3" class="headerlink" title="5.1 Hadoop 3.1.3"></a>5.1 Hadoop 3.1.3</h2><h3 id="5-1-1-安装"><a href="#5-1-1-安装" class="headerlink" title="5.1.1 安装"></a>5.1.1 安装</h3><p>安装方式同旧版本</p>
<h3 id="5-1-2-配置文件"><a href="#5-1-2-配置文件" class="headerlink" title="5.1.2 配置文件"></a>5.1.2 配置文件</h3><p>core-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定NameNode地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://bigdata1:9820&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定hadoop数据的存储目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/module/hadoop-3.1.3/data&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 配置HDFS网页登陆使用的静态用户为hxr --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hxr&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 配置hxr允许通过代理访问主机节点 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.hxr.hosts&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 配置hxr允许通过代理用户所属组 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.proxyuser.hxr.groups&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;</span><br><span class="line">        org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">        org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">        &lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>集群支持lzo压缩，还需要将lzo包<strong>hadoop-lzo-0.4.20.jar</strong>放到路径**&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;share&#x2F;hadoop&#x2F;common**下，分发到其他节点后重启hadoop集群。</p>
</blockquote>
<p>hdfs-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- nn web端访问地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 2nn web端访问地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata3:9868&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 挂载目录配置 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/data,file:///dfs/data1,file:///home/dfs/data2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 测试环境指定HDFS副本数量 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果是新添加的硬盘，为了防止原有的数据丢失，需要将原来的数据也进行挂载，默认路径是<code>file://$&#123;hadoop.tmp.dir&#125;/dfs/data</code>。<br>启动时，所有挂载的路径下，不能存在一个以上的current文件夹，否则会导致错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/dfs/data2 is in an inconsistent state: Root /home/dfs/data2: DatanodeUuid=a6602160-4a1b-445a-946c-e525affe6b42, does not match 13905291-f3df-41f3-a4ed-c7c2b5a5c63d from other StorageDirectory.</span><br></pre></td></tr></table></figure>
</blockquote>
<p>yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定MR走shuffle --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定ResourceManager的地址--&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;512&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 虚拟核数和物理核数乘数 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 该节点上YARN可使用的物理内存总量 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;30720&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    &lt;!-- resourcemanager可以分配给容器的核数 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;16&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 设置日志聚集服务器地址 --&gt;</span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">        &lt;name&gt;yarn.log.server.url&lt;/name&gt;  </span><br><span class="line">        &lt;value&gt;http://bigdata3:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 设置日志保留时间为7天 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>

<p>workers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bigdata1</span><br><span class="line">bigdata2</span><br><span class="line">bigdata3</span><br></pre></td></tr></table></figure>

<p>在&#x2F;etc&#x2F;profile.d&#x2F;env.sh中添加pid存储位置变量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># hdfs pid location</span><br><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_HOME&#125;/pids/hdfs</span><br><span class="line"># jobhistoryserver pid location</span><br><span class="line">export HADOOP_MAPRED_PID_DIR=$&#123;HADOOP_HOME&#125;/pids/mapred</span><br><span class="line"># yarn pid location</span><br><span class="line">export YARN_PID_DIR=$&#123;HADOOP_HOME&#125;/pids/yarn</span><br></pre></td></tr></table></figure>
<p>在log4j.properties中配置hadoop日志输出位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop.log.dir=/opt/module/hadoop-3.1.3/logs</span><br></pre></td></tr></table></figure>

<h3 id="5-1-3-端口"><a href="#5-1-3-端口" class="headerlink" title="5.1.3 端口"></a>5.1.3 端口</h3><p>| 组件 | 端口 |<br>| namenode-web | 9870&#x2F;9871 |<br>| namenode通讯端口 | 9820 |<br>| resourcemanager-web | 8088 |<br>| resourcemanager通讯端口 | 8031 |<br>| nodemanager-http端口 | 8042 |</p>
<h3 id="5-1-4-高可用"><a href="#5-1-4-高可用" class="headerlink" title="5.1.4 高可用"></a>5.1.4 高可用</h3><h4 id="5-1-4-1-namenode高可用"><a href="#5-1-4-1-namenode高可用" class="headerlink" title="5.1.4.1 namenode高可用"></a>5.1.4.1 namenode高可用</h4><p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html">Apache Hadoop 3.3.5 – HDFS High Availability Using the Quorum Journal Manager</a></p>
<blockquote>
<p>HA通过配置Active&#x2F;Standby两个NameNode实现在集群中对NameNode的热备份来解决上述问题。</p>
<ol>
<li>在一个典型的HA集群中，使用两台单独的机器配置为NameNodes。在任何时间点，确保NameNodes中只有一个处于Active状态，其他的处在Standby状态。其中Active对外提供服务，负责集群中的所有客户端操作，Standby仅仅充当备机，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。</li>
<li>为了能够实时同步Active和Standby两个NameNode的元数据信息（editlog），需提供一个SharedStorage（共享存储系统），可以是NFS或QJM，Active将数据写入共享存储系统，而Standby监听该系统，一旦发现有日志变更，则读取这些数据并加载到自己内存中，以保证自己内存状态与Active保持一致，则在当failover(失效转移)发生时standby便可快速切为active。</li>
<li>为了实现快速切换，Standby节点获取集群中blocks的最新位置是非常必要的。为了实现这一目标，DataNodes上需要同时配置这两个Namenode的地址，并同时给他们发送文件块信息以及心跳检测。</li>
<li>为了实现热备，增加FailoverController（故障转移控制器）和Zookeeper，FailoverController与Zookeeper通信，通过Zookeeper选举机制，FailoverController通过RPC让NameNode转换为Active或Standby。</li>
</ol>
<p>【备注】standby代替SNN的功能（edits—–fsimage）<br>启动了hadoop2.0的HA机制之后，secondarynamenode，checkpointnode，buckcupnode这些都不需要了。</p>
</blockquote>
<blockquote>
<p>Hadoop 高可用性有两种实现方式：QJM 和 NFS。QJM 通过多个 JournalNode 保证了系统的可靠性，消耗的资源很少，不需要额外的机器来启动 JournalNode 进程，只需从 Hadoop 集群中选择几个节点启动 JournalNode 即可。NFS 则是通过共享存储来实现数据共享，但是 NFS 存在单点故障问题，不如 QJM 可靠。这里使用QJM 方式实现HA。</p>
</blockquote>
<p>hdfs-site.xml添加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">    &lt;!-- NameNode高可用 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfscluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.namenodes.hdfscluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;nn1,nn2,nn3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.hdfscluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.hdfscluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.rpc-address.hdfscluster.nn3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:8020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.hdfscluster.nn1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.hdfscluster.nn2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.http-address.hdfscluster.nn3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:9870&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;qjournal://cos-bigdata-test-flink-01:8485;cos-bigdata-test-flink-02:8485;cos-bigdata-test-flink-03:8485/hdfscluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.client.failover.proxy.provider.hdfscluster&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;shell(/bin/true)&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;!--</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;sshfence&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/opt/module/hadoop/journal&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt; </span><br></pre></td></tr></table></figure>
<p>在core-site.xml中配置zk</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;cos-bigdata-test-flink-01:2181,cos-bigdata-test-flink-02:2181,cos-bigdata-test-flink-03:2181&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hdfscluster&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>在hadoop-env.sh中配置用户</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HDFS_JOURNALNODE_USER=root</span><br><span class="line">export HDFS_ZKFC_USER=root</span><br></pre></td></tr></table></figure>

<p>启动每个节点上的journalnode服务 <code>hdfs --daemon start journalnode</code><br>运行命令清空zookeeper中namenode的注册信息 <code>hdfs zkfc -formatZK</code><br>将所有journalnode启动后，在其中一台namenode下执行命令 <code>hdfs namenode -initializeSharedEdits</code><br>然后启动服务<code>start-dfs.sh</code><br>此时配置的另外两台的namenode会报错启动失败，执行命令 <code>hdfs namenode -bootstrapStandby</code><br>然后重启集群 <code>stop-dfs.sh</code>  <code>start-dfs.sh</code><br>最后检查各个namenode的状态 <code>bin/hdfs haadmin -getServiceState nn1/nn2/nn2</code></p>
<p>相比namenode单节点，多了两个进程</p>
<table>
<thead>
<tr>
<th>进程</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>JournalNode</td>
<td>用于存储namenode的数据</td>
</tr>
<tr>
<td>DFSZKFailoverController</td>
<td>用于在发生namenode宕机是进行自动故障转移</td>
</tr>
</tbody></table>
<h4 id="5-1-4-2-resourcemanger高可用"><a href="#5-1-4-2-resourcemanger高可用" class="headerlink" title="5.1.4.2 resourcemanger高可用"></a>5.1.4.2 resourcemanger高可用</h4><p>yarn-site.xml中添加配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">    &lt;!-- Yarn高可用 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarncluster&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;rm1,rm2,rm3&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname.rm3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.webapp.address.rm3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:8088&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.address.rm3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:8032&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定 AM 向 rm1 申请资源的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address.rm1&lt;/name&gt; </span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address.rm2&lt;/name&gt; </span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.scheduler.address.rm3&lt;/name&gt; </span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:8030&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 指定供 NM 连接的地址 --&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm1&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm2&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-02:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.resource-tracker.address.rm3&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-03:8031&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 启用自动恢复 --&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群 --&gt; </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; </span><br><span class="line">        &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!-- 环境变量的继承 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;!--     &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.client.failover-proxy-provider&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt; --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.zk.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;cos-bigdata-test-flink-01:2181,cos-bigdata-test-flink-02:2181,cos-bigdata-test-flink-03:2181&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>启动yarn <code>start-yarn.sh</code>，访问任何一个resourcemanager都会重定向到activate的那个resourcemanager上。<br>查看resourcemanager状态 <code>yarn rmadmin -getServiceState rm1/rm2</code></p>
<br>
**启动Flink-on-yarn任务**
因为namenode和resourcemanager都是集群，Flink-on-yarn任务无法指定其中一个节点的地址。此时可以将hadoop的配置文件引入，在flink-conf.yml中添加配置；如果配置了flink-ha，则high-availability.storageDir也需要修改为namenode集群地址。
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fs.hdfs.hadoopconf: $HADOOP_HOME/etc/hadoop</span><br><span class="line"></span><br><span class="line">yarn.application-attempts: 2</span><br><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.storageDir: hdfs:///flink/yarn/ha</span><br><span class="line">high-availability.zookeeper.quorum: bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br><span class="line">high-availability.zookeeper.path.root: /flink-yarn</span><br></pre></td></tr></table></figure>
可以将hadoop集群的地址和端口省略，例如
`./flink run -s hdfs:///flink/checkpoint/msas/msas_device_exceptions/0bbbf051f47e795067032e7aa612aecd/chk-9884/_metadata --allowNonRestoredState -m yarn-cluster -ynm ADS_DEVICE_MSAS_EXCEPTIONS_test -p 2 -ys 2 -yjm 1024 -ytm 2048m -d -c com.iotmars.compass.MsasDeviceExceptionsApp -yqu default -yD metrics.reporter.promgateway.class=org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter -yD metrics.reporter.promgateway.host=192.168.101.174 -yD metrics.reporter.promgateway.port=9091 -yD metrics.reporter.promgateway.jobName=flink-metrics- -yD metrics.reporter.promgateway.randomJobNameSuffix=true -yD metrics.reporter.promgateway.deleteOnShutdown=false -yD metrics.reporter.promgateway.groupingKey="instance=ADS_DEVICE_MSAS_EXCEPTIONS_test" /opt/jar/test/ADS_DEVICE_MSAS_EXCEPTIONS-1.0-SNAPSHOT.jar`

<blockquote>
<p>启动时namenode和resourcemanager会进行故障转移，知道找到活跃的节点。在访问日志时，historyserver也会进行故障转移，找到可用的namenode节点。</p>
</blockquote>
<br>

<h2 id="5-2-Hive-3-1-2"><a href="#5-2-Hive-3-1-2" class="headerlink" title="5.2 Hive 3.1.2"></a>5.2 Hive 3.1.2</h2><ol>
<li><p>解压</p>
</li>
<li><p>在lib中放入mysql驱动 mysql-connector-java-5.1.45.jar</p>
</li>
<li><p>conf中创建hive-site.xml配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 数据库配置 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://192.168.101.174:3306/metastore?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;Password@123&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 内部表元数据存储路径 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- Hive元数据存储版本的验证 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 元数据授权存储 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 显示查询头信息和当前数据库 --&gt;    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里没有配置hive.metastore.uris参数，所以metastore实际没有使用到，启动hiveserver2时不需要启动metastore，即hiveserver2直连数据库而不是通过metastore进行连接。实际使用中，直连mysql的任务过多会导致获取原数据失败，所以推荐使用metastore进行连接。</p>
</blockquote>
</li>
<li><p>修改&#x2F;opt&#x2F;module&#x2F;hive3.1.2&#x2F;conf&#x2F;hive-log4j.properties.template文件名称为hive-log4j.properties，修改 log 存放位置hive.log.dir&#x3D;&#x2F;opt&#x2F;module&#x2F;hive3.1.2&#x2F;logs</p>
</li>
<li><p>创建数据库metastore，执行命令<code>schematool -initSchema -dbType mysql -verbose</code>，在数据库中创建表。</p>
</li>
<li><p>修改&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh，添加环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#HIVE_HOME</span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br></pre></td></tr></table></figure></li>
<li><p>运行bin&#x2F;hive进入本地客户端。<br>  如果需要通过beeline进行远程连接，需要启动metastore服务和hiveserver2服务。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup ./hive --service metastore &amp;</span><br><span class="line">nohup ./hive --service hiveserver2 &amp;</span><br></pre></td></tr></table></figure>
<p>登陆用户在hadoop配置文件core-site.xml中配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置HDFS网页登陆使用的静态用户为hxr --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hxr&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置hxr允许通过代理访问主机节点 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.hxr.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<br>
## 5.2 Tez 0.10.1
因为tez-0.9.x版本过低，不能和hive3.1.2搭配使用，所以使用tez-0.10.1版本。

<p>下载<a href="apache.org">Tez安装包</a>](<a target="_blank" rel="noopener" href="https://dlcdn.apache.org/tez/0.10.1/">https://dlcdn.apache.org/tez/0.10.1/</a>)<br>和<a target="_blank" rel="noopener" href="https://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/0.10.1/">Tez-UI.war包</a></p>
<h3 id="5-2-1-Tez安装"><a href="#5-2-1-Tez安装" class="headerlink" title="5.2.1 Tez安装"></a>5.2.1 Tez安装</h3><ol>
<li><p>解压安装包到指定目录</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar –zxvf apache-tez-0.10.1-bin.tar.gz -C /opt/module/</span><br></pre></td></tr></table></figure>
<p>重命名解压后的文件为tez-0.10.1。</p>
</li>
<li><p>需要在hive的配置文件hive-env.sh中引入tez的所有jar包：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">export TEZ_HOME=/opt/module/tez-0.10.1</span><br><span class="line">export TEZ_JARS=&quot;&quot;</span><br><span class="line">for jar in `ls $TEZ_HOME | grep jar`;do</span><br><span class="line">        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for jar in `ls $TEZ_HOME/lib`;do</span><br><span class="line">        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">export ATLAS_HOME=/opt/module/atlas-2.1.0</span><br><span class="line">export ATLAS_JARS=&quot;&quot;</span><br><span class="line">for jar in `ls $ATLAS_HOME/hook/hive | grep jar`;do</span><br><span class="line">        export ATLAS_JARS=$ATLAS_JARS:$ATLAS_HOME/hook/hive/$jar</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for jar in `ls $ATLAS_HOME/hook/hive/atlas-hive-plugin-impl`;do</span><br><span class="line">        export ATLAS_JARS=$ATLAS_JARS:$ATLAS_HOME/hook/hive/atlas-hive-plugin-impl/$jar</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">export HIVE_AUX_JARS_PATH=$HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS$ATLAS_JARS</span><br></pre></td></tr></table></figure>
<blockquote>
<p>这里因为使用了Altas的钩子，需要添加相关的包。所以将JARS包加入到hive的读取路径中</p>
</blockquote>
</li>
<li><p>在Hive的&#x2F;opt&#x2F;module&#x2F;hive&#x2F;conf下面创建一个tez-site.xml文件，添加如下内容</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span> encoding=<span class="string">&quot;UTF-8&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xs1&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;fs.defaultFS&#125;/tez/apache-tez-0.10.1-bin.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.use.cluster.hadoop-libs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.history.logging.service.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.resource.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.container.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.runtime.io.sort.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.runtime.unordered.output.buffer.size-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.container.reuse.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.am.speculation.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.tez.auto.reducer.parallelism<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>将&#x2F;opt&#x2F;module&#x2F;tez-0.10.1上传到HDFS的&#x2F;tez路径，使所有的hdfs节点都可以使用tez。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /tez</span><br><span class="line">hadoop fs -put /opt/software/apache-tez-0.10.1-bin.tar.gz /tez</span><br><span class="line">hadoop fs -ls /tez</span><br></pre></td></tr></table></figure>
<p>放置的路径需要与tez-site.xml中配置的路径对应</p>
</li>
</ol>
<h3 id="5-2-2-Tez-UI配置"><a href="#5-2-2-Tez-UI配置" class="headerlink" title="5.2.2 Tez-UI配置"></a>5.2.2 Tez-UI配置</h3><p><a target="_blank" rel="noopener" href="https://tez.apache.org/tez-ui.html">官方配置</a></p>
<ol>
<li><p>安装Tomcat，将webapps文件挂载到宿主机</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name tomcat --privileged=true --restart=always -p 8080:8080 -v /root/docker/tomcat/conf:/usr/local/tomcat/conf -v /root/docker/tomcat/logs:/usr/local/tomcat/logs -v /root/docker/tomcat/webapps:/usr/local/tomcat/webapps -d tomcat:8.5.78-jre8-temurin-focal</span><br></pre></td></tr></table></figure></li>
<li><p>在宿主机webapps文件夹下创建tez-ui文件夹，将下载的tez-ui-0.10.1.war解压到tez-ui目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip tez-ui-0.10.1.war</span><br></pre></td></tr></table></figure>
<p>并编辑配置文件config&#x2F;configs.js</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">ENV = &#123;</span><br><span class="line">  hosts: &#123;</span><br><span class="line">    /*</span><br><span class="line">     * Timeline Server Address:</span><br><span class="line">     * By default TEZ UI looks for timeline server at http://localhost:8188, uncomment and change</span><br><span class="line">     * the following value for pointing to a different address.</span><br><span class="line">     */</span><br><span class="line">    timeline: &quot;http://192.168.101.185:8188&quot;,</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * Resource Manager Address:</span><br><span class="line">     * By default RM REST APIs are expected to be at http://localhost:8088, uncomment and change</span><br><span class="line">     * the following value to point to a different address.</span><br><span class="line">     */</span><br><span class="line">    rm: &quot;http://192.168.101.185:8088&quot;,</span><br><span class="line"></span><br><span class="line">    /*</span><br><span class="line">     * Resource Manager Web Proxy Address:</span><br><span class="line">     * Optional - By default, value configured as RM host will be taken as proxy address</span><br><span class="line">     * Use this configuration when RM web proxy is configured at a different address than RM.</span><br><span class="line">     */</span><br><span class="line">    //rmProxy: &quot;http://localhost:8088&quot;,</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  /*</span><br><span class="line">   * Time Zone in which dates are displayed in the UI:</span><br><span class="line">   * If not set, local time zone will be used.</span><br><span class="line">   * Refer http://momentjs.com/timezone/docs/ for valid entries.</span><br><span class="line">   */</span><br><span class="line">  //timeZone: &quot;UTC&quot;,</span><br><span class="line"></span><br><span class="line">  /*</span><br><span class="line">   * yarnProtocol:</span><br><span class="line">   * If specified, this protocol would be used to construct node manager log links.</span><br><span class="line">   * Possible values: http, https</span><br><span class="line">   * Default value: If not specified, protocol of hosts.rm will be used</span><br><span class="line">   */</span><br><span class="line">  //yarnProtocol: &quot;&lt;value&gt;&quot;,</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>配置timelineserver<br>  说明：timeline作为具体任务执行过程中的节点,相关数据存储的位置,tez任务的applicationMaster,具体执行任务的container,hiveserver,在执行的过程中都会将tez任务的相关信息通过http接口上报到timeline,timeline对这些数据做持久化,保存起来,再通过http接口将数据暴露给tez-ui供用户查看;</p>
</li>
</ol>
<p>在hadoop配置文件yarn-site.xml中配置 <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html">timeline相关参数</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置tez-ui所需的timeline服务 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Indicate to clients whether Timeline service is enabled or not.</span><br><span class="line">  If enabled, the TimelineClient library used by end-users will post entities</span><br><span class="line">  and events to the Timeline server.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;bigdata2&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The hostname of the Timeline service web application.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.http-cross-origin.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Enables cross-origin support (CORS) for web services where</span><br><span class="line">  cross-origin web response headers are needed. For example, javascript making</span><br><span class="line">  a web services request to the timeline server.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.system-metrics-publisher.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Publish YARN information to Timeline Server&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.generic-application-history.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Indicate to clients whether to query generic application data </span><br><span class="line">  from timeline history-service or not. If not enabled then application data </span><br><span class="line">  is queried only from Resource Manager. Defaults to false.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;bigdata2:10200&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Address for the Timeline server to start the RPC server. </span><br><span class="line">  Defaults to $&#123;yarn.timeline-service.hostname&#125;:10200.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;bigdata2:8188&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The http address of the Timeline service web application. </span><br><span class="line">  Defaults to $&#123;yarn.timeline-service.hostname&#125;:8188.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.webapp.https.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;bigdata2:8190&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The https address of the Timeline service web application. </span><br><span class="line">  Defaults to $&#123;yarn.timeline-service.hostname&#125;:8190.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.handler-thread-count&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Handler thread count to serve the client RPC requests. Defaults to 10.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>rm也需要开启cors</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.cross-origin.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Flag to enable cross-origin (CORS) support in the RM. This flag requires the</span><br><span class="line">  CORS filter initializer to be added to the filter initializers list in core-site.xml.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>如果需要timeline服务启用kerberos认证，配置如下，如无特殊需求可以不配置kerberos。Timeline服务开启kerberos认证，则tomcat也需要<a target="_blank" rel="noopener" href="https://tomcat.apache.org/tomcat-8.5-doc/windows-auth-howto.html">配置kerberos认证</a>，否则报401错误导致读取不到数据。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;kerberos&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Defines authentication used for the timeline server HTTP endpoint. </span><br><span class="line">  Supported values are: simple / kerberos /&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;rm/_HOST@IOTMARS.COM&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The Kerberos principal for the timeline server.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/security/keytab/rm.service.keytab&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The Kerberos keytab for the timeline server. </span><br><span class="line">  Defaults on Unix to to /etc/krb5.keytab.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.principal&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HTTP/_HOST@IOTMARS.COM&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.keytab&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/etc/security/keytab/spnego.service.keytab&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.timeline-service.http-authentication.simple.anonymous.allowed&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Indicates if anonymous requests are allowed by the timeline server when using ‘simple’ authentication.</span><br><span class="line">  Defaults to true.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>


<ol start="4">
<li><p>配置tez-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;tez.tez-ui.history-url.base&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://192.168.101.174:8080/tez-ui&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;description&gt;Enable Tez to use the Timeline Server for History Logging&lt;/description&gt;</span><br><span class="line">  &lt;name&gt;tez.history.logging.service.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>同步配置文件到每个hadoop节点，然后重启yarn服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo -i -u yarn stop-yarn.sh</span><br><span class="line">sudo -i -u yarn start-yarn.sh</span><br></pre></td></tr></table></figure>
<p>重启hiveserver2服务，否则hiveserver2连接的任务不会记录到tez-ui中。</p>
</li>
<li><p>启动&#x2F;关闭timeline服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn --daemon start/stop timelineserver</span><br></pre></td></tr></table></figure></li>
</ol>
<blockquote>
<p><strong>注意：</strong><br><strong>1. 如果同时使用了Hive On Spark引擎，可能会报错如下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: &gt;com/sun/jersey/core/util/FeaturesAndProperties. It appears that the timeline client failed to initiate because an incompatible dependency in classpath. If timeline service is optional to this client, try to work around by setting yarn.timeline-service.enabled to false in client configuration.</span><br></pre></td></tr></table></figure>
<p>原因是Hadoop使用的jersey包的版本低于Spark，Spark中没有yarn所需要的包，导致Spark提交任务到Yarn失败。只需要将hadoop下的三个包&#x2F;share&#x2F;hadoop&#x2F;hdfs&#x2F;lib&#x2F;jersey-core-1.19.jar、&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;lib&#x2F;jersey-client-1.19.jar、&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F;lib&#x2F;jersey-guice-1.19.jar 复制到spark的jars目录下即可。</p>
<p><strong>2. 也可能报错如下</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;com.google.protobuf.ServiceException: java.io.IOException: DestHost:destPort bigdata3:43799 , LocalHost:localPort bigdata1/192.168.101.179:0. Failed on local exception: java.io.IOException: Connection reset by peer</span><br><span class="line">&gt;Caused by: java.io.IOException: DestHost:destPort bigdata3:43799 , LocalHost:localPort bigdata1/192.168.101.179:0. Failed on local exception: java.io.IOException: Connection reset by peer</span><br><span class="line">&gt;Caused by: java.io.IOException: Connection reset by peer</span><br><span class="line">&gt;2022-04-19T16:43:25,448  INFO [f8b8e0df-ccd6-4f08-ab69-225144b91dc6 main] client.TezClient: Failed to retrieve AM Status via proxy</span><br><span class="line">&gt;com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: &quot;bigdata1/192.168.101.179&quot;; destination host is: &quot;bigdata2&quot;:38361; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</span><br><span class="line">&gt;Caused by: java.io.EOFException: End of File Exception between local host is: &quot;bigdata1/192.168.101.179&quot;; destination host is: &quot;bigdata2&quot;:38361; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException</span><br><span class="line">&gt;Caused by: java.io.EOFException]</span><br></pre></td></tr></table></figure>
<p>这些异常不准确，真正异常原因需要查看历史服务器，发现如下异常</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/exc/InputCoercionException</span><br></pre></td></tr></table></figure>
<p>原因是缺失了jackson-core-asl-1.9.13.jar和jackson-core-asl-1.9.13.jar包。因为hive-env.sh文件中加载了Tez-0.10.1目录下的所有jar包，所以只需要将jackson-core-asl-1.9.13.jar和jackson-core-asl-1.9.13.jar放到Tez-0.10.1目录下即可。</p>
</blockquote>
<br>

<h2 id="5-3-Spark-3-0-0"><a href="#5-3-Spark-3-0-0" class="headerlink" title="5.3 Spark 3.0.0"></a>5.3 Spark 3.0.0</h2><p>这里我们使用spark替代tez作为计算引擎，即Hive on Spark。Hive将HQL任务提交为Spark任务交给Spark执行，提交的配置参数可以写在spark-defaults.conf或HQL(set语句)中。</p>
<p><strong>步骤</strong></p>
<ol>
<li>Hive-3.1.2中的Spark版本是2.x.x，所以需要下载hive源码（<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/%EF%BC%89%EF%BC%8C%E5%B0%86Spark%E4%BE%9D%E8%B5%96%E7%89%88%E6%9C%AC%E4%BF%AE%E6%94%B9%E4%B8%BA3.0.0%E5%86%8D%E8%BF%9B%E8%A1%8C%E6%89%93%E5%8C%85%E3%80%82">https://mirrors.tuna.tsinghua.edu.cn/apache/hive/），将Spark依赖版本修改为3.0.0再进行打包。</a></li>
<li>Hive所在节点部署Spark，设置SPARK_HOME全局变量。因为将Hive任务转成Spark任务需要本地Spark的依赖。<br>  解压Spark压缩包到本地目录下，需要注意Spark版本与Hive中Spark依赖版本一致。</li>
</ol>
<ul>
<li>携带hadoop依赖的spark安装包：spark-3.0.0-bin-hadoop3.2.tgz</li>
<li>纯净的spark安装包：spark-3.0.0-bin-without-hadoop.tgz</li>
</ul>
<p>解压spark-3.0.0-bin-hadoop3.2.tgz到hive所在节点，添加SPARK_HOME环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># SPARK_HOME</span><br><span class="line">export SPARK_HOME=/opt/module/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>
<p>source使其生效</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile.d/my_env.sh</span><br></pre></td></tr></table></figure>

<p>在hive的conf目录下创建spark-defaults.conf文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.master                               yarn</span><br><span class="line">spark.eventLog.enabled                   true</span><br><span class="line">spark.eventLog.dir                        hdfs://bigdata1:9820/spark/history</span><br><span class="line">spark.executor.memory                    1g</span><br><span class="line">spark.driver.memory					   1g</span><br></pre></td></tr></table></figure>
<p>在HDFS创建如下路径，用于存储历史日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /spark/history</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>将Spark纯净版上传到HDFS上，Hive任务最终由Spark来执行，Spark任务资源分配由Yarn来调度，该任务有可能被分配到集群的任何一个节点。所以需要将Spark的依赖上传到HDFS集群路径，这样集群中任何一个节点都能获取到。又因为Spark3非纯净版的hive版本是2.3.7，直接使用会有版本冲突。<strong>解压Spark纯净版压缩包，将lib目录下的jar包上穿到hdfs中。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put spark-3.0.0-bin-without-hadoop/jars/* /spark-jars</span><br></pre></td></tr></table></figure>

</li>
<li><p>hive的conf下创建hive-site.xml文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 数据库配置 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://192.168.101.174:3306/metastore?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;Password@123&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 内部表元数据存储路径 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- Hive元数据存储版本的验证 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.thrift.port&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.server2.thrift.bind.host&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 元数据授权存储 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.metastore.event.db.notification.api.auth&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- 显示查询头信息和当前数据库 --&gt;    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.header&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.cli.print.current.db&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- Spark依赖位置（注意：端口号必须和namenode的端口号一致） --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;spark.yarn.jars&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://bigdata1:9820/spark/jars-3.0.0/*&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- Hive执行引擎 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.execution.engine&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;spark&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;!-- Hive和Spark连接超时时间 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hive.spark.client.connect.timeout&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;10000ms&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>第一次提交任务会很慢，因为需要启动spark-session，spark-session会一直存在，直到hive客户端关闭。后台进程为<strong>YarnCoarseGrainedExecutorBackend</strong>。</p>
<p><strong>如果需要通过beeline访问，则需要启动hiveserver2，并指定相关参数</strong><br><code>hiveserver2 --hiveconf hive.execution.engine=spark spark.master=yarn</code><br>或直接在后台运行<br><code>nohup hiveserver2 --hiveconf hive.execution.engine=spark spark.master=yarn 1&gt;/opt/module/hive-3.1.2/logs/hive-on-spark.log 2&gt;/opt/module/hive-3.1.2/logs/hive-on-spark.err &amp;</code></p>
<p>我们在配置文件中不是通过metastore连接数据库，而是直连数据库，所以不需要启动metastore进行(对比hive2.3.6的配置文件，它使用的是metastore连接数据库，所以需要启动metastore)。</p>
<p>进入beeline客户端<br><code>beeline -u jdbc:hive2://192.168.101.179:10000 -n hxr</code><br>每个beeline对应一个SparkContext，而在Spark thriftserver中，多个beeline共享一个SparkContext。</p>
<ol start="5">
<li>hive注释中文乱码解决<ul>
<li>修改mysql注释相关表字段的编码格式为UTF-8<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alter table COLUMNS_V2 modify column comment varchar(256) character set utf8;</span><br><span class="line">alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line">alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br><span class="line">alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8;</span><br><span class="line">alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;</span><br></pre></td></tr></table></figure></li>
<li>修改hive的jdbc连接配置如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;jdbc:mysql://192.168.101.174:3306/metastore?useSSL=false&amp;amp;useUnicode=true&amp;amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
完成后新建的表的中文注释就不会乱码了。</li>
</ul>
</li>
</ol>
<br>
**优化Spark(可选)**
配置Spark shuffle服务
Spark Shuffle服务的配置因Cluster Manager（standalone、Mesos、Yarn）的不同而不同。此处以Yarn作为Cluster Manager。
（1）拷贝`$SPARK_HOME/yarn/spark-3.0.0-yarn-shuffle.jar`到
`$HADOOP_HOME/share/hadoop/yarn/lib`
（2）分发`$HADOOP_HOME/share/hadoop/yarn/lib/yarn/spark-3.0.0-yarn-shuffle.jar`
（3）修改`$HADOOP_HOME/etc/hadoop/yarn-site.xml`文件
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle,spark_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services.spark_shuffle.class&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.spark.network.yarn.YarnShuffleService&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
>NOTE:在启用spark.dynamicAllocation.enabled配置后，需要启动spark.shuffle.service.enabled服务。Spark shuffle服务的作用是管理Executor中的各Task的输出文件，主要是shuffle过程map端的输出文件。由于启用资源动态分配后，Spark会在一个应用未结束前，将已经完成任务，处于空闲状态的Executor关闭。Executor关闭后，其输出的文件，也就无法供其他Executor使用了。需要启用Spark shuffle服务，来管理各Executor输出的文件，这样就能关闭空闲的Executor，而不影响后续的计算任务了。

<p>（4）分发<code>$HADOOP_HOME/etc/hadoop/yarn-site.xml</code>文件<br>（5）重启Yarn</p>
<h2 id="5-4-azkaban-3-84-4"><a href="#5-4-azkaban-3-84-4" class="headerlink" title="5.4 azkaban-3.84.4"></a>5.4 azkaban-3.84.4</h2><p>解压包 azkaban-db-3.84.4.tar.gz、azkaban-web-server-3.84.4.tar.gz、azkaban-web-server-3.84.4.tar.gz 。</p>
<p><strong>数据库创建</strong><br>创建azkaban数据库，执行azkaban-db-3.84.4.tar.gz中的create-all-sql-3.84.4.sql脚本来创建表。</p>
<p><strong>web配置</strong><br>修改配置文件conf&#x2F;azkaban.properties</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=192.168.101.174</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=Password@123</span><br><span class="line">mysql.numconnections=100</span><br><span class="line">...</span><br><span class="line">azkaban.executorselector.filters=StaticRemainingFlowSize,CpuStatus</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>说明：</strong><br>StaticRemainingFlowSize：正在排队的任务数；<br>CpuStatus：CPU占用情况<br>MinimumFreeMemory：内存占用情况。测试环境，必须将MinimumFreeMemory删除掉，否则它会认为集群资源不够，不执行。</p>
</blockquote>
<p><strong>邮箱配置</strong><br>修改azkaban.properties文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># mail settings</span><br><span class="line">mail.sender=18851703029@163.com</span><br><span class="line">mail.host=smtp.163.com</span><br><span class="line">mail.user=18851703029@163.com</span><br><span class="line">mail.password=NUJPIDRSXQWMFVDX</span><br></pre></td></tr></table></figure>

<p>修改配置文件conf&#x2F;azkaban-users.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;azkaban-users&gt;</span><br><span class="line">  &lt;user groups=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; username=&quot;azkaban&quot;/&gt;</span><br><span class="line">  &lt;user password=&quot;metrics&quot; roles=&quot;metrics&quot; username=&quot;metrics&quot;/&gt;</span><br><span class="line">  &lt;!-- 添加新用户admin --&gt;</span><br><span class="line">  &lt;user password=&quot;admin&quot; roles=&quot;metrics,admin&quot; username=&quot;admin&quot;/&gt;</span><br><span class="line"></span><br><span class="line">  &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot;/&gt;</span><br><span class="line">  &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;</span><br><span class="line">&lt;/azkaban-users&gt;</span><br></pre></td></tr></table></figure>

<br>
**exec配置**
修改配置文件conf/azkaban.properties
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#...</span><br><span class="line">default.timezone.id=Asia/Shanghai</span><br><span class="line">#...</span><br><span class="line">azkaban.webserver.url=http://bigdata1:8081</span><br><span class="line"># 直接指定executor启动的端口，防止随机分配端口</span><br><span class="line">executor.port=12321</span><br><span class="line">#...</span><br><span class="line">database.type=mysql</span><br><span class="line">mysql.port=3306</span><br><span class="line">mysql.host=192.168.101.174</span><br><span class="line">mysql.database=azkaban</span><br><span class="line">mysql.user=root</span><br><span class="line">mysql.password=Password@123</span><br><span class="line">mysql.numconnections=100</span><br><span class="line"></span><br><span class="line"># 在最后添加</span><br><span class="line">executor.metric.reports=true</span><br><span class="line">executor.metric.milisecinterval.default=60000</span><br></pre></td></tr></table></figure>
将executor分发到需要部署的节点。

<p><strong>executor需要进行激活才能使用</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -G &quot;bigdata1:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">curl -G &quot;bigdata2:12321/executor?action=activate&quot; &amp;&amp; echo</span><br><span class="line">curl -G &quot;bigdata3:12321/executor?action=activate&quot; &amp;&amp; echo</span><br></pre></td></tr></table></figure>
<!--
curl -G "bigdata3:$(<./executor.port)/executor?action=activate" && echo
-->

<p><strong>启动&#x2F;关闭命令</strong><br>bin&#x2F;start-web.sh<br>bin&#x2F;shutdown-web.sh</p>
<p>bin&#x2F;start-exec.sh<br>bin&#x2F;shutdown-exec.sh</p>
<blockquote>
<p><strong>注意：</strong>azkaban在重启executor之后可能会出现找不到executor的异常，可以通过请求azkaban的web接口来刷新 <code>WEBSERVER_URL:8081/executor?ajax=reloadExecutors</code>，注意请求头需要携带有admin权限用户的Cookie，Cookie格式为***azkaban.warn.message&#x3D;; azkaban.failure.message&#x3D;; azkaban.success.message&#x3D;; JSESSIONID&#x3D;{JSESSIONID}; azkaban.browser.session.id&#x3D;{azkaban.browser.session.id}***。</p>
</blockquote>
<p>详见<a target="_blank" rel="noopener" href="https://azkaban.github.io/azkaban/docs/latest/">官网</a>。</p>
<br>

<h2 id="5-5-DataX-amp-DataX-Web"><a href="#5-5-DataX-amp-DataX-Web" class="headerlink" title="5.5 DataX &amp; DataX-Web"></a>5.5 DataX &amp; DataX-Web</h2><p><a href="ttps://github.com/alibaba/DataX">DataX官网</a><br><a target="_blank" rel="noopener" href="https://github.com/WeiYe-Jing/datax-web/blob/master/doc/datax-web/datax-web-deploy.md">DataX-Web网址</a></p>
<h3 id="5-5-1-Data"><a href="#5-5-1-Data" class="headerlink" title="5.5.1 Data"></a>5.5.1 Data</h3><p>解压压缩包datax.tar.gz，既可直接使用(需要安装python)。</p>
<p><strong>实例</strong><br>从mysql到mysql</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;job&quot;: &#123;</span><br><span class="line">        &quot;setting&quot;: &#123;</span><br><span class="line">            &quot;speed&quot;: &#123;</span><br><span class="line">                 &quot;channel&quot;: 3</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;errorLimit&quot;: &#123;</span><br><span class="line">                &quot;record&quot;: 0,</span><br><span class="line">                &quot;percentage&quot;: 0.02</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;content&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;reader&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;mysqlreader&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;username&quot;: &quot;root&quot;,</span><br><span class="line">                        &quot;password&quot;: &quot;hxr&quot;,</span><br><span class="line">                        &quot;column&quot;: [</span><br><span class="line">                            &quot;cid&quot;,</span><br><span class="line">                            &quot;user_id&quot;,</span><br><span class="line">                            &quot;msg&quot;</span><br><span class="line">                        ],</span><br><span class="line">                        &quot;splitPk&quot;: &quot;cid&quot;,</span><br><span class="line">                        &quot;connection&quot;: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                &quot;table&quot;: [</span><br><span class="line">                                    &quot;course_1&quot;,</span><br><span class="line">                                    &quot;course_2&quot;</span><br><span class="line">                                ],</span><br><span class="line">                                &quot;jdbcUrl&quot;: [</span><br><span class="line">     &quot;jdbc:mysql://192.168.32.244:3306/sphere_test&quot;</span><br><span class="line">                                ]</span><br><span class="line">                            &#125;</span><br><span class="line">                        ]</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;writer&quot;: &#123;</span><br><span class="line">                    &quot;name&quot;: &quot;hdfswriter&quot;,</span><br><span class="line">                    &quot;parameter&quot;: &#123;</span><br><span class="line">                        &quot;defaultFS&quot;: &quot;hdfs://192.168.101.179:9820&quot;,</span><br><span class="line">                        &quot;fileType&quot;: &quot;text&quot;,</span><br><span class="line">                        &quot;path&quot;: &quot;/user/hive/warehouse/course&quot;,</span><br><span class="line">                        &quot;fileName&quot;: &quot;course&quot;,</span><br><span class="line">                        &quot;column&quot;: [</span><br><span class="line">                            &#123;</span><br><span class="line">                                &quot;name&quot;: &quot;id&quot;,</span><br><span class="line">                                &quot;type&quot;: &quot;string&quot;</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                &quot;name&quot;: &quot;userid&quot;,</span><br><span class="line">                                &quot;type&quot;: &quot;string&quot;</span><br><span class="line">                            &#125;,</span><br><span class="line">                            &#123;</span><br><span class="line">                                &quot;name&quot;: &quot;msg&quot;,</span><br><span class="line">                                &quot;type&quot;: &quot;string&quot;</span><br><span class="line">                            &#125;</span><br><span class="line">                        ],</span><br><span class="line">                        &quot;writeMode&quot;: &quot;append&quot;,</span><br><span class="line">                        &quot;fieldDelimiter&quot;: &quot;	&quot;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>具体fileType需要根据建表时表的存储格式进行指定，同时指定存储的压缩格式compress。</p>
</blockquote>
<p>执行命令 <code>python datax.py [任务文件]</code></p>
<p>打印结果如下表示执行成功：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2021-07-24 17:46:42.802 [job-0] INFO  StandAloneJobContainerCommunicator - Total 3 records, 21 bytes | Speed 2B/s, 0 records/s | Error 0 records, 0 bytes |  All Task WaitWriterTime 0.000s |  All Task WaitReaderTime 0.000s | Percentage 100.00%</span><br><span class="line">2021-07-24 17:46:42.803 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-07-24 17:46:31</span><br><span class="line">任务结束时刻                    : 2021-07-24 17:46:42</span><br><span class="line">任务总计耗时                    :                 11s</span><br><span class="line">任务平均流量                    :                2B/s</span><br><span class="line">记录写入速度                    :              0rec/s</span><br><span class="line">读出记录总数                    :                   3</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure>


<h3 id="5-5-2-DataX-Web"><a href="#5-5-2-DataX-Web" class="headerlink" title="5.5.2 DataX-Web"></a>5.5.2 DataX-Web</h3><p>安装过程见 <a target="_blank" rel="noopener" href="https://github.com/WeiYe-Jing/datax-web/blob/master/doc/datax-web/datax-web-deploy.md">官网</a></p>
<p>将datax-executor部署在datax所在的服务器上，datax-admin可以部署在其他服务器上。</p>
<p>在配置文件datax-executor&#x2F;conf&#x2F;application.yml中添加datax.py的路径，并指定admin所在节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">datax:</span><br><span class="line">  job:</span><br><span class="line">    admin:</span><br><span class="line">      ### datax admin address list, such as &quot;http://address&quot; or &quot;http://address01,http://address02&quot;</span><br><span class="line">      #addresses: http://127.0.0.1:8080</span><br><span class="line">      addresses: http://192.168.32.244:$&#123;datax.admin.port&#125;</span><br><span class="line">  ......</span><br><span class="line">pypath: /root/datax/bin/datax.py</span><br></pre></td></tr></table></figure>

<h3 id="5-5-3-Data支持Kerberos和Lzop压缩"><a href="#5-5-3-Data支持Kerberos和Lzop压缩" class="headerlink" title="5.5.3 Data支持Kerberos和Lzop压缩"></a>5.5.3 Data支持Kerberos和Lzop压缩</h3><h4 id="5-5-3-1-说明"><a href="#5-5-3-1-说明" class="headerlink" title="5.5.3.1 说明"></a>5.5.3.1 说明</h4><p>我们修改Datax源码，为了解决一下两件事</p>
<ol>
<li><p>启用Datax的Kerberos进行写入时，会报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2021-11-17 15:58:26,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /192.168.101.177:41208. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection</span><br><span class="line">org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5086 instead of deadbeef from client.</span><br></pre></td></tr></table></figure>
<p>报错是因为配置问题，在服务器端配置了dfs.data.transfer.protection为authentication，而客户端没有配置，可以在脚本里配置hadoopConfig参数，添加<br><code>&quot;dfs.data.transfer.protection&quot;: &quot;authentication&quot;</code>；<br>也可以在Datax源码中进行相应配置。</p>
</li>
<li><p>使得Datax支持Lzop压缩<br>同样需要修改Datax源码，引入Lzop压缩</p>
</li>
<li><p>Datax导入hdfs，null值默认使用空串来表示，而不是hive中的表示null值的默认值’\N\来表示。</p>
</li>
</ol>
<blockquote>
<p><strong>注意：</strong></p>
<ol>
<li>如下修改之后的lzop格式，在导入数据时会出现脏数据，目前还未解决；</li>
<li>对于null值不统一问题，可以在hive建表时指定空串为null值即可； <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;	&#x27; NULL DEFINED AS &#x27;&#x27;</span><br></pre></td></tr></table></figure></li>
</ol>
</blockquote>
<h4 id="5-5-3-2-实现"><a href="#5-5-3-2-实现" class="headerlink" title="5.5.3.2 实现"></a>5.5.3.2 实现</h4><ol>
<li><p>拉取源码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/alibaba/DataX.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加配置：<br>  在HdfsHelper类中的如下位置添加配置<code>hadoopConf.set(&quot;dfs.data.transfer.protection&quot;, &quot;authentication&quot;);</code>；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//是否有Kerberos认证</span><br><span class="line">this.haveKerberos = taskConfig.getBool(Key.HAVE_KERBEROS, false);</span><br><span class="line">if(haveKerberos)&#123;</span><br><span class="line">    this.kerberosKeytabFilePath = taskConfig.getString(Key.KERBEROS_KEYTAB_FILE_PATH);</span><br><span class="line">    this.kerberosPrincipal = taskConfig.getString(Key.KERBEROS_PRINCIPAL);</span><br><span class="line">    hadoopConf.set(HADOOP_SECURITY_AUTHENTICATION_KEY, &quot;kerberos&quot;);</span><br><span class="line"></span><br><span class="line">    // CJ:添加配置如下</span><br><span class="line">    hadoopConf.set(&quot;dfs.data.transfer.protection&quot;, &quot;authentication&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>或在hdfs-site.xml配置文件中添加配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;authentication&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>增加Lzop支持<br>  引入依赖</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.5&lt;/version&gt;</span><br><span class="line">    &lt;exclusions&gt;</span><br><span class="line">        &lt;exclusion&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;</span><br><span class="line">        &lt;/exclusion&gt;</span><br><span class="line">    &lt;/exclusions&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>修改HdfsHelper类中的如下位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public Class&lt;? extends CompressionCodec&gt; getCompressCodec(String compress) &#123;</span><br><span class="line">    Class&lt;? extends CompressionCodec&gt; codecClass = null;</span><br><span class="line">    if (null == compress) &#123;</span><br><span class="line">        codecClass = null;</span><br><span class="line">    &#125; else if (&quot;GZIP&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">        codecClass = org.apache.hadoop.io.compress.GzipCodec.class;</span><br><span class="line">    &#125; else if (&quot;BZIP2&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">        codecClass = org.apache.hadoop.io.compress.BZip2Codec.class;</span><br><span class="line">    &#125; else if (&quot;SNAPPY&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">        //todo 等需求明确后支持 需要用户安装SnappyCodec</span><br><span class="line">        codecClass = org.apache.hadoop.io.compress.SnappyCodec.class;</span><br><span class="line">        // org.apache.hadoop.hive.ql.io.orc.ZlibCodec.class  not public</span><br><span class="line">        //codecClass = org.apache.hadoop.hive.ql.io.orc.ZlibCodec.class;</span><br><span class="line">    &#125; else if (&quot;LZOP&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">        codecClass = org.apache.hadoop.io.compress.LzopCodec.class;</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        throw DataXException.asDataXException(HdfsWriterErrorCode.ILLEGAL_VALUE,</span><br><span class="line">                String.format(&quot;目前不支持您配置的 compress 模式 : [%s]&quot;, compress));</span><br><span class="line">    &#125;</span><br><span class="line">    return codecClass;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//compress check</span><br><span class="line">this.compress = this.writerSliceConfig.getString(Key.COMPRESS, null);</span><br><span class="line">if (fileType.equalsIgnoreCase(&quot;TEXT&quot;)) &#123;</span><br><span class="line">    Set&lt;String&gt; textSupportedCompress = Sets.newHashSet(&quot;GZIP&quot;, &quot;BZIP2&quot;, &quot;LZOP&quot;);</span><br><span class="line">    //用户可能配置的是compress:&quot;&quot;,空字符串,需要将compress设置为null</span><br><span class="line">    if (StringUtils.isBlank(compress)) &#123;</span><br><span class="line">        this.writerSliceConfig.set(Key.COMPRESS, null);</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        compress = compress.toUpperCase().trim();</span><br><span class="line">        if (!textSupportedCompress.contains(compress)) &#123;</span><br><span class="line">            throw DataXException.asDataXException(HdfsWriterErrorCode.ILLEGAL_VALUE,</span><br><span class="line">                    String.format(&quot;目前TEXT FILE仅支持GZIP、BZIP2、LZOP 三种压缩, 不支持您配置的 compress 模式 : [%s]&quot;,</span><br><span class="line">                            compress));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改HdfsWriter类中的如下位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//设置临时文件全路径和最终文件全路径</span><br><span class="line">if (&quot;GZIP&quot;.equalsIgnoreCase(this.compress)) &#123;</span><br><span class="line">    this.tmpFiles.add(fullFileName + &quot;.gz&quot;);</span><br><span class="line">    this.endFiles.add(endFullFileName + &quot;.gz&quot;);</span><br><span class="line">&#125; else if (&quot;BZIP2&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">    this.tmpFiles.add(fullFileName + &quot;.bz2&quot;);</span><br><span class="line">    this.endFiles.add(endFullFileName + &quot;.bz2&quot;);</span><br><span class="line">    // CJ: 新增如下</span><br><span class="line">&#125; else if (&quot;LZOP&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">    this.tmpFiles.add(fullFileName + &quot;.lzo&quot;);</span><br><span class="line">    this.endFiles.add(endFullFileName + &quot;.lzo&quot;);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    this.tmpFiles.add(fullFileName);</span><br><span class="line">    this.endFiles.add(endFullFileName);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>修改HdfsHelper类，当使用LZOP格式的压缩时，存储在hdfs上的null值为”\N”</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// CJ: 添加类属性</span><br><span class="line">public static String nullFormat = null;</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">public void textFileStartWrite(RecordReceiver lineReceiver, Configuration config, String fileName,</span><br><span class="line">                               TaskPluginCollector taskPluginCollector) &#123;</span><br><span class="line">    char fieldDelimiter = config.getChar(Key.FIELD_DELIMITER);</span><br><span class="line">    List&lt;Configuration&gt; columns = config.getListConfiguration(Key.COLUMN);</span><br><span class="line">    String compress = config.getString(Key.COMPRESS, null);</span><br><span class="line"></span><br><span class="line">    // CJ: 为了不影响其他压缩格式，这里设置如果是text格式且LZOP压缩，且添加Null值转换参数，默认是\N。</span><br><span class="line">    if(&quot;LZOP&quot;.equalsIgnoreCase(compress)) &#123;</span><br><span class="line">        nullFormat = config.getString(Key.NULL_FORMAT, Constant.DEFAULT_NULL_FORMAT);</span><br><span class="line">    &#125;</span><br><span class="line">    ......</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">    // 修改</span><br><span class="line">    public static MutablePair&lt;List&lt;Object&gt;, Boolean&gt; transportOneRecord(</span><br><span class="line">            Record record, List&lt;Configuration&gt; columnsConfiguration,</span><br><span class="line">            TaskPluginCollector taskPluginCollector) &#123;</span><br><span class="line">              ......</span><br><span class="line">                    // warn: it&#x27;s all ok if nullFormat is null</span><br><span class="line">//                    recordList.add(null);</span><br><span class="line"></span><br><span class="line">                    // CJ: null值保存为&quot;\N&quot;</span><br><span class="line">                    recordList.add(nullFormat);</span><br><span class="line">              ......</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>打包即可</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mvn -U clean package assembly:assembly -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure></li>
</ol>
<br>
## 5.6 HBase 2.0.5
首先要保证zookeeper和hadoop正常运行

<ol>
<li><p><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/hbase/2.0.5/">下载安装包</a></p>
</li>
<li><p>解压压缩包</p>
</li>
<li><p>修改配置文件<br>  <strong>修改conf&#x2F;hbase-env.sh</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_MANAGES_ZK=false</span><br></pre></td></tr></table></figure>
<p><strong>修改conf&#x2F;hbase-site.xml</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://bigdata1:9820/HBase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;bigdata1,bigdata2,bigdata3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>修改regionservers</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bigdata1</span><br><span class="line">bigdata2</span><br><span class="line">bigdata3</span><br></pre></td></tr></table></figure></li>
<li><p>分发HBase文件到其他节点</p>
</li>
<li><p>启动服务</p>
<ul>
<li>单节点启动<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata1 hbase]$ bin/hbase-daemon.sh start master</span><br><span class="line">[root@bigdata1 hbase]$ bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure></li>
<li>集群启动<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata1 hbase]$ bin/start-hbase.sh</span><br></pre></td></tr></table></figure></li>
</ul>
<blockquote>
<p>如果集群之间的节点时间不同步，会导致regionserver无法启动，抛出ClockOutOfSyncException异常。可通过配置hbase.master.maxclockskew&#x3D;180000修改容忍的时间差异。</p>
</blockquote>
</li>
<li><p>停止服务</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata1 hbase]$ bin/stop-hbase.sh</span><br></pre></td></tr></table></figure></li>
<li><p>查看UI<br>  <a target="_blank" rel="noopener" href="http://bigdata1:16010/">http://bigdata1:16010</a></p>
</li>
</ol>
<br>
## 5.7 Solr 7.7.3
Solr版本需要与Atlas匹配

<ol>
<li><p><a target="_blank" rel="noopener" href="http://archive.apache.org/dist/lucene/solr/7.7.3/">下载安装包</a></p>
</li>
<li><p>解压安装包</p>
</li>
<li><p>修改配置文件<br>  <strong>修改bin&#x2F;solr.in.sh</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ZK_HOST=&quot;bigdata1:2181,bigdata2:2181,bigdata3:2181&quot;</span><br><span class="line">SOLR_HOST=&quot;bigdata1&quot;</span><br><span class="line"># Sets the port Solr binds to, default is 8983</span><br><span class="line">SOLR_PORT=8983</span><br></pre></td></tr></table></figure></li>
<li><p>分发Solr到其他节点<br>  分发完成后，分别对bigdata2、bigdata3主机&#x2F;opt&#x2F;module&#x2F;solr-7.7.3&#x2F;bin下的solr.in.sh文件，修改为SOLR_HOST&#x3D;对应主机名。</p>
</li>
<li><p>系统配置<br>  solr推荐系统允许的最大进程数和最大打开文件数分别为65000和65000，而系统默认值低于推荐值。<br>  修改&#x2F;etc&#x2F;security&#x2F;limits.conf</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* soft nofile 65000</span><br><span class="line">* hard nofile 65000</span><br><span class="line">* soft nproc 65000</span><br></pre></td></tr></table></figure>
</li>
<li><p>集群启动<br>  在三台节点上分别启动Solr，这个就是Cloud模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/solr start</span><br></pre></td></tr></table></figure></li>
<li><p>Web界面<br>  访问8983端口，可指定三台节点中的任意一台IP<br>  <a target="_blank" rel="noopener" href="http://bigdata1:8983/solr/">http://bigdata1:8983/solr/</a></p>
</li>
</ol>
<p>UI界面出现Cloud菜单栏时，Solr的Cloud模式才算部署成功。</p>
<br>
## 5.8 Atlas 2.1.0
图形引擎Graph Engine：在内部，Atlas通过使用图模型管理元数据对象。以实现元数据对象之间的巨大灵活性和丰富的关系。图引擎是负责在类型系统的类型和实体之间进行转换的组件，以及基础图形模型。除了管理图对象之外，图引擎还为元数据对象创建适当的索引，以便有效地搜索它们。

<p>Atlas是通过HBase存储数据，通过Solr进性元数据搜索，所以需要整合HBase和Solr框架。</p>
<h3 id="5-8-1-编译源码"><a href="#5-8-1-编译源码" class="headerlink" title="5.8.1 编译源码"></a>5.8.1 编译源码</h3><ol>
<li><a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.cgi/atlas/2.1.0/apache-atlas-2.1.0-sources.tar.gz">下载源码</a></li>
<li>打包<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export MAVEN_OPTS=&quot;-Xms2g -Xmx2g&quot;</span><br><span class="line">mvn clean install -DskipTests</span><br><span class="line">mvn clean package -Pdist -DskipTests</span><br></pre></td></tr></table></figure></li>
<li>编译完成后在distro&#x2F;target&#x2F;目录下可以找到安装包<blockquote>
<p>jdk版本需要在jdk1.8.0_202及以上，如果失败的查看依赖下载是否完整，多次重试或者更换镜像源。</p>
</blockquote>
</li>
</ol>
<h3 id="5-8-2-安装"><a href="#5-8-2-安装" class="headerlink" title="5.8.2 安装"></a>5.8.2 安装</h3><ol>
<li>将打包完成的安装包放到需要安装的节点上</li>
<li>解压安装包</li>
</ol>
<h3 id="5-8-3-集成HBase"><a href="#5-8-3-集成HBase" class="headerlink" title="5.8.3 集成HBase"></a>5.8.3 集成HBase</h3><ol>
<li>修改配置文件atlas-application.properties<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#修改atlas存储数据主机</span><br><span class="line">atlas.graph.storage.hostname=bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br></pre></td></tr></table></figure></li>
<li>软路由HBase集群的配置文件到 ${ATLAS_HOME}&#x2F;conf&#x2F;hbase 目录下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /opt/module/hbase/conf/ /opt/module/atlas/conf/hbase/</span><br></pre></td></tr></table></figure></li>
<li>在conf&#x2F;atlas-env.sh中添加HBASE_CONF_DIR，指定hbase配置文件的位置<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_CONF_DIR=/opt/module/atlas-2.1.0/conf/hbase/conf</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-8-4-集成Solr"><a href="#5-8-4-集成Solr" class="headerlink" title="5.8.4 集成Solr"></a>5.8.4 集成Solr</h3><ol>
<li>修改配置文件conf&#x2F;atlas-application.properties，修改如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">atlas.graph.index.search.solr.zookeeper-url=bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br></pre></td></tr></table></figure></li>
<li>将conf&#x2F;solr文件夹复制到外部的solr目录中<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp -r /opt/module/atlas-2.1.0/conf/solr /opt/module/solr-7.7.3/</span><br></pre></td></tr></table></figure>
然后将复制过来的文件夹名字从solr改为atlas_conf。<br>最后将该atlas_conf文件分发到所有solr节点。</li>
<li>重启solr，并创建collection<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cos-bigdata-hadoop-01 solr-7.7.3]# sudo -i -u hxr bash -c &quot;cd /opt/module/solr-7.7.3;bin/solr create -c vertex_index -d /opt/module/solr-7.7.3/atlas_conf -shards 3 -replicationFactor 2&quot;</span><br><span class="line"></span><br><span class="line">[root@cos-bigdata-hadoop-01 solr-7.7.3]# sudo -i -u hxr bash -c &quot;cd /opt/module/solr-7.7.3;bin/solr create -c edge_index -d /opt/module/solr-7.7.3/atlas_conf -shards 3 -replicationFactor 2&quot;</span><br><span class="line"></span><br><span class="line">[root@cos-bigdata-hadoop-01 solr-7.7.3]# sudo -i -u hxr bash -c &quot;cd /opt/module/solr-7.7.3;bin/solr create -c fulltext_index -d /opt/module/solr-7.7.3/atlas_conf -shards 3 -replicationFactor 2&quot;</span><br></pre></td></tr></table></figure>
-shards 3：表示该集合分片数为3<br>-replicationFactor 2：表示每个分片数都有2个备份<br>vertex_index、edge_index、fulltext_index：表示集合名称，分别表示点搜索，线搜索和文本搜索。</li>
</ol>
<blockquote>
<p>注意：如果需要删除vertex_index、edge_index、fulltext_index等collection可以执行如下命令。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;[root@cos-bigdata-hadoop-01 solr-7.7.3]# bin/solr delete -c $&#123;collection_name&#125;</span><br></pre></td></tr></table></figure>
</blockquote>
<ol start="4">
<li>登陆web页面，显示如下图表示创建成功</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/21580557-d02b86d3eb6a54cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h3 id="5-8-5-集成Kafka"><a href="#5-8-5-集成Kafka" class="headerlink" title="5.8.5 集成Kafka"></a>5.8.5 集成Kafka</h3><ol>
<li><p>修改配置文件conf&#x2F;atlas-application.properties，修改如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">atlas.notification.embedded=false</span><br><span class="line">atlas.kafka.data=/opt/module/kafka-2.11/logs</span><br><span class="line">atlas.kafka.zookeeper.connect=bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br><span class="line">atlas.kafka.bootstrap.servers=bigdata1:9092,bigdata2:9092,bigdata3:9092</span><br><span class="line">atlas.kafka.zookeeper.session.timeout.ms=4000</span><br><span class="line">atlas.kafka.zookeeper.connection.timeout.ms=2000</span><br><span class="line"></span><br><span class="line">atlas.kafka.enable.auto.commit=true</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Kafka并创建topic</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@cos-bigdata-hadoop-02 kafka-2.11]# bin/kafka-topics.sh --zookeeper bigdata1:2181 --create --topic ATLAS_HOOK --partitions 3 --replication-factor 2</span><br><span class="line"></span><br><span class="line">[root@cos-bigdata-hadoop-02 kafka-2.11]# bin/kafka-topics.sh --zookeeper bigdata1:2181 --create --topic ATLAS_ENTITIES --partitions 3 --replication-factor 2</span><br></pre></td></tr></table></figure></li>
</ol>
<p>ATLAS_HOOK: 来自各个组件的Hook 的元数据通知事件通过写入到名为 ATLAS_HOOK 的 Kafka topic 发送到 Atlas；</p>
<p>ATLAS_ENTITIES：从Atlas 到其他集成组件（如Ranger）的事件写入到名为 ATLAS_ENTITIES 的 Kafka topic；</p>
<h3 id="5-8-6-Atlas其他配置"><a href="#5-8-6-Atlas其他配置" class="headerlink" title="5.8.6 Atlas其他配置"></a>5.8.6 Atlas其他配置</h3><ol>
<li><p>修改配置文件conf&#x2F;atlas-application.properties</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#########  Server Properties  #########</span><br><span class="line">atlas.rest.address=http://bigdata1:21000</span><br><span class="line"># If enabled and set to true, this will run setup steps when the server starts</span><br><span class="line">atlas.server.run.setup.on.start=false</span><br><span class="line"></span><br><span class="line">#########  Entity Audit Configs  #########</span><br><span class="line">atlas.audit.hbase.zookeeper.quorum=bigdata1:2181,bigdata2:2181,bigdata3:2181</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改conf&#x2F;atlas-log4j.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Uncomment the following for perf logs --&gt;</span><br><span class="line">&lt;!-- 去掉如下代码的注释 --&gt;</span><br><span class="line">&lt;appender name=&quot;perf_appender&quot; class=&quot;org.apache.log4j.DailyRollingFileAppender&quot;&gt;</span><br><span class="line">    &lt;param name=&quot;file&quot; value=&quot;$&#123;atlas.log.dir&#125;/atlas_perf.log&quot; /&gt;</span><br><span class="line">    &lt;param name=&quot;datePattern&quot; value=&quot;&#x27;.&#x27;yyyy-MM-dd&quot; /&gt;</span><br><span class="line">    &lt;param name=&quot;append&quot; value=&quot;true&quot; /&gt;</span><br><span class="line">    &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt;</span><br><span class="line">        &lt;param name=&quot;ConversionPattern&quot; value=&quot;%d|%t|%m%n&quot; /&gt;</span><br><span class="line">    &lt;/layout&gt;</span><br><span class="line">&lt;/appender&gt;</span><br><span class="line"></span><br><span class="line">&lt;logger name=&quot;org.apache.atlas.perf&quot; additivity=&quot;false&quot;&gt;</span><br><span class="line">    &lt;level value=&quot;debug&quot; /&gt;</span><br><span class="line">    &lt;appender-ref ref=&quot;perf_appender&quot; /&gt;</span><br><span class="line">&lt;/logger&gt;</span><br></pre></td></tr></table></figure>
<p>打开后会记录性能指标(如查询等操作)。</p>
</li>
</ol>
<h3 id="5-8-7-Kerberos相关配置"><a href="#5-8-7-Kerberos相关配置" class="headerlink" title="5.8.7 Kerberos相关配置"></a>5.8.7 Kerberos相关配置</h3><p>若Hadoop集群开启了Kerberos认证，Atlas与Hadoop集群交互之前需要先进性Kerberos认证。</p>
<ol>
<li>为Atlas创建Kerberos主题，并生成keytab文件<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kadmin -padmin/admin -wadmin -q &quot;addprinc -randkey atlas/bigdata1&quot;</span><br><span class="line">kadmin -padmin/admin -wadmin -q &quot;xst -k /etc/security/keytab/atlas.service.keytab atlas/bigdata1&quot;</span><br></pre></td></tr></table></figure></li>
<li>修改conf&#x2F;atlas-application.properties配置文件<br>  新增如下参数<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atlas.authentication.method=kerberos</span><br><span class="line">atlas.authentication.principal=atlas/bigdata1@IOTMARS.COM</span><br><span class="line">atlas.authentication.keytab=/etc/security/keytab/atlas.service.keytab</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="5-8-8-集成Hive"><a href="#5-8-8-集成Hive" class="headerlink" title="5.8.8 集成Hive"></a>5.8.8 集成Hive</h3><p>Hive有hook借口，可以通过编写实现类将hive改动的元数据信息发送到kafka中，然后再倒入到Atlas中更新hive的元数据。<br>Atlas中已经有hive-hook包实现了这个逻辑功能，只需要将包放到hive的lib目录中。</p>
<ol>
<li><p>解压hive-hook包</p>
</li>
<li><p>配置hive-hook的实现类<br>  首先将解压后文件夹中的hook和hook-bin文件夹放到已经安装的Atlas文件夹根目录下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mv /opt/module/apache-atlas-hive-hook-2.1.0/* /opt/module/atlas-2.1.0/</span><br></pre></td></tr></table></figure>
<p>然后配置hive的配置文件conf&#x2F;hive-env.sh，将hive-hook的jar包引入hive中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将atlas-hive-hook的jar包引入到hive中</span><br><span class="line">export HIVE_AUX_JARS_PATH=/opt/module/atlas-2.1.0/hook/hive</span><br></pre></td></tr></table></figure>
<p>最后修改Hive配置文件，使用Atlas实现类。在hive&#x2F;conf&#x2F;hive-site.xml文件中增加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.post.hooks&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.atlas.hive.hook.HiveHook&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改配置文件conf&#x2F;atlas-application.properties，新增如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">######### Hive Hook Configs #######</span><br><span class="line">atlas.hook.hive.synchronous=false  # hook操作是异步，不会阻塞程序运行</span><br><span class="line">atlas.hook.hive.numRetries=3 # 重试次数</span><br><span class="line">atlas.hook.hive.queueSize=10000 # 存放等待执行的任务的队列的长度</span><br><span class="line">atlas.cluster.name=primary # 集群名称，可以随便起名</span><br></pre></td></tr></table></figure>
</li>
<li><p>将atlas的配置文件atlas-application.properties复制到hive的配置文件夹中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/module/atlas-2.1.0/conf/atlas-application.properties /opt/module/hive-3.1.2/conf/</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="5-8-9-启动Atlas"><a href="#5-8-9-启动Atlas" class="headerlink" title="5.8.9 启动Atlas"></a>5.8.9 启动Atlas</h2><p>启动之前要保证如下框架正常运行</p>
<ul>
<li>hdfs&#x2F;yarn</li>
<li>zookeeper</li>
<li>kafka</li>
<li>hbase</li>
<li>solr</li>
</ul>
<p>启动Atlas</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cos-bigdata-hadoop-01 module]# sudo -i -u hxr bash -c &quot;cd /opt/module/atlas-2.1.0/;bin/atlas_start.py&quot;</span><br></pre></td></tr></table></figure>
<p>关闭Atlas</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@cos-bigdata-hadoop-01 module]# sudo -i -u hxr bash -c &quot;cd /opt/module/atlas-2.1.0/;bin/atlas_stop.py&quot;</span><br></pre></td></tr></table></figure>

<p>查看UI<br><a target="_blank" rel="noopener" href="http://bigdata1:21000/">http://bigdata1:21000</a></p>
<h2 id="5-8-10-将Hive元数据导入Atlas"><a href="#5-8-10-将Hive元数据导入Atlas" class="headerlink" title="5.8.10 将Hive元数据导入Atlas"></a>5.8.10 将Hive元数据导入Atlas</h2><p>将hive添加到系统变量中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_HOME=/opt/module/hive-3.1.2</span><br><span class="line">export PATH=$&#123;PATH&#125;:$&#123;HIVE_HOME&#125;/bin</span><br></pre></td></tr></table></figure>

<p>将Hive元数据导入到Atlas中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hxr@hadoop102 atlas-2.1.0]$ hook-bin/import-hive.sh</span><br></pre></td></tr></table></figure>
<p>输入账号密码为 admin&#x2F;admin</p>
<p>执行import-hive.sh，相当于初始化作用，Altas获取Hive的库&#x2F;表结构（注意！此时并没有获获表与表&#x2F;字段之间的血缘关系，只是获取了表的结构，即 create table语句，此时可以在web ui上面只会看到单个表的名字，location等等信息）</p>
<p>要想正真获取血缘关系，必须配置hive.exec.post.hooks，然后需要把任务重新调度一下，即执行 insert overwrite a select * from b。此时配置的hook 会监听感知到hive表中有更新操作，然后通过Kafka将更新的数据发给Atlas，Atlas会对数据修改，这样在Web Ui 就会看到 a表与b表的血缘关系。</p>
<h3 id="5-8-11-Atlas常用配置（可选）"><a href="#5-8-11-Atlas常用配置（可选）" class="headerlink" title="5.8.11 Atlas常用配置（可选）"></a>5.8.11 Atlas常用配置（可选）</h3><p><strong>① 配置内存</strong><br>如果计划存储数万个元数据对象，建议调整参数值获得最佳的JVM GC性能。以下是常见的服务器端选项<br>1）修改配置文件&#x2F;opt&#x2F;module&#x2F;atlas&#x2F;conf&#x2F;atlas-env.sh</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#设置Atlas内存</span><br><span class="line">export ATLAS_SERVER_OPTS=&quot;-server -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+CMSClassUnloadingEnabled -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+PrintTenuringDistribution -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=dumps/atlas_server.hprof -Xloggc:logs/gc-worker.log -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=1m -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps&quot;</span><br><span class="line"></span><br><span class="line">#建议JDK1.7使用以下配置</span><br><span class="line">export ATLAS_SERVER_HEAP=&quot;-Xms15360m -Xmx15360m -XX:MaxNewSize=3072m -XX:PermSize=100M -XX:MaxPermSize=512m&quot;</span><br><span class="line"></span><br><span class="line">#建议JDK1.8使用以下配置</span><br><span class="line">export ATLAS_SERVER_HEAP=&quot;-Xms15360m -Xmx15360m -XX:MaxNewSize=5120m -XX:MetaspaceSize=100M -XX:MaxMetaspaceSize=512m&quot;</span><br><span class="line"></span><br><span class="line">#如果是Mac OS用户需要配置</span><br><span class="line">export ATLAS_SERVER_OPTS=&quot;-Djava.awt.headless=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=&quot;</span><br></pre></td></tr></table></figure>
<p>参数说明： -XX:SoftRefLRUPolicyMSPerMB 此参数对管理具有许多并发用户的查询繁重工作负载的GC性能特别有用。</p>
<br>
**② 配置用户名密码**
     Atlas支持以下身份验证方法：File、Kerberos协议、LDAP协议
通过修改配置文件atlas-application.properties文件开启或关闭三种验证方法
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">atlas.authentication.method.kerberos=true|false</span><br><span class="line">atlas.authentication.method.ldap=true|false</span><br><span class="line">atlas.authentication.method.file=true|false</span><br></pre></td></tr></table></figure>
如果两个或多个身份证验证方法设置为true，如果较早的方法失败，则身份验证将回退到后一种方法。例如，如果Kerberos身份验证设置为true并且ldap身份验证也设置为true，那么，如果对于没有kerberos principal和keytab的请求，LDAP身份验证将作为后备方案。
本文主要讲解采用文件方式修改用户名和密码设置。其他方式可以参见官网配置即可。
1）打开/opt/module/atlas/conf/users-credentials.properties文件
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim users-credentials.properties</span><br><span class="line"></span><br><span class="line">#username=group::sha256-password</span><br><span class="line">admin=ADMIN::8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918</span><br><span class="line">rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d</span><br></pre></td></tr></table></figure>
admin是用户名称；8c6976e5b5410415bde908bd4dee15dfb167a9c873fc4bb8a81f6f2ab448a918是采用sha256加密的密码，默认密码为admin。

<p>2）例如：修改用户名称为atguigu，密码为atguigu<br>    （1）获取sha256加密的atguigu密码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ echo -n &quot;atguigu&quot;|sha256sum</span><br><span class="line">2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2</span><br></pre></td></tr></table></figure>
<p>（2）修改用户名和密码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 conf]$ vim users-credentials.properties</span><br><span class="line"></span><br><span class="line">#username=group::sha256-password</span><br><span class="line">atguigu=ADMIN::2628be627712c3555d65e0e5f9101dbdd403626e6646b72fdf728a20c5261dc2</span><br><span class="line">rangertagsync=RANGER_TAG_SYNC::e3f67240f5117d1753c940dae9eea772d36ed5fe9bd9c94a300e40413f1afb9d</span><br></pre></td></tr></table></figure>

<p><strong>③整合LDAP实现登陆</strong><br><a target="_blank" rel="noopener" href="http://atlas.apache.org/#/Authentication">官网配置</a></p>
<p>修改配置文件${ATLAS_HOME}conf&#x2F;atlas-application.properties，添加如下配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#### ldap.type= LDAP or AD</span><br><span class="line">atlas.authentication.method.file=true</span><br><span class="line">atlas.authentication.method.file.filename=sys:atlas.home/conf/users-credentials.properties</span><br><span class="line"># 在服务器上查询用户是否存在，不存在则报错</span><br><span class="line">atlas.authentication.method.ldap.ugi-groups=false</span><br><span class="line"></span><br><span class="line">######## LDAP properties #########</span><br><span class="line">atlas.authentication.method.ldap.url=ldap://192.168.101.174:389</span><br><span class="line">atlas.authentication.method.ldap.userDNpattern=uid=&#123;0&#125;,ou=atlas,dc=ldap,dc=chenjie,dc=asia</span><br><span class="line">atlas.authentication.method.ldap.groupSearchBase=ou=atlas,dc=ldap,dc=chenjie,dc=asia</span><br><span class="line">atlas.authentication.method.ldap.groupSearchFilter=member=uid=&#123;0&#125;,ou=atlas,dc=ldap,dc=chenjie,dc=asia</span><br><span class="line">atlas.authentication.method.ldap.groupRoleAttribute=uid</span><br><span class="line">atlas.authentication.method.ldap.base.dn=ou=atlas,dc=ldap,dc=chenjie,dc=asia</span><br><span class="line">atlas.authentication.method.ldap.bind.dn=cn=admin,dc=ldap,dc=chenjie,dc=asia</span><br><span class="line">atlas.authentication.method.ldap.bind.password=bigdata123</span><br><span class="line">atlas.authentication.method.ldap.referral=ignore</span><br><span class="line">atlas.authentication.method.ldap.user.searchfilter=uid=&#123;0&#125;</span><br><span class="line">atlas.authentication.method.ldap.default.role=ROLE_USER</span><br></pre></td></tr></table></figure>

<h3 id="5-8-12-Web使用"><a href="#5-8-12-Web使用" class="headerlink" title="5.8.12 Web使用"></a>5.8.12 Web使用</h3><p>访问地址为<a target="_blank" rel="noopener" href="http://bigdata1:21000/">http://bigdata1:21000</a></p>
<p><strong>①查询数据</strong></p>
<ul>
<li><p>可以通过Search By Type查询hive相关的一些信息，如hive_db(查询Hive数据库元)、hive_process(查询Hive进程，包括建表语句)、hive_table(查询Hive表)、hive_column(查询Hive列)。</p>
</li>
<li><p>也可以直接在Search By Text直接搜索，比如要查询name为ddate的列，那么在Search By Query 填写 where name&#x3D;”ddate”， 其他选项筛选条件写法一样。</p>
</li>
</ul>
<p><strong>②查看血缘关系</strong><br>需要先跑一遍完成的流程，Atlas才能绘制出正确且完整的血缘关系图。</p>
<p>Search By Type中选择hive_column_lineage进行血缘关系搜索。<br>也可以直接查询表或字段，选择Lineage进性查看血缘关系。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">CJ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/Hadoop%E5%8D%87%E7%BA%A7/">http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/Hadoop%E5%8D%87%E7%BA%A7/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Hexo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AC%94%E8%AE%B0/Flink%E5%AE%9E%E6%97%B6%E9%A1%B9%E7%9B%AE/" title="Flink实时项目"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Flink实时项目</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E5%8D%87%E7%BA%A7/" title="大数据框架升级"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">大数据框架升级</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJ</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E9%83%A8%E5%88%86%E6%A1%86%E6%9E%B6%E5%8D%87%E7%BA%A7"><span class="toc-number">1.</span> <span class="toc-text">五、部分框架升级</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Hadoop-3-1-3"><span class="toc-number">1.1.</span> <span class="toc-text">5.1 Hadoop 3.1.3</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-%E5%AE%89%E8%A3%85"><span class="toc-number">1.1.1.</span> <span class="toc-text">5.1.1 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-number">1.1.2.</span> <span class="toc-text">5.1.2 配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-3-%E7%AB%AF%E5%8F%A3"><span class="toc-number">1.1.3.</span> <span class="toc-text">5.1.3 端口</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-4-%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">1.1.4.</span> <span class="toc-text">5.1.4 高可用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-4-1-namenode%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">5.1.4.1 namenode高可用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-4-2-resourcemanger%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">5.1.4.2 resourcemanger高可用</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Hive-3-1-2"><span class="toc-number">1.2.</span> <span class="toc-text">5.2 Hive 3.1.2</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-1-Tez%E5%AE%89%E8%A3%85"><span class="toc-number">1.2.1.</span> <span class="toc-text">5.2.1 Tez安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-2-Tez-UI%E9%85%8D%E7%BD%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">5.2.2 Tez-UI配置</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Spark-3-0-0"><span class="toc-number">1.3.</span> <span class="toc-text">5.3 Spark 3.0.0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-azkaban-3-84-4"><span class="toc-number">1.4.</span> <span class="toc-text">5.4 azkaban-3.84.4</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-DataX-amp-DataX-Web"><span class="toc-number">1.5.</span> <span class="toc-text">5.5 DataX &amp; DataX-Web</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-1-Data"><span class="toc-number">1.5.1.</span> <span class="toc-text">5.5.1 Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-2-DataX-Web"><span class="toc-number">1.5.2.</span> <span class="toc-text">5.5.2 DataX-Web</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-3-Data%E6%94%AF%E6%8C%81Kerberos%E5%92%8CLzop%E5%8E%8B%E7%BC%A9"><span class="toc-number">1.5.3.</span> <span class="toc-text">5.5.3 Data支持Kerberos和Lzop压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-3-1-%E8%AF%B4%E6%98%8E"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">5.5.3.1 说明</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-5-3-2-%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">5.5.3.2 实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-1-%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81"><span class="toc-number">1.5.4.</span> <span class="toc-text">5.8.1 编译源码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-2-%E5%AE%89%E8%A3%85"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.8.2 安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-3-%E9%9B%86%E6%88%90HBase"><span class="toc-number">1.5.6.</span> <span class="toc-text">5.8.3 集成HBase</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-4-%E9%9B%86%E6%88%90Solr"><span class="toc-number">1.5.7.</span> <span class="toc-text">5.8.4 集成Solr</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-5-%E9%9B%86%E6%88%90Kafka"><span class="toc-number">1.5.8.</span> <span class="toc-text">5.8.5 集成Kafka</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-6-Atlas%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.9.</span> <span class="toc-text">5.8.6 Atlas其他配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-7-Kerberos%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.10.</span> <span class="toc-text">5.8.7 Kerberos相关配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-8-%E9%9B%86%E6%88%90Hive"><span class="toc-number">1.5.11.</span> <span class="toc-text">5.8.8 集成Hive</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-8-9-%E5%90%AF%E5%8A%A8Atlas"><span class="toc-number">1.6.</span> <span class="toc-text">5.8.9 启动Atlas</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-8-10-%E5%B0%86Hive%E5%85%83%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5Atlas"><span class="toc-number">1.7.</span> <span class="toc-text">5.8.10 将Hive元数据导入Atlas</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-11-Atlas%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="toc-number">1.7.1.</span> <span class="toc-text">5.8.11 Atlas常用配置（可选）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-8-12-Web%E4%BD%BF%E7%94%A8"><span class="toc-number">1.7.2.</span> <span class="toc-text">5.8.12 Web使用</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/MySQL/%E6%B3%A8%E8%A7%A3@Select%E5%92%8C@Insert/" title="注解@Select和@Insert">注解@Select和@Insert</a><time datetime="2023-05-06T05:48:28.906Z" title="发表于 2023-05-06 13:48:28">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/%E6%B3%A8%E8%A7%A3@EnableAutoConfiguration/" title="注解@EnableAutoConfiguration">注解@EnableAutoConfiguration</a><time datetime="2023-05-06T05:48:06.027Z" title="发表于 2023-05-06 13:48:06">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E6%A1%86%E6%9E%B6/" title="大数据集群监控框架">大数据集群监控框架</a><time datetime="2023-05-06T05:42:56.298Z" title="发表于 2023-05-06 13:42:56">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/HashMap%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98%E5%8F%8AConcurrentHashMap%E5%8E%9F%E7%90%86/" title="HashMap并发问题及ConcurrentHashMap原理">HashMap并发问题及ConcurrentHashMap原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/Stream%E5%8E%9F%E7%90%86/" title="Stream原理">Stream原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By CJ</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>