<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>集群问题2 | Hexo</title><meta name="author" content="CJ"><meta name="copyright" content="CJ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="踩坑一 现象：sqoop导出数据到mysql时，到reduce卡住。 原因：在yarn服务器上查询运行日志后发现，mysql数据库中的字段是not null，但是导出时的数据是null，导致报错无法导出，然后卡在了reduce。 解决：mysql中字段设置可以为null。  踩坑二 现象：sqoop导出数据到mysql时，报错Can&#39;t parse input data: &#39;\N&amp;">
<meta property="og:type" content="article">
<meta property="og:title" content="集群问题2">
<meta property="og:url" content="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="踩坑一 现象：sqoop导出数据到mysql时，到reduce卡住。 原因：在yarn服务器上查询运行日志后发现，mysql数据库中的字段是not null，但是导出时的数据是null，导致报错无法导出，然后卡在了reduce。 解决：mysql中字段设置可以为null。  踩坑二 现象：sqoop导出数据到mysql时，报错Can&#39;t parse input data: &#39;\N&amp;">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-05-06T05:31:21.063Z">
<meta property="article:modified_time" content="2023-05-06T05:31:21.063Z">
<meta property="article:author" content="CJ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '集群问题2',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-06 13:31:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Hexo"><span class="site-name">Hexo</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">集群问题2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-06T05:31:21.063Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-06T05:31:21.063Z" title="更新于 2023-05-06 13:31:21">2023-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/">大数据离线</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="集群问题2"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h3 id="踩坑一"><a href="#踩坑一" class="headerlink" title="踩坑一"></a>踩坑一</h3><ul>
<li>现象：sqoop导出数据到mysql时，到reduce卡住。</li>
<li>原因：在yarn服务器上查询运行日志后发现，mysql数据库中的字段是not null，但是导出时的数据是null，导致报错无法导出，然后卡在了reduce。</li>
<li>解决：mysql中字段设置可以为null。</li>
</ul>
<h3 id="踩坑二"><a href="#踩坑二" class="headerlink" title="踩坑二"></a>踩坑二</h3><ul>
<li>现象：sqoop导出数据到mysql时，报错<code>Can&#39;t parse input data: &#39;\N&#39;</code>。</li>
<li>原因：导入时直接使用分区字段作为mysql中的对应字段，虽然查询时会在最后一列显示分区字段，但实际上hdfs文件上并没有该字段，导致字段数量对不上。</li>
<li>解决：需要在ads表中再手动添加日期字段。</li>
</ul>
<h3 id="踩坑三"><a href="#踩坑三" class="headerlink" title="踩坑三"></a>踩坑三</h3><ul>
<li>现象：使用hive执行hql语句正常，但是使用beeline或datagrip执行复杂语句会报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">2021-06-17T17:50:15,988 ERROR [HiveServer2-Background-Pool: Thread-4814] operation.Operation: Error running hive query: </span><br><span class="line">org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</span><br><span class="line">	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380) ~[hive-service-2.3.6.jar:2.3.6]</span><br><span class="line">	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257) ~[hive-service-2.3.6.jar:2.3.6]</span><br><span class="line">	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) ~[hive-service-2.3.6.jar:2.3.6]</span><br><span class="line">	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348) ~[hive-service-2.3.6.jar:2.3.6]</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144]</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_144]</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) ~[hadoop-common-2.7.2.jar:?]</span><br><span class="line">	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362) ~[hive-service-2.3.6.jar:2.3.6]</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_144]</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_144]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：</li>
</ul>
<h3 id="踩坑四"><a href="#踩坑四" class="headerlink" title="踩坑四"></a>踩坑四</h3><ul>
<li>现象：使用datax导入到hdfs时报错没有权限<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode=&quot;/user/hive/warehouse&quot;:hxr:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure></li>
<li>原因：因为系统用户是root，所以启动python datax.py [配置文件] 时上传的用户是root，而root没有修改权限，导致写入失败。</li>
<li>解决：</li>
</ul>
<ol>
<li>在hdfs的配置文件中，将dfs.permissions.enabled修改为False</li>
<li>执行这样的操作 hadoop fs -chmod 777 &#x2F;user&#x2F;hadoop</li>
<li>修改执行时的登陆用户</li>
<li>上传时指定有权限的用户 HADOOP_USER_NAME&#x3D;hxr</li>
</ol>
<h3 id="踩坑五"><a href="#踩坑五" class="headerlink" title="踩坑五"></a>踩坑五</h3><ul>
<li>现象：HiveServer2报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread &quot;TriggerValidator&quot;</span><br><span class="line">Exception in thread &quot;org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@69feb4d9&quot; java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">AsyncLogger error handling event seq=26839, value=&#x27;[ERROR calling class org.apache.logging.log4j.core.async.RingBufferLogEvent.toString(): java.lang.NullPointerException]&#x27;:</span><br><span class="line">java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">Exception in thread &quot;HiveServer2-Handler-Pool: Thread-4418&quot; java.lang.OutOfMemoryError: Java heap space</span><br></pre></td></tr></table></figure></li>
<li>原因：heapsize太小了</li>
<li>解决：修改hive-env.sh配置文件<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if [ &quot;$SERVICE&quot; = &quot;hiveserver2&quot; ]; then</span><br><span class="line">    echo $HADOOP_OPTS</span><br><span class="line">    export HADOOP_OPTS=&quot;$HADOOP_OPTS -XX:PermSize=512M -XX:MaxPermSize=1024M&quot;</span><br><span class="line">    echo $HADOOP_OPTS</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export HADOOP_HEAPSIZE=4096</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑六"><a href="#踩坑六" class="headerlink" title="踩坑六"></a>踩坑六</h3><ul>
<li>现象：使用Ranger的hive组件后报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User: root is not allowed to impersonate root</span><br></pre></td></tr></table></figure></li>
<li>原因：可能是root用户启动的Ranger Hive Plugin，该插件通过权限判断后最终提交任务时是以root用户提交的，所以hadoop需要允许root用户进行外部访问。</li>
<li>解决：core-site.xml中添加root代理<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置root允许通过代理访问主机节点 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 配置root允许通过代理用户所属组 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑七"><a href="#踩坑七" class="headerlink" title="踩坑七"></a>踩坑七</h3><ul>
<li>现象：hive on spark 报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.util.concurrent.ExecutionException: java.net.UnknownHostException: cos-bigdata-hadoop-01: Name or service not known</span><br></pre></td></tr></table></figure></li>
<li>原因：可能是spark会读取hostname，但是hostname没有配置到DNS，所以无法解析。</li>
<li>解决：配置到hosts文件中<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.101.179 bigdata1 cos-bigdata-hadoop-01</span><br><span class="line">192.168.101.180 bigdata2 cos-bigdata-hadoop-02</span><br><span class="line">192.168.101.181 bigdata3 cos-bigdata-hadoop-03</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑八"><a href="#踩坑八" class="headerlink" title="踩坑八"></a>踩坑八</h3><ul>
<li>现象：Ranger添加hive源操作时，一直卡在Please wait..，并报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction Error Code: 1205 Call: UPDATE x_service SET tag_service = ?, UPDATE_TIME = ?, version = ? WHERE ((id = ?) AND (version = ?)) bind =&gt; [5 parameters bound] Query: UpdateObjectQuery(XXService [id=3])</span><br></pre></td></tr></table></figure></li>
<li>原因：事务执行速度慢，锁等待超时。可以查看表information_schema.innodb_trx、information_schema.innodb_locks、information_schema.innodb_lock_waits<br>查看事务和锁的情况。</li>
<li>解决：未解决</li>
</ul>
<h3 id="踩坑九"><a href="#踩坑九" class="headerlink" title="踩坑九"></a>踩坑九</h3><ul>
<li>现象：load data inpath 导入数据后，select * 查询不到数据，但是select count(*) 查询到数据；</li>
<li>原因：因为文件是text格式的，没有进行压缩，但是表是lzo压缩格式。所以hive表查询不到数据，但是select count(*) 是通过mapred或spark直接对文件进行计算，所以可以得到计算结果。</li>
<li>解决：使用insert into … select … from 进行导入；或对文件使用正确的压缩格式。</li>
</ul>
<p><strong>这里就需要提及如下配置了：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">STORED AS</span><br><span class="line">    INPUTFORMAT &#x27;com.hadoop.mapred.DeprecatedLzoTextInputFormat&#x27;</span><br><span class="line">    OUTPUTFORMAT &#x27;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#x27;</span><br></pre></td></tr></table></figure>
<p>INPUTFORMAT 是读取该表时使用的格式，而OUTPUTFORMAT 是导入该表时使用的格式；需要注意的是，load data inpath 导入时不会使用OUTPUTFORMAT 的配置，只有insert into … select … 导入时会使用OUTPUTFORMAT 配置。<br>所以该表保存的数据是lzo格式的，load data inpath 导入数据的源文件的格式应当是lzo格式，这样导入数据后才能被hive正确的读取。OUTPUTFORMAT 一般不使用，因为这种建表语句创建的表都是通过直接添加文件而不是insert into … select … 来实现数据的插入的。<br>对于flume来说需要设置输出的文件压缩格式为lzo；对于datax来说，不支持lzo格式，需要想其他办法。</p>
<h3 id="踩坑十"><a href="#踩坑十" class="headerlink" title="踩坑十"></a>踩坑十</h3><ul>
<li>现象：sqoop执行命令后卡死在reduce 0%</li>
<li>原因：①可能是mysql中不允许null值，而导入数据中存在null值，导致卡死。②类型不匹配 ③资源不足</li>
<li>解决：需要手动杀死任务 yarn application -kill application_1628043648968_0137，然后解决问题后再执行。</li>
</ul>
<h3 id="踩坑十一："><a href="#踩坑十一：" class="headerlink" title="踩坑十一："></a>踩坑十一：</h3><ul>
<li>现象：hdfs-site.xml中配置硬盘挂载路径，但是启动hdfs时，报错Too many failed volumes。NameNode启动正常，可以查询原数据，但是DataNode启动失败，无法查看文件内容。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/data,file:///dfs/data1,file:///home/dfs/data2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li>原因：hxr用户没有权限访问root用户的硬盘。将目录权限设置为777，并将存储目录用户改为启动hadoop的用户。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">2021-08-31 16:12:35,884 WARN org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker: Exception checking StorageLocation [DISK]file:/dev/mapper/centos_cos-root</span><br><span class="line">EPERM: Operation not permitted</span><br><span class="line">	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmodImpl(Native Method)</span><br><span class="line">	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmod(NativeIO.java:233)</span><br><span class="line">	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:836)</span><br><span class="line">	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508)</span><br><span class="line">	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489)</span><br><span class="line">	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511)</span><br><span class="line">	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:234)</span><br><span class="line">	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)</span><br><span class="line">	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:239)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:52)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)</span><br><span class="line">	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)</span><br><span class="line">	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)</span><br><span class="line">	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></li>
<li>另一个问题：如果挂载新盘时没有挂载原有的路径，那么启动时就会发现，原有数据会丢失；这时再挂载原有路径时，会报错<code>InconsistentFSStateException: Directory /home/dfs/data2 is in an inconsistent state</code></li>
<li>解决：启动时会在新盘下创建新的数据文件，需要将这些文件全部清空后再启动即可。</li>
</ul>
<h3 id="踩坑十二"><a href="#踩坑十二" class="headerlink" title="踩坑十二"></a>踩坑十二</h3><ul>
<li>现象：在hive执行过程中出现错误如下，这个错误在dt&#x3D;’2021-09-01’时正常，在dt&#x3D;’2021-08-31’时出现<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">2021-09-02 13:27:45,360 ERROR scheduler.AsyncEventQueue: Listener ClientListener threw an exception</span><br><span class="line">java.lang.NullPointerException</span><br><span class="line">	at org.apache.hive.spark.client.RemoteDriver$ClientListener.onJobEnd(RemoteDriver.java:491)</span><br><span class="line">	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)</span><br><span class="line">	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)</span><br><span class="line">	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:115)</span><br><span class="line">	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:99)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)</span><br><span class="line">	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)</span><br><span class="line">	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)</span><br><span class="line">	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)</span><br><span class="line">	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)</span><br><span class="line">......</span><br><span class="line">Job failed with java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper cannot be cast to org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerDirectAccess</span><br><span class="line">FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Spark job failed during runtime. Please check stacktrace for the root cause.</span><br></pre></td></tr></table></figure></li>
<li>原因：hive-on-spark模式下提交的任务中，LEFT JOIN语句右表是张空表。</li>
<li>理解：Join操作时，hive-on-spark会将小表转化为hashTable存储到内存中，然后匹配大表，当小表为空表时，hashTable为null，读取hashTable时就会报错NullPointerException。</li>
<li>解决：将hashtable关闭即可 <code>set hive.mapjoin.optimized.hashtable=false;</code></li>
</ul>
<h3 id="踩坑十三"><a href="#踩坑十三" class="headerlink" title="踩坑十三"></a>踩坑十三</h3><ul>
<li>现象：当使用hive-on-spark时，可能出现的现象①在子查询中使用datediff函数时出现空指针异常；②开窗函数中对日期进行排序，指定行不生效，默认为第一行到当前行；</li>
<li>原因：日期使用string进行保存，而不是使用date类型进行保存；</li>
<li>解决：使用date保存日期函数，或者将string类型转换为date类型；</li>
</ul>
<h3 id="踩坑十四"><a href="#踩坑十四" class="headerlink" title="踩坑十四"></a>踩坑十四</h3><ul>
<li>现象：正确的hql语句，使用hive-on-spark启动时却报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[42000][3] Error while processing statement: FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Spark job failed during runtime. Please check stacktrace for the root cause.</span><br></pre></td></tr></table></figure></li>
<li>原因：查看yarn日志，发现报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Driver stacktrace:</span><br><span class="line">Exception in thread &quot;task-result-getter-146&quot; java.lang.NoClassDefFoundError: org/antlr/runtime/tree/CommonTree</span><br><span class="line">......</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.antlr.runtime.tree.CommonTree</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br></pre></td></tr></table></figure>
说明缺少jar包。</li>
<li>解决：对于缺包的问题，当然就是找到对应的jar包，其实org&#x2F;antlr&#x2F;runtime&#x2F;tree&#x2F;CommonTree所在的jar在hive的lib库中包含。就是hive的lib下的antlr-runtime-3.5.2.jar。<br>可以通过命令进行查找该jar包<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name &quot;antlr-runtime-*&quot;</span><br></pre></td></tr></table></figure>
将hive的lib库下的antlr-runtime-3.5.2.jar拷贝到hdfs上的spark的依赖包下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put antlr-runtime-3.5.2.jar /spark/jars-3.0.0</span><br></pre></td></tr></table></figure>
杀死已经存在的spark application，然后重新执行任务<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn application -kill application_1628043648968_xxxx</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑十五"><a href="#踩坑十五" class="headerlink" title="踩坑十五"></a>踩坑十五</h3><ul>
<li>现象：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Column vector class org.apache.hadoop.hive.ql.exec.vector.VoidColumnVector is not supported!</span><br></pre></td></tr></table></figure>
或<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector</span><br></pre></td></tr></table></figure>
或<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.D</span><br></pre></td></tr></table></figure></li>
<li>原因：矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。但是vector强转报错，具体为什么会报错还不得而知。</li>
<li>解决：不使用向量化执行可以解决vector强转报错的问题，set hive.vectorized.execution.enabled&#x3D;false;</li>
</ul>
<p>报错的hql如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">         SELECT 2                                                     as type,</span><br><span class="line">                code                                                  as device_code,</span><br><span class="line">                0                                                     as time_type,</span><br><span class="line">                date_format(&#x27;2021-09-10&#x27;, &#x27;yyyy&#x27;)                     as `time`,</span><br><span class="line">                nvl(tmp_sales.sales_num, 0)                           as sales_num,</span><br><span class="line">--        nvl(tmp_sales.sales_money, 0)     as sales_money,</span><br><span class="line">                nvl(cast(tmp_sales.sales_money as decimal(20, 2)), 0) as sales_money,</span><br><span class="line">                nvl(tmp_sales.send_num, 0)                            as send_num,</span><br><span class="line">                null                                                  as rate,</span><br><span class="line">                current_timestamp                                     as create_time,</span><br><span class="line">                current_timestamp                                     as update_time</span><br><span class="line">         FROM (</span><br><span class="line">                  SELECT code</span><br><span class="line">                  FROM finedb_ads.ba_device_type</span><br><span class="line">                  WHERE dt = &#x27;2021-09-10&#x27;</span><br><span class="line">              ) tmp_device_type</span><br><span class="line">                  LEFT JOIN</span><br><span class="line">              (</span><br><span class="line">                  SELECT device_code,</span><br><span class="line">                         sum(sales_num)   as sales_num,</span><br><span class="line">                         sum(sales_money) as sales_money,</span><br><span class="line">                         sum(send_num)    as send_num</span><br><span class="line">                  FROM finedb_ads.ca_sales</span><br><span class="line">                  WHERE year(sales_date) = year(&#x27;2021-09-10&#x27;)</span><br><span class="line">                    and sales_date &lt;= &#x27;2021-09-10&#x27;</span><br><span class="line">                    and type = 2</span><br><span class="line">                    and dt = &#x27;2021-09-10&#x27;</span><br><span class="line">                  GROUP BY device_code</span><br><span class="line">              ) tmp_sales</span><br><span class="line">              ON tmp_device_type.code = tmp_sales.device_code;</span><br></pre></td></tr></table></figure>




<h3 id="踩坑十七"><a href="#踩坑十七" class="headerlink" title="踩坑十七"></a>踩坑十七</h3><ul>
<li>现象：hiveserver2拒绝连接，10000端口关闭，报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;HiveServer2-Handler-Pool: Thread-308&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded</span><br><span class="line">	at java.nio.HeapCharBuffer.&lt;init&gt;(HeapCharBuffer.java:57)</span><br><span class="line">        ......</span><br></pre></td></tr></table></figure></li>
<li>原因：hiveserver2的进程内存溢出。<br> （1）主要是由于jdbc连接hiveserver2的并发量非常高，而且某些sql任务会严重消耗hiveserver2的内存，导致hiveserver2压力巨大，比如：reducetask获取mapOutputStatus的时候。这种情况是由于hiveserver2自身的复杂压力大，内存损耗严重，严重GC进而导致hiveserver2故障。这种故障对应于上面介绍的“故障现象1”，通过jdbc无法正常连接到hiveserver2。为了解决该故障，可以通过优化内存GC可以缓解hiveserver2的GC卡死问题。<br> （2）由于执行某个大型sql任务，把资源池资源消耗殆尽，且长时间不释放，导致所有通过hiveserver2提交的sql任务都无法执行。这种故障对应于上面介绍的“故障现象2”，能够很顺利通过jdbc连接到hiveserver2，但是无法执行任何sql任务。为了解决此类故障，只能让开发者减少复杂任务执行，或者将不同类型的任务，提交到不同的资源队列执行。</li>
<li>解决：修改bin&#x2F;hive-config.sh中的HADOOP_HEAPSIZE参数，</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mnasd/article/details/82690414">Hiveserver2 性能优化与GC优化_mnasd的博客-CSDN博客</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">-Xms49152m </span><br><span class="line">-Xmx49152m </span><br><span class="line">-Djava.rmi.server.hostname=xxx.xxx.xxx.xxx </span><br><span class="line">-Dcom.sun.management.jmxremote.port=8999 </span><br><span class="line">-Dcom.sun.management.jmxremote.authenticate=false </span><br><span class="line">-Dcom.sun.management.jmxremote.ssl=false </span><br><span class="line">-XX:+UseParNewGC </span><br><span class="line">-XX:ParallelGCThreads=30 </span><br><span class="line">-XX:MaxTenuringThreshold=10 </span><br><span class="line">-XX:TargetSurvivorRatio=70 </span><br><span class="line">-XX:+UseConcMarkSweepGC </span><br><span class="line">-XX:+CMSConcurrentMTEnabled </span><br><span class="line">-XX:ParallelCMSThreads=30 </span><br><span class="line">-XX:+UseCMSInitiatingOccupancyOnly </span><br><span class="line">-XX:+CMSClassUnloadingEnabled </span><br><span class="line">-XX:+DisableExplicitGC </span><br><span class="line">-XX:CMSInitiatingOccupancyFraction=70 </span><br><span class="line">-XX:+UseCMSCompactAtFullCollection </span><br><span class="line">-XX:CMSFullGCsBeforeCompaction=1 </span><br><span class="line">-verbose:gc </span><br><span class="line">-XX:+PrintGCDetails </span><br><span class="line">-XX:+PrintGCDateStamps </span><br><span class="line">-XX:GCLogFileSize=512M </span><br><span class="line">-Xloggc:/data/log/tbds/spark/gc-sparkthrift.log-$&#123;timenow&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>-Xms49152m -Xmx49152m 这两个参数需要考虑服务器实际的可用内存资源来设定；</li>
<li>-XX:ParallelGCThreads&#x3D;30 和 -XX:ParallelCMSThreads&#x3D;30 这两个参数需要考虑服务器实际的CPU核数来决定。切记不要超过CPU核数。</li>
</ul>
<h3 id="踩坑十八"><a href="#踩坑十八" class="headerlink" title="踩坑十八"></a>踩坑十八</h3><ul>
<li>现象：在azkaban任务调度中，使用useExecutor指定192.168.101.176的节点执行的executor执行任务。直接killing，报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Failed to dispatch queued execution basic because reached azkaban.maxDispatchingErrors (tried 4 executors)</span><br><span class="line">azkaban.executor.ExecutorManagerException: azkaban.executor.ExecutorManagerException: cos-bigdata-datax-01 at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1068) at azkaban.executor.ExecutorManager.access$500(ExecutorManager.java:69) at azkaban.executor.ExecutorManager$QueueProcessorThread.selectExecutorAndDispatchFlow(ExecutorManager.java:1238) at azkaban.executor.ExecutorManager$QueueProcessorThread.processQueuedFlows(ExecutorManager.java:1210) at azkaban.executor.ExecutorManager$QueueProcessorThread.run(ExecutorManager.java:1148) Caused by: azkaban.executor.ExecutorManagerException: cos-bigdata-datax-01 at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:80) at azkaban.executor.ExecutorApiGateway.callWithExecutable(ExecutorApiGateway.java:43) at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1062) ... 4 more Caused by: java.net.UnknownHostException: cos-bigdata-datax-01 at java.net.InetAddress.getAllByName0(InetAddress.java:1280) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) </span><br></pre></td></tr></table></figure></li>
<li>原因：executor通过将信息保存到mysql中，供web-server进行调用。保存的节点信息为主机名，如果主机名无法解析，就会报错UnknownHostException。</li>
<li>解决：将所有executor的主机名添加到&#x2F;etc&#x2F;hosts文件中，这样就可以解析主机名找到对应的ip地址。此外，还需要关闭executor节点的firewalld。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">192.168.101.179 cos-bigdata-hadoop-01</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑十九"><a href="#踩坑十九" class="headerlink" title="踩坑十九"></a>踩坑十九</h3><ul>
<li>现象：创建维度表，使用了lzo压缩并且创建了索引。但是在对维度表进行join时，会发现结果多了一条null数据，并且ods表中丢失了原有的lzo索引，可能导致下次读取时无法切片的问题。</li>
<li>原因：hive在join时，会误把lzo索引当成小文件进行合并，并插入一条。<br>select * from ods_log不执行MR操作，直接采用的是ods_log建表语句中指定的DeprecatedLzoTextInputFormat，能够识别lzo.index为索引文件。<br>select count(*) from ods_log执行MR操作，会先经过hive.input.format，其默认值为CombineHiveInputFormat，其会先将索引文件当成小文件合并，将其当做普通文件处理。</li>
<li>解决：在对ods表进行降维等操作时，直接关闭hive自动合并小文件即可 <code>set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat</code></li>
</ul>
<h3 id="踩坑二十"><a href="#踩坑二十" class="headerlink" title="踩坑二十"></a>踩坑二十</h3><ul>
<li>现象：不管是mr引擎还是spark引擎都会报错，但是mr能继续运行，spark会一直处于accepted。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">No Stats for hxr_u8@ods_inventoryclass, Columns: cinvcname, iinvcgrade, cinvccode</span><br><span class="line">No Stats for hxr_u8@ods_inventory, Columns: cinvdefine1, cinvdefine2, cinvcode, cinvstd, cinvccode</span><br><span class="line">No Stats for hxr_u8@ods_dispatchlist, Columns: ddate, dlid</span><br><span class="line">No Stats for hxr_u8@ods_dispatchlists, Columns: dlid, cinvcode</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：set hive.compute.query.using.stats&#x3D;false;</li>
</ul>
<h3 id="踩坑二十一"><a href="#踩坑二十一" class="headerlink" title="踩坑二十一"></a>踩坑二十一</h3><ul>
<li>现象：使用hive-on-spark时，如果有多个hiveserver2连接，那么在队列中会维持多个spark-session。此时再提交任务，任务会超时且该任务一直处于Accepted阶段等待运行。</li>
<li>原因：该队列最大可使用的资源容量大小百分比过小。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Maximum percent of resources in the cluster which can be used to run application masters - controls number of concurrent active applications. Limits on each queue are directly proportional to their queue capacities and user limits. Specified as a float - ie 0.5 = 50%. Default is 10%. This can be set for all queues with yarn.scheduler.capacity.maximum-am-resource-percent and can also be overridden on a per queue basis by setting yarn.scheduler.capacity.&lt;queue-path&gt;.maximum-am-resource-percent</span><br></pre></td></tr></table></figure></li>
<li>解决：yarn.scheduler.capacity..maximum-capacity 的值调大为0.5。目前来看还是稳定的，有待观察。</li>
</ul>
<h3 id="踩坑二十二"><a href="#踩坑二十二" class="headerlink" title="踩坑二十二"></a>踩坑二十二</h3><ul>
<li>现象：还是上面的问题，提交任务会处于Accepted导致任务失败。尝试启动spark-shell，报错 [BindException: 地址已在使用]；重启hiveserver2，也报错 [BindException: 地址已在使用]；随后重启yarn，nodemanager进程同样报错 [BindException: 地址已在使用]。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2021-11-11 10:12:59,300 WARN util.Utils: Service &#x27;SparkUI&#x27; could not bind on a random free port. You may check whether configuring an appropriate binding address.</span><br><span class="line">2021-11-11 10:12:59,314 ERROR ui.SparkUI: Failed to bind SparkUI</span><br><span class="line">java.net.BindException: Failed to bind to /0.0.0.0:0: Service &#x27;SparkUI&#x27; failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service &#x27;SparkUI&#x27; (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.</span><br></pre></td></tr></table></figure></li>
<li>原因：查看监控，发现节点上有一个盘满了<br><img src="/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982.assets%5C45474a8c12ad4b7cab373a071a31d08d.png" alt="7262acd00c6d061bf90c9ae33ad5d02.png"></li>
<li>解决：使用 hdfs diskbalancer 命令，进性数据平衡。</li>
</ul>
<h3 id="踩坑二十三"><a href="#踩坑二十三" class="headerlink" title="踩坑二十三"></a>踩坑二十三</h3><ul>
<li>现象：部署了Atlas框架，使用import_hive.sh脚本将hive的元数据倒入到Atlas是OK的，但是运行Azkaban希望通过hive的hook同步血缘时无法同步。</li>
<li>原因：因为Atlas部署的节点和Azkaban指定运行任务的节点不是同一个；Azkaban运行的节点上的hive并没有配置hook和Atlas，导致Atlas无法同步血缘。</li>
<li>解决：修改 [Azkaban指定的运行任务的节点] 为 [配置了hive-hook和Atlas的节点] ;</li>
</ul>
<h3 id="踩坑二十四"><a href="#踩坑二十四" class="headerlink" title="踩坑二十四"></a>踩坑二十四</h3><ul>
<li>现象：通过Datax向需要Kerberos认证的hdfs中导入数据，会报错认证失败，无法导入。</li>
<li>原因：认证失败的原因是，启动azkaban时使用的jdk包中的Security目录下的两个文件US_export_policy.jar和local_policy.jar没有替换。</li>
<li>解决：<br>1、jce下载地址：<a target="_blank" rel="noopener" href="https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html">https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</a><br>2、将解压得到的local_policy.jar和US_export_policy.jar拷贝到$JAVA_HOME&#x2F;jre&#x2F;lib&#x2F;security目录下面<br>再次启动Datax任务，但是还是会报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.</span><br></pre></td></tr></table></figure>
原因是因为粗心，没有将Datax导入到新搭建的配置了Kerberos的hdfs集群。导致Datax使用了Kerberos而hdfs没有使用Kerberos认证，报了上述的错误。</li>
</ul>
<h3 id="踩坑二十五"><a href="#踩坑二十五" class="headerlink" title="踩坑二十五"></a>踩坑二十五</h3><ul>
<li>现象：书接上回，解决了Kerberos认证问题后，导入数据又出现了新的问题<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">There are 3 datanode(s) running and 3 node(s) are excluded in this operation</span><br></pre></td></tr></table></figure></li>
<li>原因：猜测被selinux或防火墙阻拦</li>
<li>解决：先使用命令<code>setenforce 0</code>临时关闭selinux，然后修改配置文件&#x2F;etc&#x2F;selinux&#x2F;config或&#x2F;etc&#x2F;sysconfig&#x2F;selinux，设置SELINUX&#x3D;disabled。还是未能解决，查看datanode日志：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2021-11-17 09:29:20,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /192.168.101.177:39434. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection</span><br><span class="line">org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50d9 instead of deadbeef from client.</span><br></pre></td></tr></table></figure>
应该是hadoop版本不一致导致的，datax当前版本Hadoop版本为2.7.1，而目标Hadoop版本为3.1.3，因此尝试下载Datax源码，将对应的hadoop版本改为3.1.3，hive版本改为兼容的2.3.6，然后重新编译打包并部署<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/alibaba/DataX.git</span><br><span class="line"># 修改hadoop和hive本版后，在根目录下运行maven进行打包</span><br><span class="line">$ mvn -U clean package assembly:assembly -Dmaven.test.skip=true</span><br></pre></td></tr></table></figure>
打包成功后的DataX包位于 target 目录下，解压后使用，发现还是有问题<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">There are 3 datanode(s) running and 3 node(s) are excluded in this operation</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2021-11-17 15:58:26,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /192.168.101.177:41208. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection</span><br><span class="line">org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5086 instead of deadbeef from client.</span><br></pre></td></tr></table></figure>
Kerberos验证通过且版本也对应了，那么这个错误的具体原因是啥？？？</li>
<li>真正的原因：最终发现是配置问题，在服务器端配置了dfs.data.transfer.protection为authentication，而客户端没有配置，只需要在Datax源码中进行相应配置即可。</li>
<li>真正的解决：在任务脚本的hadoopConfig参数中配置<code>&quot;dfs.data.transfer.protection&quot;: &quot;authentication&quot;</code><br>也可以在源码中添加配置<code>hadoopConf.set(&quot;dfs.data.transfer.protection&quot;, &quot;authentication&quot;);</code>；或在hdfs-site.xml配置文件中添加配置<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;authentication&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
然后进行Datax源码重新编译打包部署即可。<blockquote>
<p>官网解释：A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.</p>
</blockquote>
</li>
</ul>
<h3 id="踩坑二十六"><a href="#踩坑二十六" class="headerlink" title="踩坑二十六"></a>踩坑二十六</h3><ul>
<li>现象：解决上述问题后，迎来最后一个问题如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">19-11-2021 15:18:08 CST so_sodetails INFO - com.alibaba.datax.common.exception.DataXException: Code:[Framework-03], Description:[DataX引擎配置错误，该问题通常是由于DataX安装错误引起，请联系您的运维解决 .].  - 在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：在Datax脚本中添加配置<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;core&quot;: &#123;</span><br><span class="line">    &quot;transport&quot;: &#123;</span><br><span class="line">        &quot;channel&quot;: &#123;</span><br><span class="line">            &quot;speed&quot;: &#123;</span><br><span class="line">                &quot;byte&quot;: 1048576</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"> &#125;,</span><br><span class="line"> &quot;job&quot;:&#123;</span><br><span class="line">        ......</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑二十七"><a href="#踩坑二十七" class="headerlink" title="踩坑二十七"></a>踩坑二十七</h3><ul>
<li>现象：Kerberos问题解决后，为了使Datax支持Lzop压缩，引入了依赖<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.5&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
结果启动后报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">No FileSystem for scheme:file</span><br></pre></td></tr></table></figure></li>
<li>原因：因为lzo-hadoop包中引入了hadoop-core依赖，该依赖带有hdfs-site.xml配置文件，导致我们的配置都被其覆盖了</li>
<li>解决：将lzo-hadoop中的依赖排除掉<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.anarres.lzo&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;lzo-hadoop&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;1.0.5&lt;/version&gt;</span><br><span class="line">    &lt;exclusions&gt;</span><br><span class="line">        &lt;exclusion&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;hadoop-core&lt;/artifactId&gt;</span><br><span class="line">        &lt;/exclusion&gt;</span><br><span class="line">    &lt;/exclusions&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑二十八"><a href="#踩坑二十八" class="headerlink" title="踩坑二十八"></a>踩坑二十八</h3><ul>
<li>现象：动态分区插入数据时<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE compass_dwd.dwd_edb_order_detail PARTITION (ds)</span><br><span class="line">SELECT ...... ;</span><br></pre></td></tr></table></figure>
出现错误（spark引擎）<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[08S01][1] Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Exception when loading 30 in table dwd_edb_order_detail with loadPath=hdfs://cos-bigdata-test-hadoop-01:98202021-12-05T03:29:59,626  WARN [load-dynamic-partitions-0] net.NetUtils: Unable to wrap exception of type class java.nio.channels.ClosedByInterruptException: it has no (String) constructor</span><br><span class="line">java.lang.NoSuchMethodException: java.nio.channels.ClosedByInterruptException.&lt;init&gt;(java.lang.String)</span><br><span class="line"></span><br><span class="line">2021-12-05T03:29:59,627 ERROR [load-dynamic-partitions-0] metadata.Hive: Exception when loading partition with parameters  partPath=hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-28-58_216_622270070896267164-1/-ext-10000/ds=2021-11-25,  table=dwd_edb_order_detail,  partSpec=&#123;ds=2021-11-25&#125;,  loadFileType=REPLACE_ALL,  listBucketingLevel=0,  isAcid=false,  hasFollowingStatsTask=true</span><br><span class="line">org.apache.hadoop.hive.ql.metadata.HiveException: Getting globStatus hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-28-58_216_622270070896267164-1/-ext-10000/ds=2021-11-25</span><br><span class="line"></span><br><span class="line">Caused by: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: &quot;cos-bigdata-test-hadoop-01/192.168.101.184&quot;; destination host is: &quot;cos-bigdata-test-hadoop-01&quot;:9820; </span><br></pre></td></tr></table></figure>
即使换成mr引擎，还是报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2021-12-05T03:55:59,044 ERROR [load-dynamic-partitions-2] metadata.Hive: Exception when loading partition with parameters  partPath=hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-55-18_450_3496090080792646897-97/-ext-10000/ds=2021-11-16,  table=dwd_edb_order_detail,  partSpec=&#123;ds=2021-11-16&#125;,  loadFileType=REPLACE_ALL,  listBucketingLevel=0,  isAcid=false,  hasFollowingStatsTask=true</span><br><span class="line">org.apache.hadoop.hive.ql.metadata.HiveException: Getting globStatus hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-55-18_450_3496090080792646897-97/-ext-10000/ds=2021-11-16</span><br><span class="line"></span><br><span class="line">Caused by: java.io.InterruptedIOException: Interrupted waiting to send RPC request to server</span><br><span class="line"></span><br><span class="line">Caused by: java.lang.InterruptedException</span><br></pre></td></tr></table></figure></li>
<li>原因：有分区存在但是该分区的文件夹不存在，导致overwrite时找不到需要清空的分区文件夹。多分区插入时报错信息会很奇怪。</li>
<li>解决：删除分区或新建分区文件夹</li>
</ul>
<h3 id="踩坑二十九"><a href="#踩坑二十九" class="headerlink" title="踩坑二十九"></a>踩坑二十九</h3><ul>
<li>现象: 问题①使用mr执行历史数据初始化时<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2021-12-09 20:29:56,924 ERROR [IPC Server handler 0 on default port 41316] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1638981807800_0026_m_000000_0 - exited : GC overhead limit exceeded</span><br><span class="line">2021-12-09 20:29:56,924 INFO [IPC Server handler 0 on default port 41316] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1638981807800_0026_m_000000_0: Error: GC overhead limit exceeded</span><br></pre></td></tr></table></figure>
②改用hive-on-spark执行初始化任务时报错内存溢出<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">2021-12-08 19:23:56,625 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.</span><br><span class="line">2021-12-08 19:23:56,625 INFO storage.BlockManagerMaster: Removal of executor 1 requested</span><br><span class="line">2021-12-08 19:23:56,627 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1</span><br><span class="line">2021-12-08 19:23:59,607 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead)</span><br><span class="line">2021-12-08 19:23:59,608 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.</span><br><span class="line">2021-12-08 19:23:59,612 INFO yarn.YarnAllocator: Completed container container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02 (state: COMPLETE, exit status: 143)</span><br><span class="line">2021-12-08 19:23:59,612 WARN yarn.YarnAllocator: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143</span><br><span class="line">[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. </span><br><span class="line">[2021-12-08 19:23:57.659]Killed by external signal</span><br><span class="line">.</span><br><span class="line">2021-12-08 19:23:59,613 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143</span><br><span class="line">[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. </span><br><span class="line">[2021-12-08 19:23:57.659]Killed by external signal</span><br><span class="line">.</span><br><span class="line">2021-12-08 19:23:59,613 ERROR cluster.YarnClusterScheduler: Lost executor 2 on cos-bigdata-test-hadoop-02: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143</span><br><span class="line">[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. </span><br><span class="line">[2021-12-08 19:23:57.659]Killed by external signal</span><br><span class="line">.</span><br><span class="line">2021-12-08 19:23:59,614 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, cos-bigdata-test-hadoop-02, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143</span><br><span class="line">[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. </span><br><span class="line">[2021-12-08 19:23:57.659]Killed by external signal</span><br><span class="line">.</span><br><span class="line">2021-12-08 19:23:59,615 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.</span><br><span class="line">2021-12-08 19:23:59,615 INFO storage.BlockManagerMaster: Removal of executor 2 requested</span><br><span class="line">2021-12-08 19:23:59,616 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2</span><br><span class="line">2021-12-08 19:24:02,613 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead)</span><br><span class="line">2021-12-08 19:24:02,614 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.</span><br><span class="line">2021-12-08 19:24:02,618 INFO yarn.YarnAllocator: Launching container container_1637563725346_0176_01_000004 on host cos-bigdata-test-hadoop-02 for executor with ID 3</span><br><span class="line">2021-12-08 19:24:02,619 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.</span><br><span class="line">2021-12-08 19:24:05,628 INFO yarn.YarnAllocator: Launching container container_1637563725346_0176_01_000005 on host cos-bigdata-test-hadoop-02 for executor with ID 4</span><br><span class="line">2021-12-08 19:24:05,629 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 1 of them.</span><br><span class="line">2021-12-08 19:24:06,129 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.101.185:55294) with ID 3</span><br><span class="line">2021-12-08 19:24:06,272 INFO storage.BlockManagerMasterEndpoint: Registering block manager cos-bigdata-test-hadoop-02:44266 with 366.3 MiB RAM, BlockManagerId(3, cos-bigdata-test-hadoop-02, 44266, None)</span><br><span class="line">2021-12-08 19:24:06,318 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 0.0 (TID 2, cos-bigdata-test-hadoop-02, executor 3, partition 1, RACK_LOCAL, 8748 bytes)</span><br><span class="line">2021-12-08 19:24:08,434 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on cos-bigdata-test-hadoop-02:44266 (size: 94.5 KiB, free: 366.2 MiB)</span><br><span class="line">2021-12-08 19:24:08,636 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.</span><br><span class="line">2021-12-08 19:24:09,170 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.101.185:55482) with ID 4</span><br><span class="line">2021-12-08 19:24:09,294 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on cos-bigdata-test-hadoop-02:44266 (size: 90.8 KiB, free: 366.1 MiB)</span><br><span class="line">2021-12-08 19:24:09,329 INFO storage.BlockManagerMasterEndpoint: Registering block manager cos-bigdata-test-hadoop-02:32770 with 366.3 MiB RAM, BlockManagerId(4, cos-bigdata-test-hadoop-02, 32770, None)</span><br><span class="line">2021-12-08 19:24:09,382 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 3, cos-bigdata-test-hadoop-02, executor 4, partition 0, RACK_LOCAL, 14062 bytes)</span><br><span class="line">2021-12-08 19:24:11,430 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on cos-bigdata-test-hadoop-02:32770 (size: 94.5 KiB, free: 366.2 MiB)</span><br><span class="line">2021-12-08 19:24:12,304 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on cos-bigdata-test-hadoop-02:32770 (size: 90.8 KiB, free: 366.1 MiB)</span><br><span class="line">2021-12-08 19:24:21,157 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 0.0 (TID 2, cos-bigdata-test-hadoop-02, executor 3): java.lang.OutOfMemoryError: Java heap space</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.&lt;init&gt;(BytesColumnVector.java:86)</span><br></pre></td></tr></table></figure></li>
<li>原因: ①内存溢出，已经设置的配置如下：<br><strong>yarn-site.xml</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10240&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- resourcemanager可以分配给容器的核数 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;16&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 虚拟核数和物理核数乘数 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.pcores-vcores-multiplier&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 该节点上YARN可使用的物理内存总量 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;12288&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<strong>capacity-scheduler</strong><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    Maximum percent of resources in the cluster which can be used to run</span><br><span class="line">    application masters i.e. controls number of concurrent running</span><br><span class="line">    applications.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
但是没有配置map和reduce可使用的内存配置，默认时1024。</li>
<li>解决: 在mapred-site.xml中配置map和reduce的内存<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;  </span><br><span class="line">   &lt;name&gt;mapred.child.java.opts&lt;/name&gt;  </span><br><span class="line">   &lt;value&gt;-Xmx8192m&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br></pre></td></tr></table></figure>
然后执行mr任务，发现可以成功执行了，但是spark任务还是失败报错。</li>
</ul>
<p><code>一般情况下，无法开始正常的MR任务，无法显示启动了多少个map reduce，则可能是因为启动资源过大造成的内存溢出，这时候就要看表原始的数据，数据量是不是有问题；如果是map和reduce执行了一段时间显示内存溢出的问题，很有可能是脚本设计不合理或者原始数据倾斜，则需要通过set参数来进行调整，最常见的就是group by，或者map，reduce处理的数据量不均匀导致。</code>如下例子</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM (</span><br><span class="line">         SELECT line, item, row_number() over (partition by get_json_object(line, &#x27;$.tid&#x27;)) row_num</span><br><span class="line">         FROM (</span><br><span class="line">                  SELECT line</span><br><span class="line">                  FROM ods_edb.ods_edb_order</span><br><span class="line">                  where ds = &#x27;$do_date&#x27;</span><br><span class="line">                    and get_json_object(line, &#x27;$.tid_item&#x27;) is not null</span><br><span class="line">                    and get_json_object(line, &#x27;$.tid&#x27;) is not null</span><br><span class="line">                    and get_json_object(line, &#x27;$.tid&#x27;) &lt;&gt; &#x27;&#x27;</span><br><span class="line">              ) tmp_edb_order</span><br><span class="line">                  lateral view compass_dwd.explode_json_array(get_json_object(tmp_edb_order.line, &#x27;$.tid_item&#x27;)) table_tmp as item</span><br><span class="line">              -- 排除套餐、套餐内单品的子订单，只剩单品子订单(必须保证套餐一定会拆分，且销量根据拆分后计算)</span><br><span class="line">         where get_json_object(item, &#x27;$.iscombination&#x27;) = 0</span><br><span class="line">     ) tmp</span><br><span class="line">where row_num = 1;</span><br></pre></td></tr></table></figure>
<p>在脚本中多次对全量数据的json串进行解析，导致内存溢出。优化方式就是，在临时表中对全量数据一次性进行解析，然后直接读取需要的字段，避免多次进行解析。</p>
<h3 id="踩坑三十"><a href="#踩坑三十" class="headerlink" title="踩坑三十"></a>踩坑三十</h3><ul>
<li>现象：使用mr引擎执行hive初始化历史数据任务时出现错误<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">2021-12-10 15:12:04,451 WARN [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Task attempt attempt_1639119531715_0006_r_000000_0 is done from TaskUmbilicalProtocol&#x27;s point of view. java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:717)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)</span><br><span class="line">2021-12-10 15:42:40,895 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Unable to write out JobSummaryInfo to [hdfs://cos-bigdata-test-hadoop-01:9820/tmp/hadoop-yarn/staging/history/done_intermediate/chenjie/job_1639119531715_0018.summary_tmp]</span><br><span class="line">java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at org.apache.hadoop.hdfs.ExceptionLastSeen.set(ExceptionLastSeen.java:45)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:829)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:717)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)</span><br><span class="line">2021-12-10 15:42:40,897 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.</span><br><span class="line">org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:732)</span><br><span class="line">	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:383)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at org.apache.hadoop.hdfs.ExceptionLastSeen.set(ExceptionLastSeen.java:45)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:829)</span><br><span class="line">Caused by: java.lang.OutOfMemoryError: unable to create new native thread</span><br><span class="line">	at java.lang.Thread.start0(Native Method)</span><br><span class="line">	at java.lang.Thread.start(Thread.java:717)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)</span><br><span class="line">	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)</span><br></pre></td></tr></table></figure></li>
<li>原因：①可能是没有设置系统参数；②可能是动态分区插入数据时导致 内存不足。</li>
<li>解决：针对原因1，修改&#x2F;etc&#x2F;security&#x2F;limits.conf 		<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">* soft nofile 65536	</span><br><span class="line">* hard nofile 131072		</span><br><span class="line">* soft nproc 2048	</span><br><span class="line">* hard nproc 65536</span><br></pre></td></tr></table></figure>
发现还是报错。<br>针对原因2，查看SQL脚本是否有误导致分区过多，最后发现是动态分区的ds时间不是yyyy-MM-dd格式的，导致group by和最终插入的分区实在太多了，才使得内存溢出。</li>
</ul>
<h3 id="踩坑三十一"><a href="#踩坑三十一" class="headerlink" title="踩坑三十一"></a>踩坑三十一</h3><ul>
<li>现象：执行历史任务时，报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">14-12-2021 13:58:11 CST dws_total_model_init INFO - Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception from MapJoinOperator : Unexpected exception from MapJoinOperator : org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.parquet.hadoop.MemoryManager$1: New Memory allocation 1048216 bytes is smaller than the minimum allocation size of 1048576 bytes.</span><br></pre></td></tr></table></figure></li>
<li>原因：未知</li>
<li>解决：增加内存</li>
</ul>
<h3 id="踩坑三十二"><a href="#踩坑三十二" class="headerlink" title="踩坑三十二"></a>踩坑三十二</h3><ul>
<li>现象：使用mr引擎正常运行，但是使用spark引擎执行任务时报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.AssertionError:Cannot add expression of different type to set: set type is RecordType(VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f1, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f2, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f3, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f4, INTEGER $f6, INTEGER $f7, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f8, BIGINT $f9) NOT NULL expression type is RecordType(VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f1, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f2, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f3, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en_US$primary&quot; $f4, INTEGER $f6, VARCHAR(2147483647) CHARACTER SET &quot;UTF-16LE&quot; COLLATE &quot;ISO-8859-1$en ...</span><br></pre></td></tr></table></figure></li>
<li>原因：未知。查询资料，有人说这是calcite的bug。</li>
<li>解决：关闭CBO优化器 <code>set hive.cbo.enable=false</code></li>
</ul>
<h3 id="踩坑三十三"><a href="#踩坑三十三" class="headerlink" title="踩坑三十三"></a>踩坑三十三</h3><ul>
<li>现象：mr引擎执行脚本时报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">2021-12-28 10:35:12,632 ERROR [IPC Server handler 8 on default port 36440] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1639119531715_1552_m_000004_0 - exited : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row </span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:163)</span><br><span class="line">	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.mr.ExecMapRunner.run(ExecMapRunner.java:37)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)</span><br><span class="line">	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)</span><br><span class="line">	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)</span><br><span class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row </span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:973)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:154)</span><br><span class="line">	... 9 more</span><br><span class="line">Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: </span><br><span class="line">DeserializeRead detail: Reading byte[] of length 30912 at start offset 0 for length 14791 to read 10 fields with types [string, string, string, string, string, string, string, string, string, array&lt;string&gt;].  Read field #9 at field start position 0 current read offset 13539</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:928)</span><br><span class="line">	... 10 more</span><br><span class="line">Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:187)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storePrimitiveRowColumn(VectorDeserializeRow.java:588)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeComplexFieldRowColumn(VectorDeserializeRow.java:778)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeListRowColumn(VectorDeserializeRow.java:822)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeRowColumn(VectorDeserializeRow.java:938)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:1360)</span><br><span class="line">	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:923)</span><br><span class="line">	... 10 more</span><br></pre></td></tr></table></figure></li>
<li>原因：未知</li>
<li>解决：禁用hive矢量执行<br><code>set hive.vectorized.execution.enabled=false;</code><br><code>set hive.vectorized.execution.reduce.enabled=false;</code><br><code>set hive.vectorized.execution.reduce.groupby.enabled=false;</code></li>
</ul>
<h3 id="踩坑三十四"><a href="#踩坑三十四" class="headerlink" title="踩坑三十四"></a>踩坑三十四</h3><ul>
<li><p>问题：mapred的中间任务启动的mappers数量过大，导致整个任务慢</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">31-12-2021 20:36:20 CST dwd_edb_order_info_init INFO - Stage-5 is selected by condition resolver.</span><br><span class="line">31-12-2021 20:36:23 CST dwd_edb_order_info_init INFO - Starting Job = job_1640834494731_0089, Tracking URL = [http://cos-bigdata-test-hadoop-02:8088/proxy/application_1640834494731_0089/](http://cos-bigdata-test-hadoop-02:8088/proxy/application_1640834494731_0089/)</span><br><span class="line">31-12-2021 20:36:23 CST dwd_edb_order_info_init INFO - Kill Command = /opt/module/hadoop-3.1.3/bin/mapred job  -kill job_1640834494731_0089</span><br><span class="line">31-12-2021 20:36:31 CST dwd_edb_order_info_init INFO - Hadoop job information for Stage-5: number of mappers: 930; number of reducers: 0</span><br><span class="line">31-12-2021 20:36:31 CST dwd_edb_order_info_init INFO - 2021-12-31 20:36:31,852 Stage-5 map = 0%,  reduce = 0%&lt;/pre&gt;</span><br></pre></td></tr></table></figure>
<p><img src="/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982.assets%5C1f2bb214e38745ec818af628921bea03.png" alt="image.png"></p>
</li>
<li><p>原因：</p>
</li>
</ul>
<h3 id="踩坑三十五"><a href="#踩坑三十五" class="headerlink" title="踩坑三十五"></a>踩坑三十五</h3><ul>
<li>问题：azkaban重启之后，有时会发生找不到executor的情况<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Failed to dispatch queued execution hxr_compass_hive because reached azkaban.maxDispatchingErrors (tried 1 executors)</span><br><span class="line">azkaban.executor.ExecutorManagerException: azkaban.executor.ExecutorManagerException: java.nio.file.FileSystemException: executions/1446/flow20.project -&gt; /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1068) at azkaban.executor.ExecutorManager.access$500(ExecutorManager.java:69) at azkaban.executor.ExecutorManager$QueueProcessorThread.selectExecutorAndDispatchFlow(ExecutorManager.java:1238) at azkaban.executor.ExecutorManager$QueueProcessorThread.processQueuedFlows(ExecutorManager.java:1210) at azkaban.executor.ExecutorManager$QueueProcessorThread.run(ExecutorManager.java:1148) Caused by: azkaban.executor.ExecutorManagerException: java.nio.file.FileSystemException: executions/1446/flow20.project -&gt; /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:80) at azkaban.executor.ExecutorApiGateway.callWithExecutable(ExecutorApiGateway.java:43) at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1062) ... 4 more Caused by: java.io.IOException: java.nio.file.FileSystemException: executions/1446/flow20.project -&gt; /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorApiGateway.callForJsonObjectMap(ExecutorApiGateway.java:108) at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:78) ... 6 more</span><br></pre></td></tr></table></figure></li>
<li>原因：发现是有一个executor异常导致分配到该executor时出错。根据异常信息查看该project文件夹下的18.8文件夹，怀疑是权限问题，上次接收任务文件时是root用户，现在是hxr用户运行的executor程序，导致无法修改该文件而报错。</li>
<li>解决：尝试将所有文件修改为启动executor的用户，效果待观察。。。</li>
</ul>
<h3 id="踩坑三十六"><a href="#踩坑三十六" class="headerlink" title="踩坑三十六"></a>踩坑三十六</h3><ul>
<li>问题：多次连接Mysql失败后报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.sql.SQLException: null,  message from server: &quot;Host &#x27;192.168.101.177&#x27; is blocked because of many connection errors; unblock with &#x27;mysqladmin flush-hosts&#x27;&quot;.</span><br></pre></td></tr></table></figure></li>
<li>原因：  max_connect_errors是一个MySQL中与安全有关的计数器值，它负责阻止过多尝试失败的客户端以防止暴力破解密码的情况。max_connect_errors的值与性能并无太大关系。当此值设置为10时，意味着如果某一客户端尝试连接此MySQL服务器，但是失败（如密码错误等等）10次，则MySQL会无条件强制阻止此客户端连接。</li>
<li>解决：重置此计数器的值，则必须重启MySQL服务器或者执行 mysql&gt; flush hosts; 命令。当这一客户端成功连接一次MySQL服务器后，针对此客户端的max_connect_errors会清零。</li>
</ul>
<h3 id="踩坑三十七"><a href="#踩坑三十七" class="headerlink" title="踩坑三十七"></a>踩坑三十七</h3><ul>
<li>问题：初始化脚本运行，在写入了部分分区后报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Exception when loading 1 in table</span><br></pre></td></tr></table></figure></li>
<li>原因：分区存在，但是分区文件夹被删除了</li>
<li>解决：修复分区表<code>MSCK REPAIR TABLE table_name</code>；或直接重建表和删除所有分区文件夹，然后重新运行脚本。</li>
</ul>
<h3 id="踩坑三十八"><a href="#踩坑三十八" class="headerlink" title="踩坑三十八"></a>踩坑三十八</h3><ul>
<li>问题：使用Tez引擎，map和reducer一直pending，任务无法运行。日志如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2022-01-18 01:43:08,450 [WARN] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Held container expected to be not null for a non-AM-released container</span><br><span class="line">2022-01-18 01:43:08,450 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Ignoring unknown container: container_1642441041019_0001_01_000087</span><br><span class="line">2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Preempting new container: container_1642441041019_0001_01_000088 with priority: 152 to free resource for request: Capability[&lt;memory:10240, vCores:2&gt;]Priority[122]AllocationRequestId[0]ExecutionTypeRequest[&#123;Execution Type: GUARANTEED, Enforce Execution Type: false&#125;]Resource Profile[null] . Current free resources: &lt;memory:9216, vCores:30&gt;</span><br><span class="line">2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Resending request for task again: attempt_1642441041019_0001_1_10_000000_0</span><br><span class="line">2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Deallocating task: attempt_1642441041019_0001_1_10_000000_0 before allocation</span><br><span class="line">2022-01-18 01:43:10,459 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Allocation request for task: attempt_1642441041019_0001_1_10_000000_0 with request: Capability[&lt;memory:10240, vCores:2&gt;]Priority[152]AllocationRequestId[0]ExecutionTypeRequest[&#123;Execution Type: GUARANTEED, Enforce Execution Type: false&#125;]Resource Profile[null] host: cos-bigdata-test-hadoop-01 rack: null</span><br><span class="line">2022-01-18 01:43:10,459 [INFO] [DelayedContainerManager] |rm.YarnTaskSchedulerService|: AssignAll - Skipping delayed container as container is no longer running, containerId=container_1642441041019_0001_01_000088</span><br></pre></td></tr></table></figure></li>
<li>原因：mr的mapreduce.reduce.cpu.vcores和yarn.scheduler.minimum-allocation-vcores配置对Tez不起作用，需要手动设置。</li>
<li>解决：set hive.tez.cpu.vcores&#x3D;1;</li>
</ul>
<h3 id="踩坑三十九"><a href="#踩坑三十九" class="headerlink" title="踩坑三十九"></a>踩坑三十九</h3><ul>
<li>问题：Tez执行任务报错 <code>Too many counters: 121 max=120</code></li>
<li>原因：</li>
<li>解决：修改Hadoop的mapred-site.xml配置文件的mapreduce.job.counters.max参数，默认为120，需要设置成更大的值。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt; </span><br><span class="line">    &lt;name&gt;mapreduce.job.counters.max&lt;/name&gt; </span><br><span class="line">    &lt;value&gt;200&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑四十"><a href="#踩坑四十" class="headerlink" title="踩坑四十"></a>踩坑四十</h3><ul>
<li>问题：Tez执行过程中报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OutOfMemoryError: unable to create new native thread</span><br></pre></td></tr></table></figure></li>
<li>原因：可能是因为最大创建线程限制</li>
<li>解决：<br>查看当前允许最大线程<code>sysctl kernel.pid_max</code><br>查看当前系统线程总数<code>ps -eLf | wc -l</code><br>临时修改<code>echo 1000000 &gt; /proc/sys/kernel/pid_max</code><br>永久修改<code>echo &quot;kernel.pid_max=1000000 &quot; &gt;&gt; /etc/sysctl.conf; sysctl -p</code></li>
</ul>
<h3 id="踩坑四十一"><a href="#踩坑四十一" class="headerlink" title="踩坑四十一"></a>踩坑四十一</h3><ul>
<li>问题：使用Azkaban调度时，偶尔会报错如下，但是资源应该是充足的，且重新跑会成功执行<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：</li>
</ul>
<h3 id="踩坑四十二"><a href="#踩坑四十二" class="headerlink" title="踩坑四十二"></a>踩坑四十二</h3><ul>
<li>问题：使用Tez引擎时，如果使用INSERT … UNION ALL语句，那么会出现查询不到数据的情况。</li>
<li>原因：Tez引擎对于insert union操作会进行优化，通过并行加快速度，为防止有相同文件输出，所以对并行的输出各自生成成了一个子目录，在子目录中存放结果。而MR和Spark引擎不会递归查询分区目录下的文件。</li>
<li>解决：尝试使用UNION语句后再SELECT到一张临时表中再进行INSERT，还是会有子目录出现。<ul>
<li>方法一：查询时引擎设置为Tez；</li>
<li>方法二：设置mr引擎递归查询 <code>set mapreduce.input.fileinputformat.input.dir.recursive=true;</code> 对spark引擎同样生效。但是加上这个语句会造成运行insert union语句卡死；</li>
<li>方法三：使用union替代union all。</li>
</ul>
</li>
</ul>
<h3 id="踩坑四十三"><a href="#踩坑四十三" class="headerlink" title="踩坑四十三"></a>踩坑四十三</h3><ul>
<li>问题：使用Hive-on-spark引擎计算开窗函数，那么开窗函数中的row between … and … 语句将不会生效，默认是从第一行到当前行。</li>
<li>原因：Hive-on-spark引擎的bug。</li>
<li>解决：切换回MR引擎则恢复正常。</li>
</ul>
<h3 id="踩坑四十四"><a href="#踩坑四十四" class="headerlink" title="踩坑四十四"></a>踩坑四十四</h3><ul>
<li>问题：使用Hive-on-spark引擎，在子查询中使用datediff函数，且函数中的日期是string类型的，那么就会报空指针异常。</li>
<li>原因：Hive-on-spark引擎的bug。</li>
<li>解决：1. 切换回MR引擎； 2. 将string类型转换为date类型。</li>
</ul>
<h3 id="踩坑四十五"><a href="#踩坑四十五" class="headerlink" title="踩坑四十五"></a>踩坑四十五</h3><ul>
<li>问题：使用Hive-on-spark引擎时，报错 Caused by: java.lang.ClassNotFoundException: org.antlr.runtime.tree.CommonTree</li>
<li>原因：缺少包 antlr-runtime-3.5.2.jar</li>
<li>解决：需要在hive-site.xml中配置的spark.yarn.jars的hdfs路径中，添加缺失的包 antlr-runtime-3.5.2.jar</li>
</ul>
<h3 id="踩坑四十六"><a href="#踩坑四十六" class="headerlink" title="踩坑四十六"></a>踩坑四十六</h3><ul>
<li>问题：使用Mapreduce引擎对历史数据进行动态分区数据插入，报错如下<br><strong>Error: Java heap space</strong>或<strong>Error: GC overhead limit exceeded</strong>；不动态插入，直接查询就没有问题；</li>
<li>原因：Parquet和ORC是列式批处理文件格式。这些格式要求在写入文件之前将批次的行（batches of rows）缓存在内存中。在这种情况下，每个mapper必须为遇到的每个动态分区创建一个新的文件写入器（file writer）。在执行INSERT语句时，动态分区目前的实现是：至少为每个动态分区目录打开一个文件写入器（file writer）。由于这些缓冲区是按分区维护的，因此在运行时所需的内存量随着分区数量的增加而增加。所以经常会导致mappers或reducers的OOM，具体取决于打开的文件写入器（file writer）的数量。默认情况下，Hive为每个打开的Parquet文件缓冲区（file buffer）分配128MB。这个buffer大小由参数parquet.block.size控制。为获得最佳性能，parquet的buffer size需要与HDFS的block size保持对齐（比如相等），从而使每个parquet文件在单个HDFS的块中，以便每个I&#x2F;O请求都可以读取整个数据文件，而无需通过网络传输访问后续的block。<code>set parquet.block.size=268435456;(256MB)</code></li>
<li>解决：</li>
</ul>
<ol>
<li>首先确定开启了动态分区和非严格模式.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.dynamic.partition=true;</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br></pre></td></tr></table></figure></li>
<li>启用自动排序参数，强制产生reduce任务。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.optimize.sort.dynamic.partition=true;</span><br></pre></td></tr></table></figure></li>
<li>如果还报错内存溢出，强制修改map端内存大小.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set mapreduce.map.java.opts=-Xmx4096m</span><br></pre></td></tr></table></figure></li>
<li>如果仍然报错内存溢出，调整时间间隔，分多次进行动态分区或增加map或者reduce的数量。</li>
</ol>
<p>动态分区导致内存溢出，以上优化无效时，可以打开这个参数<br><code>hive.optimize.sort.dynamic.partition=true;</code><br>产生OOM问题的原因是同时打开了太多写分区的record writer同时写入文件，开启该参数的话，分区列会全局排序，使得reduce端每个分区只有一个文件写入，降低reduce的内存压力，但会严重降低reduce处理并写入一个分区的速度。</p>
<blockquote>
<p>官网解释是：When enabled, dynamic partitioning column will be globally sorted. This way we can keep only one record writer open for each partition value in the reducer thereby reducing the memory pressure on reducers.</p>
</blockquote>
<h3 id="踩坑四十七"><a href="#踩坑四十七" class="headerlink" title="踩坑四十七"></a>踩坑四十七</h3><ul>
<li>现象：使用mr引擎时，使用如下sql会出现相同的数据无法匹配的问题<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">set hive.execution.engine=mr;</span><br><span class="line">set mapred.reduce.tasks=5;</span><br><span class="line">select *</span><br><span class="line">from (</span><br><span class="line">         select t1.id, t1.name</span><br><span class="line">         from default.test t1</span><br><span class="line">                  left join</span><br><span class="line">              default.test t2</span><br><span class="line">              on t1.id = t2.id</span><br><span class="line">     ) t3</span><br><span class="line">         full outer join</span><br><span class="line">     default.test t4</span><br><span class="line">     on t3.id = t4.id;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>奇怪的是，如下的每一种操作都会使结果正确：<br>①使用spark引擎时结果正确，<br>②去掉LEFT JOIN 语句结果也是正确的<br>③将full outer join 换成inner join也是正确的。<br>④reduce任务数量为10，如果将reduce数量设置为1</p>
<ul>
<li>原因：自己建假数据也能复现这个问题，感觉是个bug，需要后续观察；</li>
<li>解决：目前来看最好的办法就是将 full join 前一张表的left join删掉，或者直接挪到最后。</li>
</ul>
<p>###踩坑五十</p>
<ul>
<li>问题：执行mr任务时，reduce任务进程到100%，但是一直没有完成，直到报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">AttemptID:attempt_1648013040469_0016_r_000000_0 Timed out after 600 secs [2022-03-23 14:49:58.214]Sent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 11603 as user hive for container container_1648013040469_0016_01_000005, result=success [2022-03-23 14:49:58.219]Container killed by the ApplicationMaster. [2022-03-23 14:49:58.224]Container killed on request. Exit code is 143 [2022-03-23 14:49:58.243]Container exited with a non-zero exit code 143. </span><br></pre></td></tr></table></figure></li>
<li>原因：因为我在inner join&#x2F;left join&#x2F;full outer join的连接条件中使用了if 和 case when语句。hql的join on操作只支持相等条件，但是又不会报错，只会卡死在那里。</li>
<li>解决：不再连接条件on后面使用if 和 case when 。如果连接条件比较复杂，可以在连接表中对列进行处理，再作为连接条件即可。</li>
</ul>
<h3 id="踩坑五十一"><a href="#踩坑五十一" class="headerlink" title="踩坑五十一"></a>踩坑五十一</h3><ul>
<li>问题：执行tez任务时，设置的参数 hive.tez.cpu.vcores 不等于1时，就会出现任务一直pending</li>
<li>原因：未知</li>
<li>解决：将参数 hive.tez.cpu.vcores 设置为1</li>
</ul>
<h3 id="踩坑五十二"><a href="#踩坑五十二" class="headerlink" title="踩坑五十二"></a>踩坑五十二</h3><ul>
<li>问题：执行hive脚本时，发现如下脚本中的hql语句，插入的分区都是**<strong>HIVE_DEFAULT_PARTITION</strong>**，但是直接执行hql是正常的<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE compass_dwd.dwd_edb_send PARTITION (ds)</span><br><span class="line">SELECT tmp_send.dlid,</span><br><span class="line">       tmp_send.csocode,</span><br><span class="line">       regexp_extract(cdefine32, &#x27;(\d)&#123;4&#125;-(\d)&#123;2&#125;-(\d)&#123;2&#125;&#x27;, 0) as ds</span><br><span class="line">FROM (</span><br><span class="line">         SELECT *</span><br><span class="line">         FROM compass_dwd.dwd_so_send</span><br><span class="line">         where ds = &#x27;$do_date&#x27;</span><br><span class="line">     ) tmp_send</span><br><span class="line">         JOIN</span><br><span class="line">     (</span><br><span class="line">         SELECT *</span><br><span class="line">         FROM compass_dwd.dwd_edb_order_paid</span><br><span class="line">     ) tmp_order_model</span><br><span class="line">     ON tmp_send.cdlcode = tmp_order_model.tid;</span><br></pre></td></tr></table></figure></li>
<li>原因：在脚本中，函数 <strong>regexp_extract(cdefine32, ‘(\d){4}-(\d){2}-(\d){2}’, 0)</strong>  中的特殊符号需要再次转义。同datax脚本中的转义 <strong>${ds} as ds</strong></li>
<li>解决：添加转义符如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">INSERT OVERWRITE TABLE compass_dwd.dwd_edb_send PARTITION(ds)</span><br><span class="line">SELECT tmp_send.dlid,</span><br><span class="line">       tmp_send.csocode,</span><br><span class="line">       regexp_extract(cdefine32, &#x27;(\\d)&#123;4&#125;-(\\d)&#123;2&#125;-(\\d)&#123;2&#125;&#x27;, 0) as ds</span><br><span class="line">FROM (</span><br><span class="line">         SELECT *</span><br><span class="line">         FROM compass_dwd.dwd_so_send</span><br><span class="line">         where ds = &#x27;2022-04-17&#x27;</span><br><span class="line">     ) tmp_send</span><br><span class="line">         JOIN</span><br><span class="line">     (</span><br><span class="line">         SELECT *</span><br><span class="line">         FROM compass_dwd.dwd_edb_order_paid</span><br><span class="line">     ) tmp_order_model</span><br><span class="line">     ON tmp_send.cdlcode = tmp_order_model.tid</span><br><span class="line">WHERE regexp_extract(cdefine32, &#x27;(\\d)&#123;4&#125;-(\\d)&#123;2&#125;-(\\d)&#123;2&#125;&#x27;, 0) is not null;</span><br></pre></td></tr></table></figure>
所以在用到特殊字符时，需要注意是否需要转义，如 <code>select str_to_map(&quot;name=cj|age=18&quot;,&quot;\|&quot;,&quot;=&quot;);</code>  中的符号需要转义，而在shell脚本中还需要对转义符转义。</li>
</ul>
<h3 id="踩坑五十三"><a href="#踩坑五十三" class="headerlink" title="踩坑五十三"></a>踩坑五十三</h3><ul>
<li>问题：dwt_so_paid_order_topic计算同比环比时，发现理应有值的字段结果却是 null ，设置 set hive.vectorized.execution.enabled&#x3D;false; 后就正常了</li>
<li>原因：未知</li>
<li>解决：设置  set hive.vectorized.execution.enabled&#x3D;false;</li>
</ul>
<h3 id="踩坑五十四"><a href="#踩坑五十四" class="headerlink" title="踩坑五十四"></a>踩坑五十四</h3><ul>
<li>问题：还是上面的同一个脚本 dwt_so_paid_order_topic 计算同比环比时，有两个LEFT JOIN 语句如下，发现最后一张LEFT JOIN 表的字段结果都是 null，tmp_mom和tmp_yoy表无论谁放在后面都会变成null。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM tmp_date tmp_ori</span><br><span class="line">         LEFT JOIN tmp_date tmp_mom</span><br><span class="line">                   ON tmp_ori.mom_date = tmp_mom.date_end and tmp_ori.date_type = tmp_mom.date_type</span><br><span class="line">                       and nvl(tmp_ori.cinvcode, &#x27;null&#x27;) = nvl(tmp_mom.cinvcode, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.cinvccode, &#x27;null&#x27;) = nvl(tmp_mom.cinvccode, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.group_code, &#x27;null&#x27;) = nvl(tmp_mom.group_code, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.division_code, &#x27;null&#x27;) = nvl(tmp_mom.division_code, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.shopid, &#x27;null&#x27;) = nvl(tmp_mom.shopid, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.channel_id, &#x27;null&#x27;) = nvl(tmp_mom.channel_id, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.class_id, &#x27;null&#x27;) = nvl(tmp_mom.class_id, &#x27;null&#x27;)</span><br><span class="line">         LEFT JOIN tmp_date tmp_yoy</span><br><span class="line">                   ON tmp_ori.yoy_date = tmp_yoy.date_end and tmp_ori.date_type = tmp_yoy.date_type</span><br><span class="line">                       and nvl(tmp_ori.cinvcode, &#x27;null&#x27;) = nvl(tmp_yoy.cinvcode, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.cinvccode, &#x27;null&#x27;) = nvl(tmp_yoy.cinvccode, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.group_code, &#x27;null&#x27;) = nvl(tmp_yoy.group_code, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.division_code, &#x27;null&#x27;) = nvl(tmp_yoy.division_code, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.shopid, &#x27;null&#x27;) = nvl(tmp_yoy.shopid, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.channel_id, &#x27;null&#x27;) = nvl(tmp_yoy.channel_id, &#x27;null&#x27;)</span><br><span class="line">                       and nvl(tmp_ori.class_id, &#x27;null&#x27;) = nvl(tmp_yoy.class_id, &#x27;null&#x27;);</span><br></pre></td></tr></table></figure>
配置中配置了mapjoin的相关参数<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join=true;</span><br><span class="line">set hive.mapjoin.smalltable.filesize=250000000;</span><br><span class="line">set hive.auto.convert.join.noconditionaltask=true;</span><br><span class="line">set hive.auto.convert.join.noconditionaltask.size=500000000;</span><br></pre></td></tr></table></figure></li>
<li>原因：未知，不知道是不是 三张表其实都是同一张表，即 自己join自己join自己 的原因</li>
<li>解决：直接关闭mapjoin<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set hive.auto.convert.join=false;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑五十五"><a href="#踩坑五十五" class="headerlink" title="踩坑五十五"></a>踩坑五十五</h3><ul>
<li>问题：执行任务时报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception: Too many counters: 121 max=120</span><br></pre></td></tr></table></figure></li>
<li>原因：如果执行引擎时 tez，则说明当前作业的 counters 数量超过 tez 默认的 counters 限制。</li>
<li>解决：设置新的限制值<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set tez.counters.max =20000;</span><br><span class="line">set tez.counters.max.groups=10000;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="踩坑五十六"><a href="#踩坑五十六" class="headerlink" title="踩坑五十六"></a>踩坑五十六</h3><ul>
<li>问题：跑任务时报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalArgumentException: tez.runtime.io.sort.mb 2048 should be larger than 0 and should be less than the available task memory (MB):1456</span><br></pre></td></tr></table></figure></li>
<li>原因：设置的tez.runtime.io.sort.mb比hive.tez.container.size相等或还要大，tez.runtime.io.sort.mb无法获取指定的内存大小导致任务失败</li>
<li>解决：增大hive.tez.container.size或减小tez.runtime.io.sort.mb</li>
</ul>
<h3 id="踩坑五十七"><a href="#踩坑五十七" class="headerlink" title="踩坑五十七"></a>踩坑五十七</h3><ul>
<li>问题：执行完任务进行多分区插入时报错<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - Loading data to table compass_dwd.dwd_so_stock_out partition (ds=null)</span><br><span class="line">30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - </span><br><span class="line">30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - </span><br><span class="line">30-05-2022 18:26:14 CST dwd_so_stock_out_init INFO - 	 Time taken to load dynamic partitions: 367.517 seconds</span><br><span class="line">30-05-2022 18:26:16 CST dwd_so_stock_out_init INFO - 	 Time taken for adding to write entity : 1.362 seconds</span><br><span class="line">30-05-2022 18:27:48 CST dwd_so_stock_out_init INFO - FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.StatsTask. GC overhead limit exceeded</span><br><span class="line">30-05-2022 18:27:48 CST dwd_so_stock_out_init INFO - Process with id 17249 completed unsuccessfully in 602 seconds.</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：</li>
</ul>
<h3 id="踩坑五十八"><a href="#踩坑五十八" class="headerlink" title="踩坑五十八"></a>踩坑五十八</h3><ul>
<li>问题：自定义UDTF函数compass_dws.parseErrorCodeAndGetDiff(arg1,arg2)，在执行<code>select compass_dws.parseErrorCodeAndGetDiff(&quot;12&quot;, &quot;0&quot;)</code>是有两个结果，但是执行如下sql时没有数据<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM (</span><br><span class="line">         SELECT 1 as id</span><br><span class="line">         UNION ALL</span><br><span class="line">         SELECT 2 as id</span><br><span class="line">     ) tmp_event LATERAL VIEW compass_dws.parseErrorCodeAndGetDiff(&quot;12&quot;, &quot;0&quot;) tmp as fault_code;</span><br></pre></td></tr></table></figure></li>
<li>原因：因为在写自定义函数的java程序中，通过forward返回的结果需要是一个数组，如果不是数组，那么就会出现上述这种情况。</li>
<li>解决：将forward返回的结果用数组包裹起来。</li>
</ul>
<h3 id="踩坑六十一"><a href="#踩坑六十一" class="headerlink" title="踩坑六十一"></a>踩坑六十一</h3><ul>
<li>问题：执行sql报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 5, vertexId=vertex_1649781999334_26772_1_00, diagnostics=[Task failed, taskId=task_1649781999334_26772_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : java.lang.RuntimeException: java.lang.AssertionError: Output column number expected to be 0 when isRepeating</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：禁用vectorized  <code>set hive.vectorized.execution.enabled=false;</code></li>
</ul>
<h3 id="踩坑六十二"><a href="#踩坑六十二" class="headerlink" title="踩坑六十二"></a>踩坑六十二</h3><ul>
<li>问题：使用Tez引擎进行计算时报错如下(使用DataGrip用hiveserver2连接)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Dag submit failed due to unexpected checked exception stack trace: [org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1167), org.apache.hadoop.ipc.Client.call(Client.java:1441), org.apache.hadoop.ipc.Client.call(Client.java:1388), org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233), org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118), com.sun.proxy.$Proxy60.submitDAG(Unknown Source), org.apache.tez.client.FrameworkClient.submitDag(FrameworkClient.java:133), org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:702), org.apache.tez.client.TezClient.submitDAG(TezClient.java:611), org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:538), org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:216), org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205), org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97), org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2664), org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2335), org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2011), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1709), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1703), org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157), org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224), org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316), java.security.AccessController.doPrivileged(Native Method), javax.security.auth.Subject.doAs(Subject.java:422), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149), java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624), java.lang.Thread.run(Thread.java:748)] retrying...</span><br><span class="line">Session re-established.</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</span><br></pre></td></tr></table></figure></li>
<li>原因：直接用hive本地客户端跑是可以成功的，怀疑是hiveserver2中的配置问题导致任务提交失败。</li>
<li>解决：<a target="_blank" rel="noopener" href="https://my.oschina.net/u/4413909/blog/3659191">HDFS问题集（一），使用命令报错：com.google.protobuf.ServiceException:java.lang.OutOfMemoryError:java heap space - osc_z7d2bxvl的个人空间 - OSCHINA - 中文开源技术交流社区</a></li>
</ul>
<h3 id="踩坑六十三"><a href="#踩坑六十三" class="headerlink" title="踩坑六十三"></a>踩坑六十三</h3><ul>
<li>问题：使用如下计算时，会发现year_month_day不为null时，显示的结果不是year_month_day的值。如year_month_day为2022-06-18时，最终date_end的结果为2022-06-30，但是修改为when year_month_day is not null then date_add(year_month_day,1)时，结果时2022-06-19是正确的。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">SELECT productkey,</span><br><span class="line">       iotid,</span><br><span class="line">       devicename,</span><br><span class="line">       event_name,</span><br><span class="line">       count(*)       as device_uv,</span><br><span class="line">       case</span><br><span class="line">           when year_month_day is not null then year_month_day</span><br><span class="line">           when year_week is not null and date_sub(next_day(max(ds), &#x27;MO&#x27;), 1) &lt;= &#x27;2022-06-18&#x27;</span><br><span class="line">               then date_sub(next_day(max(ds), &#x27;MO&#x27;), 1)</span><br><span class="line">           when year_month is not null and last_day(max(ds)) &lt;= &#x27;2022-06-18&#x27; then last_day(max(ds))</span><br><span class="line">           when year is not null and concat(year, &#x27;-12-31&#x27;) &lt;= &#x27;2022-06-18&#x27; then concat(year, &#x27;-12-31&#x27;)</span><br><span class="line">           else &#x27;2022-06-18&#x27;</span><br><span class="line">           end        as date_end,</span><br><span class="line">       case</span><br><span class="line">           when year_month_day is not null then 3</span><br><span class="line">           when year_week is not null then 2</span><br><span class="line">           when year_month is not null then 1</span><br><span class="line">           when year is not null then 0</span><br><span class="line">           else 4 end as date_type</span><br><span class="line">FROM (</span><br><span class="line">         SELECT productkey,</span><br><span class="line">                iotid,</span><br><span class="line">                devicename,</span><br><span class="line">                event_name,</span><br><span class="line">                happen_count,</span><br><span class="line">                ds                                                                  as year_month_day,</span><br><span class="line">                concat(year(date_add(next_day(ds, &#x27;MO&#x27;), -4)), &#x27;-&#x27;, weekofyear(ds)) as year_week,</span><br><span class="line">                date_format(ds, &#x27;yyyy-MM&#x27;)                                          as year_month,</span><br><span class="line">                year(ds)                                                            as year,</span><br><span class="line">                ds</span><br><span class="line">         FROM compass_dws.dws_feiyan_device_event_count_day</span><br><span class="line">         WHERE ds &lt;= &#x27;2022-06-18&#x27;</span><br><span class="line">           and happen_count &gt; 0</span><br><span class="line">           and iotid is not null</span><br><span class="line">           and opt_id is null</span><br><span class="line">     ) tmp_event</span><br><span class="line">GROUP BY productkey, iotid, devicename, event_name, year_month_day, year_week, year_month, year</span><br><span class="line">    GROUPING SETS (</span><br><span class="line">         -- 计算总设备日周月年总活跃数和每个产品下的日周月年总活跃度</span><br><span class="line">    ( productkey, event_name, year_month_day),</span><br><span class="line">    ( productkey, event_name, year_week),</span><br><span class="line">    ( productkey, event_name, year_month),</span><br><span class="line">    ( productkey, event_name, year),</span><br><span class="line">    ( productkey, event_name),</span><br><span class="line">    ( event_name, year_month_day),</span><br><span class="line">    ( event_name, year_week),</span><br><span class="line">    ( event_name, year_month),</span><br><span class="line">    ( event_name, year),</span><br><span class="line">    ( event_name)</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></li>
<li>原因：奇怪的bug，原因未知。不管是hive本地客户端还是datagrip连接hiveserver2，显示的结果是错误的，但是如果对该字段进行计算或筛选，会发现最终的计算结果时正确的；如果去掉GROUPING SETS中的聚合条件只剩下一个，最终结果也是正确的。但是在某些条件下还是会影响结果正确性。</li>
<li>解决：修改为 <code>when year_month_day is not null then date_add(year_month_day,0)</code></li>
</ul>
<h3 id="踩坑六十四"><a href="#踩坑六十四" class="headerlink" title="踩坑六十四"></a>踩坑六十四</h3><ul>
<li>问题：执行sql如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">SELECT tmp_event_count.productkey,</span><br><span class="line">       tmp_event_count.iotid,</span><br><span class="line">       tmp_event_count.devicename,</span><br><span class="line">       tmp_event_count.event_name,</span><br><span class="line">       tmp_event_count.opt_id,</span><br><span class="line">       tmp_event_count.opt_name,</span><br><span class="line">       tmp_event_count.event_num,</span><br><span class="line">       tmp_event_count.year_month_day,</span><br><span class="line">       tmp_event_count.year_week,</span><br><span class="line">       tmp_event_count.year_month,</span><br><span class="line">       tmp_event_count.year,</span><br><span class="line">       tmp_event_count.date_end,</span><br><span class="line">       tmp_device_uv.device_uv</span><br><span class="line">FROM tmp_event_count</span><br><span class="line">         LEFT JOIN</span><br><span class="line">     tmp_device_uv</span><br><span class="line">     ON tmp_event_count.event_name = tmp_device_uv.event_name</span><br><span class="line">          and NVL(tmp_event_count.productkey, &#x27;NULL&#x27;) = NVL(tmp_device_uv.productkey, &#x27;NULL&#x27;)</span><br><span class="line">         and tmp_event_count.date_type = tmp_device_uv.date_type</span><br><span class="line">         and tmp_event_count.date_end = tmp_device_uv.date_end</span><br><span class="line">         and tmp_event_count.iotid is null</span><br><span class="line">         and tmp_event_count.opt_id is null</span><br></pre></td></tr></table></figure>
会发现结果如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">NULL,,,ErrorCode,,,135,2022-06-18,,,,2022-06-18,88</span><br><span class="line">NULL,,,StOvMode,,,115,2022-06-18,,,,2022-06-18,70</span><br><span class="line">,,,MultiStageName,,,15,2022-06-18,,,,2022-06-18,14</span><br><span class="line">a17JZbZVctc,,,MultiStageName,,,15,2022-06-18,,,,2022-06-18,14</span><br><span class="line">,,,StOvMode,,,115,2022-06-18,,,,2022-06-18,70</span><br><span class="line">,,,ErrorCode,,,135,2022-06-18,,,,2022-06-18,88</span><br></pre></td></tr></table></figure>
可以发现有部分productkey被替换为了连接条件中的”NULL”，</li>
<li>原因：应该时bug，同if和case when一样，这些条件不能用在连接条件ON里面</li>
<li>解决：先对表中的列进行处理(如将null值替换为空串)，然后用处理后的列进行连接。</li>
</ul>
<h3 id="踩坑六十五"><a href="#踩坑六十五" class="headerlink" title="踩坑六十五"></a>踩坑六十五</h3><ul>
<li>问题：sql中使用full outer join时，莫名其妙多了一条数据，且该数据时脏数据<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">WITH tmp_order AS (</span><br><span class="line">    SELECT date_range_type,</span><br><span class="line">           date_range,</span><br><span class="line">           smart_product_code,</span><br><span class="line">           sum(sale_num)       as order_num,</span><br><span class="line">           sum(send_stock_out) as stock_out_num</span><br><span class="line">    FROM (</span><br><span class="line">             SELECT smart_product_code, smart_product_name, product_code</span><br><span class="line">             FROM compass_dim.dim_feiyan_smart_product</span><br><span class="line">             WHERE ds = &#x27;2022-06-19&#x27;</span><br><span class="line">         ) tmp_smart_product</span><br><span class="line">             INNER JOIN</span><br><span class="line">         (</span><br><span class="line">             SELECT data[&#x27;date_range_type&#x27;]                        as date_range_type,</span><br><span class="line">                    data[&#x27;date_range&#x27;]                             as date_range,</span><br><span class="line">                    cast(nvl(data[&#x27;sale_num&#x27;], 0) as bigint)       as sale_num,</span><br><span class="line">                    cast(nvl(data[&#x27;send_stock_out&#x27;], 0) as bigint) as send_stock_out,</span><br><span class="line">                    cinvccode</span><br><span class="line">             FROM (</span><br><span class="line">                      SELECT cinvccode,</span><br><span class="line">                             ds,</span><br><span class="line">                             sale_num_day,</span><br><span class="line">                             send_stock_out_day,</span><br><span class="line">                             sale_num_week,</span><br><span class="line">                             send_stock_out_week,</span><br><span class="line">                             sale_num_month,</span><br><span class="line">                             send_stock_out_month,</span><br><span class="line">                             sale_num_year,</span><br><span class="line">                             send_stock_out_year</span><br><span class="line">                      FROM compass_dwt.dwt_so_paid_order_topic</span><br><span class="line">                      WHERE ds = &#x27;2022-06-19&#x27;</span><br><span class="line">                        and cinvcode is null</span><br><span class="line">                        and cinvccode is not null</span><br><span class="line">                        and class_id is null</span><br><span class="line">                  ) tmp_order LATERAL VIEW explode(</span><br><span class="line">                     array(map(&#x27;date_range_type&#x27;, &#x27;3&#x27;, &#x27;date_range&#x27;, ds, &#x27;sale_num&#x27;, sale_num_day,</span><br><span class="line">                               &#x27;send_stock_out&#x27;, send_stock_out_day),</span><br><span class="line">                           map(&#x27;date_range_type&#x27;, &#x27;2&#x27;,</span><br><span class="line">                               &#x27;date_range&#x27;, concat(year(date_add(next_day(ds, &#x27;MO&#x27;), -4)), &#x27;-&#x27;, weekofyear(ds)),</span><br><span class="line">                               &#x27;sale_num&#x27;, sale_num_week, &#x27;send_stock_out&#x27;, send_stock_out_week),</span><br><span class="line">                           map(&#x27;date_range_type&#x27;, &#x27;1&#x27;, &#x27;date_range&#x27;, date_format(ds, &#x27;yyyy-MM&#x27;)</span><br><span class="line">                               , &#x27;sale_num&#x27;, sale_num_month, &#x27;send_stock_out&#x27;, send_stock_out_month),</span><br><span class="line">                           map(&#x27;date_range_type&#x27;, &#x27;0&#x27;, &#x27;date_range&#x27;, year(ds), &#x27;sale_num&#x27;, sale_num_year,</span><br><span class="line">                               &#x27;send_stock_out&#x27;,</span><br><span class="line">                               send_stock_out_year))</span><br><span class="line">                 ) tmp as data</span><br><span class="line">             UNION ALL</span><br><span class="line">             SELECT &#x27;4&#x27;                         as date_range_type,</span><br><span class="line">                    &#x27;total&#x27;                     as date_range,</span><br><span class="line">                    nvl(sale_num_year, 0)       as sale_num,</span><br><span class="line">                    nvl(send_stock_out_year, 0) as send_stock_out_year,</span><br><span class="line">                    cinvccode</span><br><span class="line">             FROM compass_dwt.dwt_so_paid_order_topic</span><br><span class="line">             WHERE (ds = concat(year(ds), &#x27;-12-31&#x27;) or ds = &#x27;2022-06-19&#x27;)</span><br><span class="line">               and cinvcode is null</span><br><span class="line">               and cinvccode is not null</span><br><span class="line">               and class_id is null</span><br><span class="line">         ) tmp_order</span><br><span class="line">         ON tmp_smart_product.product_code = tmp_order.cinvccode</span><br><span class="line">    GROUP BY smart_product_code, date_range_type, date_range</span><br><span class="line">)</span><br><span class="line">   , tmp_install AS (</span><br><span class="line">    SELECT 4             as date_range_type,</span><br><span class="line">           &#x27;total&#x27;       as date_range,</span><br><span class="line">           &#x27;a17JZbZVctc&#x27; as smart_product_code,</span><br><span class="line">           0             as install_num</span><br><span class="line">)</span><br><span class="line">   , tmp_app_bind AS (</span><br><span class="line">    SELECT 4             as date_range_type,</span><br><span class="line">           &#x27;total&#x27;       as date_range,</span><br><span class="line">           &#x27;a17JZbZVctc&#x27; as smart_product_code,</span><br><span class="line">           0             as binding_num</span><br><span class="line">)</span><br><span class="line">INSERT OVERWRITE TABLE compass_ads.ads_cb_hifun_device_info</span><br><span class="line">SELECT coalesce(tmp_order.date_range_type, tmp_app_bind.date_range_type, tmp_install.date_range_type)</span><br><span class="line">           as date_ranger_type,</span><br><span class="line">       coalesce(tmp_order.date_range, tmp_app_bind.date_range, tmp_install.date_range)</span><br><span class="line">           as date_ranger_type,</span><br><span class="line">       coalesce(tmp_order.smart_product_code, tmp_app_bind.smart_product_code, tmp_install.smart_product_code)</span><br><span class="line">           as date_ranger_type,</span><br><span class="line">       nvl(order_num, 0) as order_num,</span><br><span class="line">       nvl(stock_out_num, 0) as stock_out_num,</span><br><span class="line">       nvl(install_num, 0) as install_num,</span><br><span class="line">       nvl(binding_num, 0) as binding_num</span><br><span class="line">FROM tmp_order</span><br><span class="line">         FULL OUTER JOIN tmp_app_bind</span><br><span class="line">                         ON tmp_order.date_range = tmp_app_bind.date_range</span><br><span class="line">                             and tmp_order.date_range_type = tmp_app_bind.date_range_type</span><br><span class="line">                             and tmp_order.smart_product_code = tmp_app_bind.smart_product_code</span><br><span class="line">         FULL OUTER JOIN tmp_install</span><br><span class="line">                         ON tmp_order.date_range = tmp_install.date_range</span><br><span class="line">                             and tmp_order.date_range_type = tmp_install.date_range_type</span><br><span class="line">                             and tmp_order.smart_product_code = tmp_install.smart_product_code;</span><br></pre></td></tr></table></figure>
结果为<img src="/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982.assets%5C463d180020ce4128b7877396b9be08a4.png" alt="image.png"><br>而实际结果为<img src="/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982.assets%5C3b6479abedb642b99f33b908cbfdf1c3.png" alt="image.png"></li>
<li>原因：当关闭mapjoin后，结果正常。推断时mapjoin的bug。</li>
<li>解决：关闭mapjoin <code>set hive.merge.mapredfiles=false;</code></li>
</ul>
<h3 id="踩坑六十七"><a href="#踩坑六十七" class="headerlink" title="踩坑六十七"></a>踩坑六十七</h3><ul>
<li>问题：namenode高可用配置后，运行了6个小时，active namenode转为standby，但是没有选举出新的active namenode节点<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">2023-04-21 09:15:34,484 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode</span><br><span class="line">2023-04-21 09:15:34,493 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn2, ipcAddress=cos-bigdata-test-flink-02/192.168.101.194:8020, httpAddress=http://cos-bigdata-test-flink-02:9870], try next.</span><br><span class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error</span><br><span class="line">2023-04-21 09:15:34,497 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn3, ipcAddress=cos-bigdata-test-flink-03/192.168.101.195:8020, httpAddress=http://cos-bigdata-test-flink-03:9870], try next.</span><br><span class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error</span><br><span class="line">2023-04-21 09:15:34,504 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN</span><br><span class="line">java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!</span><br><span class="line">	at java.util.concurrent.FutureTask.report(FutureTask.java:122)</span><br><span class="line">	at java.util.concurrent.FutureTask.get(FutureTask.java:206)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:416)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:475)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$300(EditLogTailer.java:441)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:458)</span><br><span class="line">	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:454)</span><br><span class="line">Caused by: java.io.IOException: Cannot find any valid remote NN to service request!</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:573)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure></li>
<li>原因：未找到原因</li>
<li>解决：关闭hdfs集群<code>stop-dfs.sh</code>，格式化zookeeper <code>hdfs zkfc -formatZK</code>，重启hdfs集群 <code>start-dfs.sh</code>后正常。此时再kill掉namenode后可以正常进行故障转移。</li>
</ul>
<h3 id="踩坑六十八"><a href="#踩坑六十八" class="headerlink" title="踩坑六十八"></a>踩坑六十八</h3><ul>
<li>现象：在运行几周后，101.194节点上的namenode掉线，报错如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">2023-04-30 02:48:42,880 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Took 2112ms to send a batch of 1 edits (17 bytes) to remote journal 192.168.101.193:8485</span><br><span class="line">2023-04-30 02:48:45,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem write lock held for 7419 ms via java.lang.Thread.getStackTrace(Thread.java:1559)</span><br><span class="line">org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032)</span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:263)</span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:225)</span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1624)</span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4692)</span><br><span class="line">org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1311)</span><br><span class="line">org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)</span><br><span class="line">org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)</span><br><span class="line">org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)</span><br><span class="line">org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)</span><br><span class="line">org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)</span><br><span class="line">org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)</span><br><span class="line">java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)</span><br><span class="line">org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)</span><br><span class="line">	Number of suppressed write-lock reports: 0</span><br><span class="line">	Longest write-lock held interval: 7419.0 </span><br><span class="line">	Total suppressed write-lock held time: 0.0</span><br><span class="line">2023-04-30 02:56:52,890 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 6001 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.</span><br><span class="line">2023-04-30 02:56:53,892 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 7002 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.</span><br><span class="line">2023-04-30 02:57:05,904 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 19015 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.</span><br><span class="line">2023-04-30 02:57:06,891 FATAL org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: starting log segment 578736 failed for required journal (JournalAndStream(mgr=QJM to [192.168.101.193:8485, 192.168.101.194:8485, 192.168.101.195:8485], stream=null))</span><br><span class="line">java.io.IOException: Timed out waiting 20000ms for a quorum of nodes to respond.</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138)</span><br><span class="line">	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.startLogSegment(QuorumJournalManager.java:436)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream.startLogSegment(JournalSet.java:95)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.JournalSet$1.apply(JournalSet.java:210)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:385)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.JournalSet.startLogSegment(JournalSet.java:207)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1385)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegmentAndWriteHeaderTxn(FSEditLog.java:1397)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog(FSEditLog.java:1321)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog(FSImage.java:1367)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4690)</span><br><span class="line">	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1311)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)</span><br><span class="line">	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)</span><br><span class="line">	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)</span><br><span class="line">	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)</span><br><span class="line">	at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">	at javax.security.auth.Subject.doAs(Subject.java:422)</span><br><span class="line">	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)</span><br><span class="line">	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)</span><br><span class="line">2023-04-30 02:57:06,898 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: Error: starting log segment 578736 failed for required journal (JournalAndStream(mgr=QJM to [192.168.101.193:8485, 192.168.101.194:8485, 192.168.101.195:8485], stream=null))</span><br><span class="line">2023-04-30 02:57:06,906 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: FSImageSaver clean checkpoint: txid = 99507 when meet shutdown.</span><br><span class="line">2023-04-30 02:57:06,922 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: FSImageSaver clean checkpoint: txid = 143829 when meet shutdown.</span><br></pre></td></tr></table></figure>
其中193的journa日志如下<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2023-04-30 02:57:08,532 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on default port 8485, call Call#176302 Retry#0 org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.startLogSegment from 192.168.101.194:57076: output error</span><br><span class="line">2023-04-30 02:57:12,834 WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log EditLogFile(file=/opt/module/hadoop/journal/hdfscluster/current/edits_inprogress_0000000000000578736,first=0000000000000578736,last=-000000000000012345,inProgress=true,hasCorruptHeader=false) has no transactions. moving it aside and looking for previous log ; journal id: hdfscluster</span><br></pre></td></tr></table></figure></li>
<li>原因：</li>
<li>解决：</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">CJ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982/">http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E9%9B%86%E7%BE%A4%E9%97%AE%E9%A2%982/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Hexo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E5%85%A8%E8%84%9A%E6%9C%AC/" title="离线数仓全脚本"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">离线数仓全脚本</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E6%8A%80%E6%9C%AF%E9%97%AE%E9%A2%98/" title="技术问题"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">技术问题</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJ</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%80"><span class="toc-number">1.</span> <span class="toc-text">踩坑一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C"><span class="toc-number">2.</span> <span class="toc-text">踩坑二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89"><span class="toc-number">3.</span> <span class="toc-text">踩坑三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B"><span class="toc-number">4.</span> <span class="toc-text">踩坑四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94"><span class="toc-number">5.</span> <span class="toc-text">踩坑五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD"><span class="toc-number">6.</span> <span class="toc-text">踩坑六</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%83"><span class="toc-number">7.</span> <span class="toc-text">踩坑七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AB"><span class="toc-number">8.</span> <span class="toc-text">踩坑八</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B9%9D"><span class="toc-number">9.</span> <span class="toc-text">踩坑九</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81"><span class="toc-number">10.</span> <span class="toc-text">踩坑十</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%B8%80%EF%BC%9A"><span class="toc-number">11.</span> <span class="toc-text">踩坑十一：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%BA%8C"><span class="toc-number">12.</span> <span class="toc-text">踩坑十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%B8%89"><span class="toc-number">13.</span> <span class="toc-text">踩坑十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E5%9B%9B"><span class="toc-number">14.</span> <span class="toc-text">踩坑十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%BA%94"><span class="toc-number">15.</span> <span class="toc-text">踩坑十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%B8%83"><span class="toc-number">16.</span> <span class="toc-text">踩坑十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E5%85%AB"><span class="toc-number">17.</span> <span class="toc-text">踩坑十八</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%8D%81%E4%B9%9D"><span class="toc-number">18.</span> <span class="toc-text">踩坑十九</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81"><span class="toc-number">19.</span> <span class="toc-text">踩坑二十</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%B8%80"><span class="toc-number">20.</span> <span class="toc-text">踩坑二十一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%BA%8C"><span class="toc-number">21.</span> <span class="toc-text">踩坑二十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%B8%89"><span class="toc-number">22.</span> <span class="toc-text">踩坑二十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E5%9B%9B"><span class="toc-number">23.</span> <span class="toc-text">踩坑二十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%BA%94"><span class="toc-number">24.</span> <span class="toc-text">踩坑二十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E5%85%AD"><span class="toc-number">25.</span> <span class="toc-text">踩坑二十六</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%B8%83"><span class="toc-number">26.</span> <span class="toc-text">踩坑二十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E5%85%AB"><span class="toc-number">27.</span> <span class="toc-text">踩坑二十八</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%8C%E5%8D%81%E4%B9%9D"><span class="toc-number">28.</span> <span class="toc-text">踩坑二十九</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81"><span class="toc-number">29.</span> <span class="toc-text">踩坑三十</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%B8%80"><span class="toc-number">30.</span> <span class="toc-text">踩坑三十一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%BA%8C"><span class="toc-number">31.</span> <span class="toc-text">踩坑三十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%B8%89"><span class="toc-number">32.</span> <span class="toc-text">踩坑三十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E5%9B%9B"><span class="toc-number">33.</span> <span class="toc-text">踩坑三十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%BA%94"><span class="toc-number">34.</span> <span class="toc-text">踩坑三十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E5%85%AD"><span class="toc-number">35.</span> <span class="toc-text">踩坑三十六</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%B8%83"><span class="toc-number">36.</span> <span class="toc-text">踩坑三十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E5%85%AB"><span class="toc-number">37.</span> <span class="toc-text">踩坑三十八</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%B8%89%E5%8D%81%E4%B9%9D"><span class="toc-number">38.</span> <span class="toc-text">踩坑三十九</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81"><span class="toc-number">39.</span> <span class="toc-text">踩坑四十</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E4%B8%80"><span class="toc-number">40.</span> <span class="toc-text">踩坑四十一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E4%BA%8C"><span class="toc-number">41.</span> <span class="toc-text">踩坑四十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E4%B8%89"><span class="toc-number">42.</span> <span class="toc-text">踩坑四十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E5%9B%9B"><span class="toc-number">43.</span> <span class="toc-text">踩坑四十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E4%BA%94"><span class="toc-number">44.</span> <span class="toc-text">踩坑四十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E5%85%AD"><span class="toc-number">45.</span> <span class="toc-text">踩坑四十六</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%9B%9B%E5%8D%81%E4%B8%83"><span class="toc-number">46.</span> <span class="toc-text">踩坑四十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E4%B8%80"><span class="toc-number">47.</span> <span class="toc-text">踩坑五十一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E4%BA%8C"><span class="toc-number">48.</span> <span class="toc-text">踩坑五十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E4%B8%89"><span class="toc-number">49.</span> <span class="toc-text">踩坑五十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E5%9B%9B"><span class="toc-number">50.</span> <span class="toc-text">踩坑五十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E4%BA%94"><span class="toc-number">51.</span> <span class="toc-text">踩坑五十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E5%85%AD"><span class="toc-number">52.</span> <span class="toc-text">踩坑五十六</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E4%B8%83"><span class="toc-number">53.</span> <span class="toc-text">踩坑五十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E4%BA%94%E5%8D%81%E5%85%AB"><span class="toc-number">54.</span> <span class="toc-text">踩坑五十八</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E4%B8%80"><span class="toc-number">55.</span> <span class="toc-text">踩坑六十一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E4%BA%8C"><span class="toc-number">56.</span> <span class="toc-text">踩坑六十二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E4%B8%89"><span class="toc-number">57.</span> <span class="toc-text">踩坑六十三</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E5%9B%9B"><span class="toc-number">58.</span> <span class="toc-text">踩坑六十四</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E4%BA%94"><span class="toc-number">59.</span> <span class="toc-text">踩坑六十五</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E4%B8%83"><span class="toc-number">60.</span> <span class="toc-text">踩坑六十七</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B8%A9%E5%9D%91%E5%85%AD%E5%8D%81%E5%85%AB"><span class="toc-number">61.</span> <span class="toc-text">踩坑六十八</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/MySQL/%E6%B3%A8%E8%A7%A3@Select%E5%92%8C@Insert/" title="注解@Select和@Insert">注解@Select和@Insert</a><time datetime="2023-05-06T05:48:28.906Z" title="发表于 2023-05-06 13:48:28">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/%E6%B3%A8%E8%A7%A3@EnableAutoConfiguration/" title="注解@EnableAutoConfiguration">注解@EnableAutoConfiguration</a><time datetime="2023-05-06T05:48:06.027Z" title="发表于 2023-05-06 13:48:06">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E6%A1%86%E6%9E%B6/" title="大数据集群监控框架">大数据集群监控框架</a><time datetime="2023-05-06T05:42:56.298Z" title="发表于 2023-05-06 13:42:56">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/HashMap%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98%E5%8F%8AConcurrentHashMap%E5%8E%9F%E7%90%86/" title="HashMap并发问题及ConcurrentHashMap原理">HashMap并发问题及ConcurrentHashMap原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/Stream%E5%8E%9F%E7%90%86/" title="Stream原理">Stream原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By CJ</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>