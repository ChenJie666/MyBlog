<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark内核解析 | Hexo</title><meta name="author" content="CJ"><meta name="copyright" content="CJ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="rdd和pairrdd：rdd中保存的是v、双v和action类型的函数；pairRDD中保存的是k-v类型的函数。rdd调用k-v类型的函数时会通过RDD的伴生对象中的隐式转换函数implicit def rddToPairRDDFunctions进行二次编译。 spark的url	spark：&#x2F;&#x2F;host：port 类加载器，加载类和资源（配置文件等） 反射调用方法只是普通">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark内核解析">
<meta property="og:url" content="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="rdd和pairrdd：rdd中保存的是v、双v和action类型的函数；pairRDD中保存的是k-v类型的函数。rdd调用k-v类型的函数时会通过RDD的伴生对象中的隐式转换函数implicit def rddToPairRDDFunctions进行二次编译。 spark的url	spark：&#x2F;&#x2F;host：port 类加载器，加载类和资源（配置文件等） 反射调用方法只是普通">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-05-06T05:31:21.051Z">
<meta property="article:modified_time" content="2023-05-06T05:31:21.051Z">
<meta property="article:author" content="CJ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark内核解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-05-06 13:31:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Hexo"><span class="site-name">Hexo</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark内核解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-06T05:31:21.051Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-06T05:31:21.051Z" title="更新于 2023-05-06 13:31:21">2023-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/">大数据实时</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark内核解析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>rdd和pairrdd：rdd中保存的是v、双v和action类型的函数；pairRDD中保存的是k-v类型的函数。rdd调用k-v类型的函数时会通过RDD的伴生对象中的隐式转换函数implicit def rddToPairRDDFunctions进行二次编译。</p>
<p>spark的url	spark：&#x2F;&#x2F;host：port</p>
<p>类加载器，加载类和资源（配置文件等）</p>
<p>反射调用方法只是普通调用，不会产生线程和进程，进程是开启一个新的类，线程是thread.start()开启。</p>
<p>多线程的锁是监听器monitor</p>
<p>后台用于处理数据和通讯</p>
<p>spark shell 不能用集群模式</p>
<p>Driver是运行在AM主方法进程上的线程。Driver线程中会执行方法创建上下文。上下文的类可以理解为Driver。</p>
<p>本地化级别：进程本地化（） 节点本地化（） 机架本地化（）</p>
<p>RDD：首选位置</p>
<p>ExecutorBackend，通讯终端的名字是Executor。计算器是Executor对象。</p>
<p>NettyRpcEnv</p>
<p>Dispatcher：调度器</p>
<p>RpcEndpoint：终端的生命周期为constructor -&gt; onStart -&gt; receive* -&gt; onStop</p>
<p>内核术语：</p>
<p>RPC：JVM进程之间进行交互的方式；rpcEvn 环境对象</p>
<p>inbox：收件箱</p>
<p>backend：后台（处理数据和通讯）<br>endpoint：终端（集群中的节点都是终端）</p>
<p>终端是内部通讯，后台与其他交互。</p>
<p>每个shuffleMapTask和resultTask都有读和写的操作。shuffleMapTask的写方法在类中，resultTask会读取RDD并计算。</p>
<p>分为三部分①整体架构②组件的通讯方式③任务的调度、分配和执行</p>
<p>源码解析：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">向yarn提交了一个任务：</span><br><span class="line">bin/spark-submit \</span><br><span class="line">--<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">examples</span>.<span class="title">SparkPi</span> <span class="title">\</span></span></span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode client \</span><br><span class="line">./examples/jars/spark-examples_2<span class="number">.11</span><span class="number">-2.1</span><span class="number">.1</span>.jar \</span><br><span class="line"><span class="number">100</span></span><br><span class="line"></span><br><span class="line">执行spark-submit脚本，实际开启了一个java虚拟机，执行了<span class="type">SparkSubmit</span>类的main方法，产生sparksubmit进程</span><br><span class="line">exec <span class="string">&quot;$&#123;SPARK_HOME&#125;&quot;</span>/bin/spark-<span class="class"><span class="keyword">class</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">deploy</span>.<span class="title">SparkSubmit</span> &quot;<span class="title">$@</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="type">TODO</span> 执行<span class="type">SparkSubmit</span>类的main方法：</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)<span class="comment">//内部通过parse(args.asJava)调用handle方法，handle方法中通过模式匹配将参数赋值给对象属性，将属性封装为SparkSubmitArguments对象</span></span><br><span class="line"></span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="comment">//内部通过action = Option(action).getOrElse(SUBMIT)给action赋值为SUBMIT</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)<span class="comment">//执行submit方法，传入封装的参数</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="type">TODO</span> 执行submit方法</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)	<span class="comment">//将参数进行封装处理，其中childMainClass后面会用到</span></span><br><span class="line">    doRunMain()	<span class="comment">//如果不是单机模式，调用doRunMain方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        ...</span><br><span class="line">        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)<span class="comment">//最后会调用runMain方法</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="type">TODO</span> 执行runMain方法：</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(......): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)<span class="comment">//装载类加载器</span></span><br><span class="line">        addJarToClasspath(jar, loader)<span class="comment">//加载资源</span></span><br><span class="line">        <span class="comment">//通过反射获取childMainClass的main方法，然后调用main方法</span></span><br><span class="line">        mainClass = <span class="type">Utils</span>.classForName(childMainClass)</span><br><span class="line">        <span class="keyword">val</span> mainMethod = mainClass.getMethod(<span class="string">&quot;main&quot;</span>, <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">0</span>).getClass)</span><br><span class="line">        mainMethod.invoke(<span class="literal">null</span>, childArgs.toArray)</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"><span class="type">TODO</span> prepareSubmitEnvironment方法封装childMainClass</span><br><span class="line">    <span class="keyword">private</span>[deploy] <span class="function"><span class="keyword">def</span> <span class="title">prepareSubmitEnvironment</span></span>(args: <span class="type">SparkSubmitArguments</span>)｛</span><br><span class="line">    	<span class="keyword">var</span> childMainClass = <span class="string">&quot;&quot;</span></span><br><span class="line">    	<span class="keyword">if</span> (deployMode == <span class="type">CLIENT</span> || isYarnCluster) &#123;childMainClass = args.mainClass&#125;</span><br><span class="line">    	<span class="keyword">if</span> (isYarnCluster) &#123;childMainClass = <span class="string">&quot;org.apache.spark.deploy.yarn.Client&quot;</span>&#125;</span><br><span class="line">    	(childArgs, childClasspath, sysProps, childMainClass)	<span class="comment">//作为返回值返回</span></span><br><span class="line">	&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">如果是client模式，childMainClass就是args.mainClass，在SparkSubmitArguments类中进行模式匹配：</span></span><br><span class="line"><span class="comment">      case CLASS =&gt;</span></span><br><span class="line"><span class="comment">        mainClass = value</span></span><br><span class="line"><span class="comment">CLASS是“--class”，将命令行中的给定的类传给mainClass；</span></span><br><span class="line"><span class="comment">如果是cluster模式，childMainClass就是&quot;org.apache.spark.deploy.yarn.Client&quot;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">所以client模式一上来就运行给定的任务，而cluster模式会先运行client类的main方法。</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">添加依赖：</span><br><span class="line">&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">&lt;artifactId&gt;spark-yarn_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">&lt;version&gt;<span class="number">2.1</span><span class="number">.1</span>&lt;/version&gt;</span><br><span class="line"><span class="comment">//进入Client类：</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">Client</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">        <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">ClientArguments</span>(argStrings)<span class="comment">//将参数封装为一个对象</span></span><br><span class="line">    	<span class="keyword">new</span> <span class="type">Client</span>(args, sparkConf).run()<span class="comment">//新建yarnclient对象并运行run方法</span></span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> amMemory = ...</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> amMemoryOverhead = ...</span><br><span class="line">        <span class="keyword">private</span> <span class="keyword">val</span> amCores = ...<span class="comment">//同时还配置了ApplicationMaster的一些属性</span></span><br><span class="line">  	&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//调用主构造函数会执行构造函数中的可执行语句，新建了yarn的客户端yarnClient</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">Client</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    val args: <span class="type">ClientArguments</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val hadoopConf: <span class="type">Configuration</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val sparkConf: <span class="type">SparkConf</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Logging</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> yarnClient = <span class="type">YarnClient</span>.createYarnClient</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> yarnConf = <span class="keyword">new</span> <span class="type">YarnConfiguration</span>(hadoopConf)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">yarnClient继承自YarnClientImpl，也继承了YarnClientImpl中的rmClient和rmAddress，可以认为该yarnClient对象中包含了ResourceManager的url地址，可以与RM进行通讯。</span></span><br><span class="line"><span class="comment">  protected ApplicationClientProtocol rmClient;</span></span><br><span class="line"><span class="comment">  protected InetSocketAddress rmAddress;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//进入new Client(args, sparkConf).run()方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">this</span>.appId = submitApplication()<span class="comment">//提交用户应用</span></span><br><span class="line">    <span class="keyword">val</span> report = getApplicationReport(appId)<span class="comment">//等待提交的返回结果用于展示</span></span><br><span class="line">    <span class="keyword">val</span> state = report.getYarnApplicationState</span><br><span class="line">	<span class="keyword">val</span> (yarnApplicationState, finalApplicationStatus) = monitorApplication(appId)<span class="comment">//监听作业的状态</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入submitApplication（）方法</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(): <span class="type">ApplicationId</span> = &#123;</span><br><span class="line">	  yarnClient.init(yarnConf)<span class="comment">//yarnClient即是上面创建的yarn客户端对象</span></span><br><span class="line">      yarnClient.start()<span class="comment">//与yarn进行联通</span></span><br><span class="line">      <span class="comment">//创建提交的appContext</span></span><br><span class="line">      <span class="keyword">val</span> containerContext = createContainerLaunchContext(newAppResponse)</span><br><span class="line">      <span class="keyword">val</span> appContext = createApplicationSubmissionContext(newApp, containerContext)</span><br><span class="line">      yarnClient.submitApplication(appContext)<span class="comment">//真正向yarn提交应用</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入yarnClient的submitApplication（）方法</span></span><br><span class="line">public <span class="type">ApplicationId</span> submitApplication（<span class="type">ApplicationSubmissionContext</span> appContext）&#123;</span><br><span class="line">    rmClient.submitApplication(request);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入创建提交对象appContext的createContainerLaunchContext()方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createContainerLaunchContext</span></span>(newAppResponse: <span class="type">GetNewApplicationResponse</span>)= &#123;</span><br><span class="line">	<span class="keyword">val</span> amClass =</span><br><span class="line">      <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">        <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ApplicationMaster&quot;</span>).getName</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Utils</span>.classForName(<span class="string">&quot;org.apache.spark.deploy.yarn.ExecutorLauncher&quot;</span>).getName</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> amArgs =</span><br><span class="line">      <span class="type">Seq</span>(amClass) ++ userClass ++ userJar ++ primaryPyFile ++ primaryRFile ++ userArgs ++ <span class="type">Seq</span>( <span class="string">&quot;--properties-file&quot;</span>, buildPath(<span class="type">YarnSparkHadoopUtil</span>.expandEnvironment(<span class="type">Environment</span>.<span class="type">PWD</span>) , <span class="type">LOCALIZED_CONF_DIR</span>, <span class="type">SPARK_CONF_FILE</span>))</span><br><span class="line">	<span class="keyword">val</span> commands = prefixEnv ++ <span class="type">Seq</span>(</span><br><span class="line">        <span class="type">YarnSparkHadoopUtil</span>.expandEnvironment(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>) + <span class="string">&quot;/bin/java&quot;</span>, <span class="string">&quot;-server&quot;</span> ) ++ javaOpts ++ amArgs ++</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*上面的方法拼接了一个command指令，发往ResourceManager节点，</span></span><br><span class="line"><span class="comment">如果是cluster模式，拼接为/bin/java  org.apache.spark.deploy.yarn.ApplicationMaster启动进程</span></span><br><span class="line"><span class="comment">如果不是cluster模式，拼接为/bin/java  org.apache.spark.deploy.yarn.ExecutorLauncher启动进程</span></span><br><span class="line"><span class="comment">所以启动sparkshell，shell只能启动client模式，所以可以jps看到有ExecutorLauncher进程</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">尽管client启动了ExecutorLauncher类的main方法，ExecutorLauncher，但是main方法会调用ApplicationMaster的main方法，两者效果一样，进程名字不同。</span></span><br><span class="line"><span class="comment">  def main(args: Array[String]): Unit = &#123;</span></span><br><span class="line"><span class="comment">    ApplicationMaster.main(args)</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">将拼接的command发往<span class="type">ResourceManager</span>，<span class="type">ResourceManager</span>会在一个节点上启动<span class="type">ApplicationMaster</span>进程。</span><br></pre></td></tr></table></figure>

<p>至此，客户端的操作结束了，开始yarn的框架搭建：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//进入ApplicationMaster类</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ApplicationMaster</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> amArgs = <span class="keyword">new</span> <span class="type">ApplicationMasterArguments</span>(args)<span class="comment">//对参数的封装</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//将RMClient对象作为参数创建AppMaster对象master，那么可以和RM进行交互</span></span><br><span class="line">  <span class="type">SparkHadoopUtil</span>.get.runAsSparkUser &#123; () =&gt;</span><br><span class="line">      master = <span class="keyword">new</span> <span class="type">ApplicationMaster</span>(amArgs, <span class="keyword">new</span> <span class="type">YarnRMClient</span>)</span><br><span class="line">      <span class="type">System</span>.exit(master.run())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入ApplicationMaster对象的主构造方法</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ApplicationMaster</span>(<span class="params">args: <span class="type">ApplicationMasterArguments</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">  client: <span class="type">YarnRMClient</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> heartbeatInterval = &#123;......&#125;<span class="comment">//心跳周期</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入run方法</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> appAttemptId = client.getAttemptId()<span class="comment">//这是yarn进行任务时的全局id，即AppMaster进程的id</span></span><br><span class="line">    <span class="keyword">if</span> (isClusterMode) &#123;</span><br><span class="line">        runDriver(securityMgr)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        runExecutorLauncher(securityMgr)</span><br><span class="line">      &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*如果是cluster，执行runDriver方法，即运行Driver类，此时任务划分stage，然后生成taskset</span></span><br><span class="line"><span class="comment">如果不是，执行runExecutorLauncher方法</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//进入runExecutorLauncher方法</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runExecutorLauncher</span></span>(securityMgr: <span class="type">SecurityManager</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> port = sparkConf.getInt(<span class="string">&quot;spark.yarn.am.port&quot;</span>, <span class="number">0</span>)</span><br><span class="line">    rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">&quot;sparkYarnAM&quot;</span>, <span class="type">Utils</span>.localHostName, port, sparkConf, securityMgr,</span><br><span class="line">      clientMode = <span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">val</span> driverRef = waitForSparkDriver()</span><br><span class="line">    addAmIpFilter()</span><br><span class="line">    registerAM(sparkConf, rpcEnv, driverRef, sparkConf.get(<span class="string">&quot;spark.driver.appUIAddress&quot;</span>, <span class="string">&quot;&quot;</span>),securityMgr)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// In client mode the actor will stop the reporter thread.</span></span><br><span class="line">    reporterThread.join()</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">client模式也需要向RM注册AM，获取资源再进行分配；不同的是少了startUserApplication()方法，没有在AppMaster上启动Driver线程，因为在提交任务的节点上已经启动了Driver线程。所以client模式和cluster模式最大区别在于Driver在提交任务的节点启动还是在AppMaster上启动。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//进入runDriver方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(securityMgr: <span class="type">SecurityManager</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    userClassThread = startUserApplication()<span class="comment">//创建Driver线程，生成taskset</span></span><br><span class="line">    </span><br><span class="line">    rpcEnv = sc.env.rpcEnv<span class="comment">//创建交互环境对象</span></span><br><span class="line">    <span class="keyword">val</span> driverRef = runAMEndpoint(<span class="comment">//通过amEndpoint = rpcEnv.setupEndpoint创建AppMaster终端</span></span><br><span class="line">          sc.getConf.get(<span class="string">&quot;spark.driver.host&quot;</span>),</span><br><span class="line">          sc.getConf.get(<span class="string">&quot;spark.driver.port&quot;</span>),</span><br><span class="line">          isClusterMode = <span class="literal">true</span>)	<span class="comment">//得到Driver的引用</span></span><br><span class="line">    registerAM(sc.getConf, rpcEnv, driverRef, sc.ui.map(_.appUIAddress).getOrElse(<span class="string">&quot;&quot;</span>),</span><br><span class="line">          securityMgr)<span class="comment">//向yarn注册AppMaster</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入startUserApplication()方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startUserApplication</span></span>(): <span class="type">Thread</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> userClassLoader = ...... <span class="comment">//获取类加载器</span></span><br><span class="line">    <span class="comment">//在ApplicationMasterArguments中将--class的参数封装成了userClass属性★★★</span></span><br><span class="line">    <span class="keyword">val</span> mainMethod = userClassLoader.loadClass(args.userClass)</span><br><span class="line">      .getMethod(<span class="string">&quot;main&quot;</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line">    <span class="keyword">val</span> userThread = <span class="keyword">new</span> <span class="type">Thread</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">        	mainMethod.invoke(<span class="literal">null</span>, userArgs.toArray)<span class="comment">//调用用户提交类的main方法，生成sc对象</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    userThread.setContextClassLoader(userClassLoader)</span><br><span class="line">    userThread.setName(<span class="string">&quot;Driver&quot;</span>)<span class="comment">//命名为Driver线程</span></span><br><span class="line">    userThread.start()<span class="comment">//启动线程，执行run方法，调用用户提交类的main方法</span></span><br><span class="line">    userThread<span class="comment">//返回Driver线程，Driver线程我们可以获取</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*由上可知，我们提交的类不应该称为Driver，而是ApplicationMaster主线程中启动的一个Driver线程，所以jps看不到Driver线程；同时Driver主线程被调用，生成sparkcontext上下文对象，sc中包含了后台的通讯对象CoarseGrainedSchedulerBackend</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//进入注册AppMaster的方法registerAM</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerAM</span></span>(...)&#123;</span><br><span class="line">    <span class="comment">//client是YarnRMClient对象，表示向RM注册并传入参数，申请资源</span></span><br><span class="line">    allocator = client.register(driverUrl, </span><br><span class="line">      driverRef,</span><br><span class="line">      yarnConf,</span><br><span class="line">      _sparkConf,</span><br><span class="line">      uiAddress,</span><br><span class="line">      historyAddress,</span><br><span class="line">      securityMgr,</span><br><span class="line">      localResources)</span><br><span class="line">    allocator.allocateResources()<span class="comment">//分配资源</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入allocateResources()方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocateResources</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">	<span class="keyword">val</span> allocatedContainers = allocateResponse.getAllocatedContainers()<span class="comment">//返回可用的容器资源</span></span><br><span class="line">    <span class="keyword">if</span> (allocatedContainers.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">        handleAllocatedContainers(allocatedContainers.asScala)<span class="comment">//容器不为空，则分配资源</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入handleAllocatedContainers方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handleAllocatedContainers</span></span>(allocatedContainers: <span class="type">Seq</span>[<span class="type">Container</span>]) = &#123;</span><br><span class="line">    <span class="comment">// Match incoming requests by host</span></span><br><span class="line">    <span class="comment">// Match remaining by rack</span></span><br><span class="line">    <span class="comment">// Assign remaining that are neither node-local nor rack-local</span></span><br><span class="line">    <span class="comment">//以上进行本地化操作（进程本地化，节点本地化，机架本地化），将任务与节点关联；然后运行容器</span></span><br><span class="line">    runAllocatedContainers(containersToUse)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入runAllocatedContainers方法,为每个可用的container创建NMClient，并启动container</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runAllocatedContainers</span></span>(containersToUse: <span class="type">ArrayBuffer</span>[<span class="type">Container</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">for</span> (container &lt;- containersToUse) &#123;</span><br><span class="line">	  launcherPool.execute(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;<span class="comment">//ThreadUtils.newDaemonCachedThreadPool线程池对象</span></span><br><span class="line">   		<span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ExecutorRunnable</span>(</span><br><span class="line">                  <span class="type">Some</span>(container),</span><br><span class="line">                  conf,</span><br><span class="line">                  sparkConf,</span><br><span class="line">                  driverUrl,</span><br><span class="line">                  executorId,</span><br><span class="line">                  executorHostname,</span><br><span class="line">                  executorMemory,</span><br><span class="line">                  executorCores,</span><br><span class="line">                  appAttemptId.getApplicationId.toString,</span><br><span class="line">                  securityMgr,</span><br><span class="line">                  localResources</span><br><span class="line">                ).run()	<span class="comment">//获取NodeManager客户端，用于连接NodeManager</span></span><br><span class="line">                updateInternalState() </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;                     </span><br><span class="line"><span class="comment">//进入ExecutorRunnable对象的run方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;Starting Executor Container&quot;</span>)</span><br><span class="line">    nmClient = <span class="type">NMClient</span>.createNMClient()<span class="comment">//创建nodemanager的客户端</span></span><br><span class="line">    nmClient.init(conf)</span><br><span class="line">    nmClient.start()<span class="comment">//启动nodemanager的客户端</span></span><br><span class="line">    startContainer()<span class="comment">//启动nodemanager中的container</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入startContainer()方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startContainer</span></span>(): java.util.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">ByteBuffer</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> commands = prepareCommand()</span><br><span class="line">    ctx.setCommands(commands.asJava)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入prepareCommand()方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareCommand</span></span>(): <span class="type">List</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> commands = prefixEnv ++ <span class="type">Seq</span>(</span><br><span class="line">      <span class="type">YarnSparkHadoopUtil</span>.expandEnvironment(<span class="type">Environment</span>.<span class="type">JAVA_HOME</span>) + <span class="string">&quot;/bin/java&quot;</span>,</span><br><span class="line">      <span class="string">&quot;-server&quot;</span>) ++</span><br><span class="line">      javaOpts ++</span><br><span class="line">    <span class="comment">//执行java指令，在NodeManager上创建CoarseGrainedExecutorBackend进程</span></span><br><span class="line">      <span class="type">Seq</span>(<span class="string">&quot;org.apache.spark.executor.CoarseGrainedExecutorBackend&quot;</span>,</span><br><span class="line">        <span class="string">&quot;--driver-url&quot;</span>, masterAddress,</span><br><span class="line">        <span class="string">&quot;--executor-id&quot;</span>, executorId,</span><br><span class="line">        <span class="string">&quot;--hostname&quot;</span>, hostname,</span><br><span class="line">        <span class="string">&quot;--cores&quot;</span>, executorCores.toString,</span><br><span class="line">        <span class="string">&quot;--app-id&quot;</span>, appId) ++</span><br><span class="line">      userClassPath ++</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">最终拼成指令 /bin/java  org.apache.spark.executor.CoarseGrainedExecutorBackend，向nodemanager发送指令运行CoarseGrainedExecutorBackend类，该类是通讯后台，只做通讯，不进行计算</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>

<p>在NodeManager上创建CoarseGrainedExecutorBackend进程：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">CoarseGrainedExecutorBackend</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>])&#123;</span><br><span class="line">        run(driverUrl, executorId, hostname, cores, appId, workerUrl, userClassPath)</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入run方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(...)&#123;</span><br><span class="line">    <span class="keyword">val</span> fetcher = <span class="type">RpcEnv</span>.create(<span class="comment">//抓取环境对象的信息</span></span><br><span class="line">        <span class="string">&quot;driverPropsFetcher&quot;</span>,</span><br><span class="line">        hostname,</span><br><span class="line">        port,</span><br><span class="line">        executorConf,</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SecurityManager</span>(executorConf),</span><br><span class="line">        clientMode = <span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">val</span> driver = fetcher.setupEndpointRefByURI(driverUrl)<span class="comment">//创建driver的引用</span></span><br><span class="line">    <span class="comment">//通过创建CoarseGrainedExecutorBackend()</span></span><br><span class="line">    env.rpcEnv.setupEndpoint(<span class="string">&quot;Executor&quot;</span>, <span class="keyword">new</span> <span class="type">CoarseGrainedExecutorBackend</span>()</span><br><span class="line">&#125;<span class="comment">//将其封装为endpoint，封装时名字为executor，所以也可以称为executor</span></span><br><span class="line"><span class="comment">//进入实现类NettyRpcEnv的setupEndpoint方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupEndpoint</span></span>(name: <span class="type">String</span>, endpoint: <span class="type">RpcEndpoint</span>): <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">    dispatcher.registerRpcEndpoint(name, endpoint)<span class="comment">//向调度器注册终端程序</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">registerRpcEndpoint方法中会创建一个EndpointData（终端数据）对象，该对象中包含了一个inbox（收件箱）对象，inbox中有一个messages = new LinkedList[InboxMessage]集合，默认会messages.add(OnStart)往里面添加OnStart对象。</span></span><br><span class="line"><span class="comment">RpcEndpoint终端的生命周期为constructor -&gt; onStart -&gt; receive* -&gt; onStop，所有的终端都遵循这个生命周期</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//CoarseGrainedExecutorBackend收到onStart信息</span></span><br><span class="line">rivate[spark] <span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedExecutorBackend</span>(<span class="params">...</span>)</span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</span><br><span class="line">        rpcEnv.asyncSetupEndpointRefByURI(driverUrl).flatMap &#123; ref =&gt;</span><br><span class="line">          <span class="comment">// This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot;</span></span><br><span class="line">          driver = <span class="type">Some</span>(ref)</span><br><span class="line">          ref.ask[<span class="type">Boolean</span>](<span class="type">RegisterExecutor</span>(executorId, self, hostname, cores, extractLogUrls))<span class="comment">//向driver发送ask请求，反向注册到driver</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">RegisteredExecutor</span> =&gt; &#123; executor = <span class="keyword">new</span> <span class="type">Executor</span>(executorId, hostname, env, userClassPath, isLocal = <span class="literal">false</span>) &#125;<span class="comment">//返回注册成功信息，则创建Executor计算对象★★★</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt; &#123;executor.launchTask(<span class="keyword">this</span>, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,taskDesc.name, taskDesc.serializedTask)&#125;<span class="comment">//如果driver端通过CoarseGraineSchedulerBackend对象发送启动任务信息，则启动任务executor.launchTask</span></span><br><span class="line">&#125;</span><br><span class="line">                             </span><br><span class="line"><span class="comment">//Driver类中的SparkContext类中SchedulerBackend类对象处理这个ask请求：</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="params">config: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//查看SchedulerBackend的实现类CoarseGrainedSchedulerBackend</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedSchedulerBackend</span>(<span class="params">...</span>)</span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">DriverEndpoint</span>(<span class="params">//该终端内部类中同样有onStart、receive、onStop方法</span></span></span><br><span class="line"><span class="params"><span class="class">        override def onStart(</span>)</span>&#123;&#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive=</span></span>&#123;&#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>)= &#123;</span><br><span class="line">         ......</span><br><span class="line">         executorRef.send(<span class="type">RegisteredExecutor</span>)<span class="comment">//处理完请求后向注册的Executor发送消息</span></span><br><span class="line">        &#125;<span class="comment">//Driver启动的SparkContext上下文对象的属性中有CoarseGrainedSchedulerBackend类对象，其内部类接收CoarseGrainedExecutorBackend发送的请求并向其进行回复,收到注册成功信息则创建executor对象</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>()&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>至此，yarn中的架构建立完成，等待driver发送计算任务。</p>
<p>任务的调度和划分：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](...）&#123;</span><br><span class="line">    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入dagScheduler.runJob方法（DAGScheduler类中）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](...): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">	eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, rdd, func2, partitions.toArray, callSite, waiter,<span class="type">SerializationUtils</span>.clone(properties)))<span class="comment">//向阻塞式双端队列中放置任务</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入eventProcessLoop.post方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    eventQueue.put(event)<span class="comment">//向阻塞式双端队列中放提交的任务（封装在eventProcessLoop对象中）</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//与此相对应的，在DAGScheduler类中有onReceive方法从队列中取数据</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)<span class="comment">//从eventProcessLoop对象中取队列中的数据</span></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入doOnReceive方法，取数据并判断数据的类型</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)<span class="comment">//如果是JobSubmitted，则执行dagScheduler.handleJobSubmitted方法</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入handleJobSubmitted方法</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(...)&#123;</span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)       </span><br><span class="line">    <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)       submitStage(finalStage)<span class="comment">//提交当前阶段的作业</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入createResultStage方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(...)&#123;</span><br><span class="line">    <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)<span class="comment">//得到上级的stages</span></span><br><span class="line">    <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)<span class="comment">//将上级的stages包装到resultStage中</span></span><br><span class="line">    stage<span class="comment">//将stage返回</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">    getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">      getOrCreateShuffleMapStage(shuffleDep, firstJobId)<span class="comment">//如果匹配到一个shuffleDep，然后切分为一个stage</span></span><br><span class="line">    &#125;.toList</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入getShuffleDependencies(rdd)方法</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">      rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;<span class="comment">//rdd一定是最后一个shuffleRDD</span></span><br><span class="line">    <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]</span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    waitingForVisit.push(rdd)</span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> toVisit = waitingForVisit.pop()</span><br><span class="line">      <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">        visited += toVisit</span><br><span class="line">        toVisit.dependencies.foreach &#123;<span class="comment">//对只有一个shuffleDependency对象的集合进行遍历</span></span><br><span class="line">          <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">            parents += shuffleDep<span class="comment">//将这个shuffleDependency加入到parents集合中</span></span><br><span class="line">          <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">            waitingForVisit.push(dependency.rdd)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    parents<span class="comment">//只在while循环了一遍，parents中只有一个shuffleDependency对象</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入dependencies方法</span></span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">dependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = &#123;</span><br><span class="line">    checkpointRDD.map(r =&gt; <span class="type">List</span>(<span class="keyword">new</span> <span class="type">OneToOneDependency</span>(r))).getOrElse &#123;</span><br><span class="line">      <span class="keyword">if</span> (dependencies_ == <span class="literal">null</span>) &#123;</span><br><span class="line">        dependencies_ = getDependencies<span class="comment">//dependencies_为空，所以调用getDependencies方法</span></span><br><span class="line">      &#125;</span><br><span class="line">      dependencies_</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入getDependencies方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = &#123;</span><br><span class="line">    <span class="type">List</span>(<span class="keyword">new</span> <span class="type">ShuffleDependency</span>(prev, part, serializer, keyOrdering, aggregator, mapSideCombine))<span class="comment">//返回只有一个shuffleDependency对象的集合，该对象包含了上一个rdd的信息</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//getOrCreateShuffleMapStage方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateShuffleMapStage</span></span>(...) = &#123;</span><br><span class="line">    shuffleIdToMapStage.get(shuffleDep.shuffleId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(stage) =&gt;</span><br><span class="line">        stage</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; </span><br><span class="line">        createShuffleMapStage(shuffleDep, firstJobId)<span class="comment">//如果新建了一个stage，则递归调用createShuffleMapStage方法，通过递归，得到所有的stage。</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入createShuffleMapStage方法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>(...) : <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> rdd = shuffleDep.rdd<span class="comment">//这个rdd是shuffleRDD的上一个rdd</span></span><br><span class="line">    <span class="keyword">val</span> numTasks = rdd.partitions.length</span><br><span class="line">    <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)<span class="comment">//对rdd再次调用getOrCreateParentStages方法</span></span><br><span class="line">    <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ShuffleMapStage</span>(id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep)</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">再次调用getOrCreateParentStages，得到shuffle前最后一个rdd的所有的依赖，然后从后往前，将每一个窄依赖的rdd进行push到队列中，直到遇到宽依赖的rdd，再次创建stage；直到所有的rdd都被遍历，所有的shuffle都创建了stage；然后从队列中弹栈，因为队列中的rdd都已经visited了，所以都不执行，返回stage的集合parents。</span></span><br><span class="line"><span class="comment">与下面的getMissingParentStages的区别是，getMissingParentStages将所有的stage放到一个集合中，而上面的方法返回一个层层包裹的stage对象，最里层是最开始的stage，最外层是resultStage。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">                           </span><br><span class="line"><span class="comment">//进入handleJobSubmitted中的submitStage方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">        <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">        logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">        <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">          logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">          submitMissingTasks(stage, jobId.get)<span class="comment">//如果没有shuffle，则提交stage</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">            submitStage(parent)<span class="comment">//如果有shuffle，则遍历stage，每个stage递归调用次方法的submitMissingTasks方法提交stage</span></span><br><span class="line">          &#125;</span><br><span class="line">          waitingStages += stage</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入getMissingParentStages(stage)方法（和getOrCreateParentStages方法相同）</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="comment">// We are manually maintaining a stack here to prevent StackOverflowError</span></span><br><span class="line">    <span class="comment">// caused by recursively visiting</span></span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">        visited += rdd</span><br><span class="line">        <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">        <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">          <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">            dep <span class="keyword">match</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">                <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)<span class="comment">//通过递归调用不断将前一级创建的stage的集合合并到后一级的集合中，直到得到所有的stage，每个stage都有id号。</span></span><br><span class="line">                <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</span><br><span class="line">                  missing += mapStage</span><br><span class="line">                &#125;</span><br><span class="line">              <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</span><br><span class="line">                waitingForVisit.push(narrowDep.rdd)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    waitingForVisit.push(stage.rdd)</span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      visit(waitingForVisit.pop())</span><br><span class="line">    &#125;</span><br><span class="line">    missing.toList<span class="comment">//返回本层递归的值</span></span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//进入submitMissingTasks方法,每阶段匹配ShuffleMapStage或ResultStage，查找分区，每个分区新建一个task（DAGScheduler类）</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">    <span class="comment">//匹配stage，返回任务首选位置信息</span></span><br><span class="line">    <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">        <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">            (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">          &#125;.toMap</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//匹配stage，根据分区生成task</span></span><br><span class="line">    <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">      stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">              taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, <span class="type">Option</span>(jobId),<span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId) </span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">            <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">            <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">            <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">              taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</span><br><span class="line">              <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))<span class="comment">//将每个stage的所有任务封装为TaskSet进行提交</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入partitionsToCompute方法</span></span><br><span class="line"><span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"><span class="comment">//进入findMissingPartitions方法，返回所有的分区</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> missing = (<span class="number">0</span> until numPartitions).filter(id =&gt; outputLocs(id).isEmpty)</span><br><span class="line">    assert(missing.size == numPartitions - _numAvailableOutputs,</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;missing.size&#125;</span> missing, expected <span class="subst">$&#123;numPartitions - _numAvailableOutputs&#125;</span>&quot;</span>)</span><br><span class="line">    missing</span><br><span class="line">&#125;   </span><br><span class="line"><span class="comment">//Stage类中的numPartitions属性值</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = rdd.partitions.length<span class="comment">//所有stage都是根据最后一个rdd（即shufDep.rdd）创建出来的，所以stage的分区数是根据这个rdd的分区数来决定的</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">val parents = getOrCreateParentStages(rdd, jobId) 和 val stage = new ShuffleMapStage(id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep) 找到shuffleDep后就会根据该依赖的rdd再次调用方法找到shuffleDep，直到不再有shuffleDep；然后创建stage，返回给上一层，再创建stage，第一次stage，形参层层包裹的stage。所以创建每层stage的都是以最后一个rdd为参数的。</span></span><br><span class="line"><span class="comment">*/</span>     </span><br><span class="line">                           </span><br><span class="line"><span class="comment">//进入submitTasks方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)<span class="comment">//将taskSet包装为taskSetManager，然后加入到rootPool任务池中（schedulableBuilder具体有FIFO和Fair两种实现）</span></span><br><span class="line">    backend.reviveOffers()<span class="comment">//发送信息给自己</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入backend.reviveOffers()方法（CoarseGrainedSchedulerBackend类）</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoarseGrainedSchedulerBackend（</span>...<span class="title">）</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">DriverEndpoint</span>(<span class="params">...</span>)</span>&#123;  <span class="comment">//后台中的内部类--终端</span></span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">            makeOffers()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>() &#123;</span><br><span class="line">            driverEndpoint.send(<span class="type">ReviveOffers</span>)<span class="comment">//发送信息给自己</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//CoarseGrainedSchedulerBackend接收</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">        makeOffers()<span class="comment">//模式匹配后调用makeOffers方法</span></span><br><span class="line">&#125;               </span><br><span class="line"><span class="comment">//进入makeOffers方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>() &#123;</span><br><span class="line">   <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(executorIsAlive)</span><br><span class="line">   <span class="keyword">val</span> workOffers = activeExecutors.map &#123; <span class="keyword">case</span> (id, executorData) =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores)</span><br><span class="line">      &#125;.toIndexedSeq<span class="comment">//获取可用资源</span></span><br><span class="line">   launchTasks(scheduler.resourceOffers(workOffers))<span class="comment">//在可用资源上启动任务</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入launchTasks方法</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTask = ser.serialize(task)<span class="comment">//将任务序列化</span></span><br><span class="line">    <span class="comment">//向GoarseGrainedExecutorBackend发送用LaunchTask对象封装后的序列化任务</span></span><br><span class="line">    executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> erializableBuffer(serializedTask)))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//查看GoarseGrainedExecutorBackend的receive方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisteredExecutor</span> =&gt; executor = <span class="keyword">new</span> <span class="type">Executor</span>(executorId, hostname, env, userClassPath, isLocal = <span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt; &#123;</span><br><span class="line">    	<span class="keyword">val</span> taskDesc = ser.deserialize[<span class="type">TaskDescription</span>](data.value)<span class="comment">//反序列化任务</span></span><br><span class="line">    	executor.launchTask(<span class="keyword">this</span>, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,taskDesc.name, taskDesc.serializedTask)<span class="comment">//计算对象启动任务</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入executor.launchTask方法(Executor类)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(...)&#123;</span><br><span class="line">    <span class="keyword">val</span> taskDesc = ser.deserialize[<span class="type">TaskDescription</span>](data.value)</span><br><span class="line">	<span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskId = taskId, attemptNumber = attemptNumber, taskName,serializedTask)</span><br><span class="line">    runningTasks.put(taskId, tr)</span><br><span class="line">    threadPool.execute(tr)<span class="comment">//调用线程池线程执行tr的run（）方法，即ShuffleMapTask和ResultTask的run方法</span></span><br><span class="line">&#125;</span><br><span class="line">                           </span><br><span class="line">                           </span><br><span class="line">                           </span><br><span class="line">                           </span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">ShuffleMapTask和ResultTask的run方法都继承自父类Task，父类的run方法又会调用子类的runTask方法。ShuffleMapTask的runTask开始执行任务，并会将结果进行写出落盘；</span></span><br><span class="line"><span class="comment">ResultTask的runTask开始执行任务，并会读取</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//进入ShuffleMapTask的runTask方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">MapStatus</span> = &#123;</span><br><span class="line">    .....</span><br><span class="line">    <span class="keyword">var</span> writer: <span class="type">ShuffleWriter</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> manager = <span class="type">SparkEnv</span>.get.shuffleManager<span class="comment">//获取环境对象中的shuffleManager，shuffleManager只有一个实现类SortShuffleManager，那么manager就是SortShuffleManager</span></span><br><span class="line">      writer = manager.getWriter[<span class="type">Any</span>, <span class="type">Any</span>](dep.shuffleHandle, partitionId, context)<span class="comment">//启动写入流</span></span><br><span class="line">      writer.write(rdd.iterator(partition, context).asInstanceOf[<span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">Any</span>, <span class="type">Any</span>]]])</span><br><span class="line">      writer.stop(success = <span class="literal">true</span>).get</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//进入manager.getWriter方法，根据传入的参数handle类型不同，采用了不同的shuffle处理方式</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">      mapId: <span class="type">Int</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    numMapsForShuffle.putIfAbsent(</span><br><span class="line">      handle.shuffleId, handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[_, _, _]].numMaps)</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">    handle <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">          context.taskMemoryManager(),</span><br><span class="line">          unsafeShuffleHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          shuffleBlockResolver.asInstanceOf[<span class="type">IndexShuffleBlockResolver</span>],</span><br><span class="line">          bypassMergeSortHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf)</span><br><span class="line">      <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;<span class="comment">//默认为BaseShuffleHandle</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(shuffleBlockResolver, other, mapId, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">如果dep.mapSideCombine为false，则可以为BypassMergeSortShuffleHandle。所以reduceByKey不能采用BypassMergeSortShuffleWriter。reduceByKey的函数中的属性mapSideCombine: Boolean = true默认值为true。创建shuffleRDD：new ShuffledRDD[K, V, C]().setMapSideCombine(mapSideCombine)会传入该属性，所以dep.mapSideCombine得到为true。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">private[spark] object SortShuffleWriter &#123;</span></span><br><span class="line"><span class="comment">  def shouldBypassMergeSort(conf: SparkConf, dep: ShuffleDependency[_, _, _]): Boolean = &#123;</span></span><br><span class="line"><span class="comment">    // We cannot bypass sorting if we need to do map-side aggregation.</span></span><br><span class="line"><span class="comment">    if (dep.mapSideCombine) &#123; //不能是会进行预聚合的算子</span></span><br><span class="line"><span class="comment">      require(dep.aggregator.isDefined, &quot;Map-side combine without Aggregator specified!&quot;)</span></span><br><span class="line"><span class="comment">      false</span></span><br><span class="line"><span class="comment">    &#125; else &#123;</span></span><br><span class="line"><span class="comment">      val bypassMergeThreshold: Int = conf.getInt(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, 200) //从配置文件中获取可以进行bypassmerge的分区数的阈值属性，如果没有则默认为200个分区</span></span><br><span class="line"><span class="comment">      dep.partitioner.numPartitions &lt;= bypassMergeThreshold</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(<span class="type">SparkEnv</span>.get.conf, dependency)) &#123;</span><br><span class="line">	<span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>]() <span class="comment">//如果dep.mapSideCombine为false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">如果在依赖中声明了聚合器的情况下不能使用UnsafeShuffleWriter。如reduceByKey算子中创建的shuffleRDD：new ShuffledRDD[K, V, C]().setAggregator(aggregator)会指定聚合器，不能用UnsafeShuffleWriter。</span></span><br><span class="line"><span class="comment">def canUseSerializedShuffle(dependency: ShuffleDependency[_, _, _]): Boolean = &#123;</span></span><br><span class="line"><span class="comment">    val shufId = dependency.shuffleId</span></span><br><span class="line"><span class="comment">    val numPartitions = dependency.partitioner.numPartitions</span></span><br><span class="line"><span class="comment">    if (dependency.aggregator.isDefined) &#123; //如果aggregator已经定义了，就不能用UnsafeShuffleWriter写出</span></span><br><span class="line"><span class="comment">      log.debug(</span></span><br><span class="line"><span class="comment">        s&quot;Can&#x27;t use serialized shuffle for shuffle $shufId because an aggregator is defined&quot;)</span></span><br><span class="line"><span class="comment">      false</span></span><br><span class="line"><span class="comment">    &#125; else if (numPartitions &gt; MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE) &#123;//分区数不能大于指定的值1&lt;&lt;24</span></span><br><span class="line"><span class="comment">      log.debug(s&quot;Can&#x27;t use serialized shuffle for shuffle $shufId because it has more than &quot; +</span></span><br><span class="line"><span class="comment">        s&quot;$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE partitions&quot;)</span></span><br><span class="line"><span class="comment">      false</span></span><br><span class="line"><span class="comment">    &#125; else &#123;</span></span><br><span class="line"><span class="comment">      log.debug(s&quot;Can use serialized shuffle for shuffle $shufId&quot;)</span></span><br><span class="line"><span class="comment">      true</span></span><br><span class="line"><span class="comment">    &#125;</span></span><br><span class="line"><span class="comment">  &#125;</span></span><br><span class="line"><span class="comment">&#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">elseif (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">	<span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>]()</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">其他情况</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">else</span> &#123;</span><br><span class="line">	<span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)<span class="comment">//所以有预聚合，分区数大于200，有聚合器的情况下只能使用SortShuffleWriter</span></span><br><span class="line">&#125;</span><br><span class="line">*/</span><br><span class="line"></span><br><span class="line"><span class="comment">//进入SortShuffleWriter类的write方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//如果有预聚合的话用ExternalSorter</span></span><br><span class="line">    sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      require(dep.aggregator.isDefined, <span class="string">&quot;Map-side combine without Aggregator specified!&quot;</span>)</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">        context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">//否则用ExternalSorter，两者的参数不同，形成重载</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">        context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">    &#125;</span><br><span class="line">     sorter.insertAll(records)<span class="comment">//向排序器中插入数据进行排序。内部会进行判断，如果需要spill（）溢写磁盘，则会释放内存releaseMemory（）中缓存的已经溢写完成的数据。该溢写方法中记录了批处理大小batchSize、分段segment等信息，和进行溢写的方法，最关键的是partitionId信息，写到对应的磁盘文件中。writeIndexFileAndCommit方法写出数据文件dataFile和indexFile。视频见day10_14</span></span><br><span class="line">&#125;</span><br><span class="line">                           </span><br><span class="line">                           </span><br><span class="line">                           </span><br><span class="line"><span class="comment">//进入ResultTask的runTask方法,读取过程涉及许多方法</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">runTask</span></span>(context: <span class="type">TaskContext</span>): <span class="type">U</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> (rdd, func) = ser.deserialize[(<span class="type">RDD</span>[<span class="type">T</span>], (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>)](...)</span><br><span class="line">	func(context, rdd.iterator(partition, context))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">      getOrCompute(split, context)<span class="comment">//执行</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      computeOrReadCheckpoint(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;                        </span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">    <span class="type">SparkEnv</span>.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context).read().asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br><span class="line">                           </span><br><span class="line"><span class="comment">//进入TaskRunner的run方法(Executor类)</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](taskBytes, <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">    <span class="keyword">val</span> res = task.run(</span><br><span class="line">            taskAttemptId = taskId,</span><br><span class="line">            attemptNumber = attemptNumber,</span><br><span class="line">            metricsSystem = env.metricsSystem)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><img src="F:/Typora/图片/网络编程.PNG" alt="网络编程"></p>
<p>序列化+动态代理+远程调用：</p>
<p>java的Runtime.getRuntime().exec(commandStr)可以调用执行cmd指令</p>
<p>new PrintWriter()	</p>
<p>cmd &#x2F;c notepad打开记事本命令</p>
<p>输出流</p>
<p>BIO(阻塞IO，tomcat默认):进程会一直等待IO处理的数据。<br> NIO(非阻塞IO，redis的多路复用)：进程会轮询IO处理是否完成，期间线程不阻塞。<br> AIO(异步IO，window系统支持)：如果IO处理完成，会通知进程并将数据缓存在指定位置（类似回调方法），进程不需要关心IO处理。</p>
<p>RDD  PairRDD</p>
<p>GIT  SVN</p>
<p>kafka物理偏移量</p>
<p>indexFile  dataFile两个文件</p>
<p>bypass什么时候起作用：是否是预聚合，bypassMergeThrold&lt;&#x3D;200。面试重点：需要结合底层代码来讲。底层用的是sortShuffleManager，writer时有多个handle，bypass只是其中一种情况。。。。。。<br>&#x2F;&#x2F;所以有预聚合，分区数大于200，有聚合器的情况下只能使用SortShuffleWriter</p>
<p>动态占用机制：重点</p>
<p>所以cache会丢数据，因为动态占用机制可能会淘汰数据。</p>
<p>concurrentHashMap用到了分段锁，线程安全。HashMap线程不安全，HashTable线程安全</p>
<p>storage存储cache数据和广播变量；other存储rdd数据；execution存储计算过程。</p>
<p>钨丝计划：将堆内堆外的内存作为同一管理</p>
<p>去中心化：ngix宕掉整个系统宕机；redis如果一个节点宕掉，集群还能运行；</p>
<p>动态内存管理、静态内存管理、动态占用</p>
<p>Garbage first：面向多核的垃圾回收器（）  -XX:+UseG1GC指定使用G1垃圾回收器</p>
<p>Driver端不用于计算，但是累加器和collect等算计会返回大数据就可能出现OOM</p>
<p>trancient导致rdd为null产生空指针异常就，可以用广播变量解决。</p>
<p>闭包检测会对序列化进行检查。</p>
<p>javaSerial(user,”e:&#x2F;user.dat”);用java序列化器对对象进行序列化并保存到指定路径。<br>kryoSerial(user,”e:&#x2F;user.dat”);用kryo序列化器对对象进行序列化并保存到指定路径。</p>
<p>kryoDeserial(User.class,”e:&#x2F;user1.dat”)；将指定目录进行反序列为User类对象。</p>
<p>ArrayList和HashMap中的数组属性是用transient修饰的，所以序列化器序列化不了ArrayList和HashMap中的数据；而kryo可以进行序列化。<br>spark程序从redis中获取到的set集合，因为内部属性是用transcient修饰的，导致无法序列化出现空指针异常。如果因为序列化问题导致空指针异常，可以用广播变量广播。</p>
<p>kryo优势：①序列化后文件是java序列化的10倍，减少网络io  ②能绕过java序列化的机制（如transcient不起作用）</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">CJ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/">http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Hexo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark/" title="Spark"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%AE%9E%E6%97%B6%E9%A1%B9%E7%9B%AE/" title="Spark实时项目"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark实时项目</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJ</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/MySQL/%E6%B3%A8%E8%A7%A3@Select%E5%92%8C@Insert/" title="注解@Select和@Insert">注解@Select和@Insert</a><time datetime="2023-05-06T05:48:28.906Z" title="发表于 2023-05-06 13:48:28">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/%E6%B3%A8%E8%A7%A3@EnableAutoConfiguration/" title="注解@EnableAutoConfiguration">注解@EnableAutoConfiguration</a><time datetime="2023-05-06T05:48:06.027Z" title="发表于 2023-05-06 13:48:06">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E6%A1%86%E6%9E%B6/" title="大数据集群监控框架">大数据集群监控框架</a><time datetime="2023-05-06T05:42:56.298Z" title="发表于 2023-05-06 13:42:56">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/HashMap%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98%E5%8F%8AConcurrentHashMap%E5%8E%9F%E7%90%86/" title="HashMap并发问题及ConcurrentHashMap原理">HashMap并发问题及ConcurrentHashMap原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/Stream%E5%8E%9F%E7%90%86/" title="Stream原理">Stream原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By CJ</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>