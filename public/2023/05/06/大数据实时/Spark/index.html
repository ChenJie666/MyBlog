<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark | Hexo</title><meta name="author" content="CJ"><meta name="copyright" content="CJ"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="六种实现wordCount方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869    &#x2F;&#x2F;method0	val rdd1: RDD[(String, Iterable[(String,">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="六种实现wordCount方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869    &#x2F;&#x2F;method0	val rdd1: RDD[(String, Iterable[(String,">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-05-06T05:31:21.051Z">
<meta property="article:modified_time" content="2023-05-06T05:31:21.051Z">
<meta property="article:author" content="CJ">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-05-06 13:31:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><hr/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Hexo"><span class="site-name">Hexo</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-06T05:31:21.051Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-05-06T05:31:21.051Z" title="更新于 2023-05-06 13:31:21">2023-05-06</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/">大数据实时</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>六种实现wordCount方式</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">//method0</span></span><br><span class="line">	<span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = rdd.groupBy &#123;</span><br><span class="line">      <span class="keyword">case</span> (s, num) =&gt; s</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd1.mapValues(datas =&gt; datas.map(_._2).sum)</span><br><span class="line">    rdd2.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">	<span class="comment">//method1</span></span><br><span class="line">    rdd.groupByKey.map&#123;</span><br><span class="line">      <span class="keyword">case</span> (s,datas) =&gt; (s,datas.sum)</span><br><span class="line">    &#125;.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//method2</span></span><br><span class="line">    rdd.reduceByKey((x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//method3</span></span><br><span class="line">    rdd.aggregateByKey(<span class="number">0</span>)((x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y,(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//method4</span></span><br><span class="line">    rdd.foldByKey(<span class="number">0</span>)((x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//method5</span></span><br><span class="line">    rdd.combineByKey((num:<span class="type">Int</span>)=&gt;num,(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y,(x:<span class="type">Int</span>,y:<span class="type">Int</span>)=&gt;x+y).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//method6</span></span><br><span class="line">    rdd.flatMap&#123;</span><br><span class="line">      <span class="keyword">case</span> (s:<span class="type">String</span>,num:<span class="type">Int</span>) =&gt;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">        <span class="keyword">for</span>(elem &lt;- <span class="number">1</span> to num)&#123;</span><br><span class="line">          list.append(s)</span><br><span class="line">        &#125;</span><br><span class="line">      list</span><br><span class="line">    &#125;.groupBy((s:<span class="type">String</span>)=&gt;s).map&#123;</span><br><span class="line">      <span class="keyword">case</span> (s,datas) =&gt; (s,datas.size)</span><br><span class="line">    &#125;.collect.foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Method7</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;wc&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.flatMap &#123;</span><br><span class="line">      <span class="keyword">case</span> (s, num) =&gt;</span><br><span class="line">        <span class="keyword">val</span> list = <span class="keyword">new</span> <span class="type">ListBuffer</span>[(<span class="type">String</span>, <span class="type">Int</span>)]()</span><br><span class="line">        <span class="keyword">for</span> (elem &lt;- <span class="number">1</span> to num) &#123;</span><br><span class="line">          list.append((s, <span class="number">0</span>))</span><br><span class="line">        &#125;</span><br><span class="line">        list</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> stringToLong: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = rdd1.countByKey()</span><br><span class="line">    println(stringToLong)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Method8</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;wc&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.parallelize(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (s, num) =&gt; (s + <span class="string">&quot; &quot;</span>) * num</span><br><span class="line">    &#125;.flatMap &#123;</span><br><span class="line">      _.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> stringToLong: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = rdd1.countByValue()</span><br><span class="line">    println(stringToLong)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>foreachPartition		countByValue</p>
<p>RPC远程过程调用</p>
<p>RMI框架  &#x3D;&gt; EJB &#x3D;&gt; Spring</p>
<h2 id="Hadoop历史："><a href="#Hadoop历史：" class="headerlink" title="Hadoop历史："></a>Hadoop历史：</h2><p>hadoop于2008年发布0.X版本，2011发布1.X版本，2013年10月发布了2.X版本（Yarn版本）。</p>
<p>1）1.X版本缺陷：①单节点问题，没有HA模式②硬件有上限③MR因为要落盘，不支持迭代操作④MR框架和资源管理耦合在一起，无法分离。</p>
<p><img src="F:/Typora/图片/11.PNG" alt="11"></p>
<p>2）2.X版本改进：①实现HA②使用Yarn作为资源管理框架，MR只做任务调度与执行。将Yarn和MR解耦，实现计算框架的可插拔。通过中间层Container实现资源与计算的解耦。</p>
<p><img src="F:/Typora/图片/22.PNG" alt="22"></p>
<p>Spark历史：</p>
<p>2013年6月发布，采用函数式编程方式，优化了MR框架，适合迭代式计算。Spark的框架和第一代的Hadoop框架相似，不考虑框架的可插拔，因为spark性能足够高，不需要替换。</p>
<p><img src="F:/Typora/图片/33.PNG" alt="33"></p>
<p>现实中我们将hadoop的MR计算引擎替换为Spark来实现高效的计算：</p>
<p><img src="F:/Typora/图片/44.PNG" alt="44"></p>
<p>内置模块：</p>
<p><img src="F:/Typora/图片/55.PNG" alt="55"></p>
<p>早期通过JNDI，将应用程序与资源服务进行解耦合：</p>
<p><img src="F:/Typora/图片/77.PNG" alt="77"></p>
<p>Driver：初始化上下文（sparkcontext），任务的切分和调度</p>
<p>Driver和Executor负责计算，RM和NM负责资源的调度。通过AppMaster进行通讯并完成两者的解耦。</p>
<p><img src="F:/Typora/图片/66.PNG" alt="66"></p>
<pre><code>**spark执行任务的基本流程**①客户端将任务提交给Driver，Drivre向资源管理者注册应用程序，然后资源管理者启动Executor，Executor反向注册在Driver上；Driver初始化sparkContext上下文，将任务划分，并调度给各个Executor执行任务。将结果返回给Driver，关闭上下文，结束流程。

yarn模式的spark执行任务基本流程：①客户端将App提交给RM，RM选择NM启动ApplicationMaster，在AM（用于解耦）中启动Driver，然后AM根据任务启动Executor，Driver创建sc并切分和调度任务给Executor，任务结束后AM从RM上注销。



进入shell窗口，就会开启一个虚拟机，在输入指令前会处于阻塞状态，后台存在一个spark submit进程，可以在4040端口网页查看Driver的详情。退出shell窗口，则程序结束，虚拟机销毁。



Spark的一个分布式数据集的分析框架，将计算单元缩小为更适合分布式计算和并行计算的模型，称之为RDD。
</code></pre>
<p>转换流是装饰者设计模式，RDD的原理和IO流基本类似：</p>
<p> <img src="F:/Typora/图片/88.PNG" alt="88"></p>
<p><img src="F:/Typora/图片/99.PNG" alt="99"></p>
<h2 id="spark-core"><a href="#spark-core" class="headerlink" title="spark-core"></a>spark-core</h2><p>RDD：弹性分布式数据集</p>
<pre><code>弹性：计算逻辑，血缘关系，分区

分布式：分布式计算，数据的来源，数据目的地

数据集：数据的类型&amp;计算逻辑的封装（数据模型）
</code></pre>
<p>RDD：不可变、可分区、里面的元素可并行计算的集合</p>
<pre><code>不可变：计算逻辑不可变

可分区：提高数据处理能力

并行计算：多任务同时执行
</code></pre>
<p>算子：可以理解为方法和函数。</p>
<p>RDD中的方法：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]	<span class="comment">//返回多个分区</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]	<span class="comment">//计算每个分区的函数</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps	<span class="comment">//获取依赖关系</span></span><br><span class="line"><span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span>	<span class="comment">//分区器，默认为none，因为只有k-v类型的RDD有分区器，会在pairRDD中重写partitionre方法，返回为some，通过key进行分区。</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span> <span class="comment">//每个分区的优先位置，优先将计算任务分配到和计算数据一个节点上。</span></span><br></pre></td></tr></table></figure>



<p>读取文件的方式基于hadoop读取文件的方式，读取文件的数据是一行一行的字符串，源码如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>用makeRDD从内存中获取数组创建RDD时，可以传入的切片数量参数：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> =</span><br><span class="line">  scheduler.conf.getInt(<span class="string">&quot;spark.default.parallelism&quot;</span>, totalCores)<span class="comment">//如果参数已经设置，则返回参数值，如果参数未设置，则返回totalCores。totalCores根据local或集群的拥有的核心数而定。</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getInt</span></span>(key: <span class="type">String</span>, defaultValue: <span class="type">Int</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  getOption(key).map(_.toInt).getOrElse(defaultValue)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>数组的具体的切片规则：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">  (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">    <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">    <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">	(start, end)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>文件的切片规则：9:20,</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">      path: <span class="type">String</span>,</span><br><span class="line">      minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">      minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">defaultMinPartitions</span></span>: <span class="type">Int</span> = math.min(defaultParallelism, <span class="number">2</span>)<span class="comment">//defaultParalelism值与数组的切片默认值相同，与配置参数或totalcores有关，min方法取最小值，一般返回2。</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">/*实际读取时底层会调用hadoop的FileInputFormat类的getSplits方法进行切片。先统计所有文件的大小，然后除以切片数，获得目标切片的大小，因为可能除不尽，所以多余的部分再划一个切片。因此指定的参数是最小切片数，实际可能要大。</span></span><br><span class="line"><span class="comment">	读取行时，行末有cr和lf两个字符，写入时行末只有lf字符。对文档中的1,2,3切片，分区行号为（0-2,3-5,6），将1、2行放到分片一，3、4、5行放到分片二，6行放到分片三，最终得到1，2一个分片，3一个分片，还有一个空分片。需要注意切片有1.1的系数，如果最后一个切片很小，则不切。视频见spark-77*/</span></span><br></pre></td></tr></table></figure>



<p>从内存创建RDD：sc.makeRDD&#x2F;parallelize</p>
<p>从磁盘创建RDD：sc.textFile()</p>
<p>从RDD转换RDD：所有算子的逻辑计算操作都是发送给Executor执行的</p>
<p>new JdbcRDD（）直接创建RDD</p>
<p>newAPIHadoopRDD</p>
<pre><code>mapPartitions可能会出现OOM，因为map处理时可以将已经用完的数据GC，而mapPartitions所有数据以iterator进行传输，是一个整体，不能对部分进行GC。在OOM前一定会请求Full GC。
</code></pre>
<p><img src="F:/Typora/图片/111.PNG" alt="111"></p>
<pre><code>mapPartitions只能返回可迭代的数组，所以不能实现取各分区最大值、平均值等功能；而glom可以实现。
</code></pre>
<p>sample方法（）</p>
<p>抽取不放回：false，fraction：抽取的概率</p>
<p>抽取放回：true，fraction：抽取的次数</p>
<p>随机数种子   new Random(1000)	1000就是随机数种子，随机数算法读取上一个数的值进行计算，如果随机数种子相等，那么之后计算出来的数也都相等。</p>
<p>打分：随机数种子会根据随机数算法给数字打分，然后小于概率的数取出，大于概率的数忽略。</p>
<p>shuffle：将数据打乱重组的过程。打乱重组的过程一定会写磁盘。shuffleWrite和shuffleRead进行磁盘写和读。</p>
<p>rddToPairRDDFunctions</p>
<p>K-V类型的算子的源码都不在RDD中，通过隐式转换在PariRDDFunctions源码中查找。RDD中的partitioner</p>
<p>&#x2F;&#x2F;极限情况下向hashmap中放入11个数据会变成红黑树。</p>
<p>groupBy的区别是，groupBy可以自定义分区，而groupByKey只能通过key来分区。</p>
<p>reduceByKey和foldByKey的区别是foldByKey可以带入一个外来值。</p>
<p><strong>八种实现wordCount的方法：</strong></p>
<p>groupby</p>
<p>groupByKey和map</p>
<p>reduceByKey</p>
<p>aggregateByKey</p>
<p>foldByKey</p>
<p>combineByKey</p>
<p>countByKey</p>
<p>countByValue</p>
<p>会进行shuffle的算子：groupBy、repartition、groupByKey、reduceByKey、aggregateByKey、combineByKey、</p>
<pre><code>reduceByKey和aggregateByKey在shuffle前会进行combine预聚合减少数据量，而groupByKey没有预聚合。reduceByKey在分区内和分区间的计算逻辑完全相同。

aggregateByKey使用了函数柯里化，第一个参数列表中是分区内计算的初始值，第二个参数列表中：第一个参数表示分区内计算规则，第二个参数表示分区间计算规则。

foldByKey就是aggregateByKey简化版，当aggregateByKey中分区内和分区间的计算规则一样时使用。

combineByKey需要传递三个参数	1.将第一个key出现的v转换结构的计算规则	2.第二个参数表示分区内计算规则	3.第三个参数表示分区间计算规则
</code></pre>
<h2 id="算子和分区器"><a href="#算子和分区器" class="headerlink" title="算子和分区器"></a>算子和分区器</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//value型</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapPartitions</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.mapPartitions &#123;</span><br><span class="line">      datas =&gt; datas.map(_.*(<span class="number">2</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    val rdd2: RDD[Int] = rdd.mapPartitions &#123;</span></span><br><span class="line">    <span class="comment">//      datas =&gt; Iterator(datas.sum)</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line"></span><br><span class="line">    rdd2.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapPartitionsWithIndex</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = rdd.mapPartitionsWithIndex &#123;</span><br><span class="line">      <span class="keyword">case</span> (index, datas) =&gt;</span><br><span class="line">        println(<span class="string">&quot;**************&quot;</span>)</span><br><span class="line">        datas.map((index, _))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rdd2.collect().foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Glom</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    val rdd: RDD[(String, Int)] = sc.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,4),(&quot;a&quot;,5)),2)</span></span><br><span class="line">    <span class="comment">//    val arrayRDD: RDD[Array[(String, Int)]] = rdd.glom()</span></span><br><span class="line">    <span class="comment">//    arrayRDD.collect.foreach&#123;</span></span><br><span class="line">    <span class="comment">////      println(&quot;****************&quot;)</span></span><br><span class="line">    <span class="comment">//      _.foreach&#123;</span></span><br><span class="line">    <span class="comment">//        println(&quot;****************&quot;)</span></span><br><span class="line">    <span class="comment">//        println&#125;</span></span><br><span class="line">    <span class="comment">//    &#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> arrayRDD: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = rdd.glom()</span><br><span class="line">    arrayRDD.collect.foreach &#123;</span><br><span class="line">      println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line">      datas =&gt;</span><br><span class="line">        <span class="comment">//            println(&quot;****************&quot;)</span></span><br><span class="line">        datas.foreach &#123;</span><br><span class="line">          <span class="comment">//        println(&quot;****************&quot;)</span></span><br><span class="line">          println</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupBy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = rdd.groupBy &#123;</span><br><span class="line">      <span class="keyword">case</span> (c, i) =&gt; c</span><br><span class="line">    &#125;</span><br><span class="line">    rdd2.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Filter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.filter &#123;</span><br><span class="line">      <span class="keyword">case</span> (c, i) =&gt; <span class="string">&quot;c&quot;</span>.equals(c)</span><br><span class="line">    &#125;</span><br><span class="line">    rdd1.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Sample</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="number">1.</span>to(<span class="number">10</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.sample(<span class="literal">false</span>, <span class="number">0.5</span>) <span class="comment">//随机数种子会根据随机数算法给数字打分，然后小于概率的数取出，大于概率的数忽略。</span></span><br><span class="line">    rdd2.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.sample(<span class="literal">true</span>, <span class="number">2</span>, <span class="number">20</span>) <span class="comment">//同一个数据可能被抽取的次数，可以用于解决HBase的热点问题</span></span><br><span class="line">    rdd3.collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Distinct</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>), <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    rdd.distinct(<span class="number">10</span>).collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Coalesce</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>), <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.filter(_ % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd2.coalesce(<span class="number">2</span>, <span class="literal">true</span>) <span class="comment">//shuffle默认为false，单纯合并两个分区；shuffle设为true，重新洗牌</span></span><br><span class="line">    rdd3.glom().collect().foreach &#123; datas =&gt;</span><br><span class="line">      println(<span class="string">&quot;*****************&quot;</span>)</span><br><span class="line">      datas.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Repartition</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>), <span class="number">3</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.filter(_ % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd1.repartition(<span class="number">2</span>) <span class="comment">//底层调用coalesce方法</span></span><br><span class="line">    rdd2.glom().collect.foreach &#123; datas =&gt;</span><br><span class="line">      println(<span class="string">&quot;***************&quot;</span>)</span><br><span class="line">      datas.foreach(println)</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SortBy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.sortBy &#123;</span><br><span class="line">      <span class="keyword">case</span> (s, i) =&gt; s</span><br><span class="line">    &#125;</span><br><span class="line">    rdd1.collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//pipe(脚本路径)  将每个分区的数据用脚本处理</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//双value交互型</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UnionAndSubtractAndIntersection</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.union(rdd1) <span class="comment">//全集，相当于union all</span></span><br><span class="line">    rdd2.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.subtract(rdd1) <span class="comment">//差集</span></span><br><span class="line">    rdd3.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd4: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.intersection(rdd1) <span class="comment">//交集</span></span><br><span class="line">    rdd4.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd5: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = rdd.cartesian(rdd1) <span class="comment">//求笛卡尔积</span></span><br><span class="line">    rdd5.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd6: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = rdd.zip(rdd1) <span class="comment">//拉链，但是数据个数或分区数不一致会抛异常</span></span><br><span class="line">    rdd6.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//key-value类型</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionBy</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//Spark中默认的分区器为HashPartitioner</span></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="comment">//RDD类中没有partitionBy，通过RDD伴生对象中的隐式转换类转换成rddToPairRDDFunctions对象，转换前提是k-v对</span></span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = rdd1.mapPartitionsWithIndex &#123;</span><br><span class="line">      <span class="keyword">case</span> (index, datas) =&gt; datas.map(t =&gt; (index, (t._1, t._2)))</span><br><span class="line">    &#125;</span><br><span class="line">    rdd2.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//可以自定义类继承Partitioner，实现numPartition和getPartition方法</span></span><br><span class="line">    <span class="keyword">val</span> rdd3: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">val</span> rdd4: <span class="type">RDD</span>[(<span class="type">Int</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = rdd3.mapPartitionsWithIndex &#123;</span><br><span class="line">      <span class="keyword">case</span> (index, datas) =&gt; datas.map(t =&gt; (index, (t._1, t._2)))</span><br><span class="line">    &#125;</span><br><span class="line">    rdd4.collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义分区器</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">num: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = num</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GroupByKey</span> </span>&#123; <span class="comment">//与groupBy的区别是，groupBy可以自定义分区，而groupByKey只能通过key来分区</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = rdd.groupByKey()</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd1.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (s, datas) =&gt; (s, datas.sum)</span><br><span class="line">    &#125;</span><br><span class="line">    rdd2.collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ReduceByKey</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.reduceByKey(_ + _) <span class="comment">//相较groupBy操作，会在区内进行combine预聚合,更加需求选择合适的</span></span><br><span class="line">    rdd1.collect.foreach(println)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AggregateByKey</span> </span>&#123; <span class="comment">//分区内和分区间的计算逻辑不同，用该算子</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.aggregateByKey(<span class="number">0</span>)(<span class="type">Math</span>.max(_, _), _ + _) <span class="comment">//第一个参数是初始值，第二个参数是分区内和分区间的计算逻辑</span></span><br><span class="line">    rdd1.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FoldByKey</span> </span>&#123; <span class="comment">//如果aggregateByKey的分区内和分区间的计算逻辑相同，可以用foldByKey</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.foldByKey(<span class="number">0</span>)(_ + _)</span><br><span class="line">    rdd1.collect.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CombineByKey</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;mp&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">//求相同key的平均值</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>)), <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd.combineByKey[(<span class="type">Int</span>,<span class="type">Int</span>)]((num:<span class="type">Int</span>) =&gt; (num, <span class="number">1</span>), (t:(<span class="type">Int</span>,<span class="type">Int</span>), num1:<span class="type">Int</span>) =&gt; (t._1 + num1, t._2 + <span class="number">1</span>), (t1:(<span class="type">Int</span>,<span class="type">Int</span>), t2:(<span class="type">Int</span>,<span class="type">Int</span>)) =&gt; (t1._1 + t2._1, t1._2 + t2._2))</span><br><span class="line">    rdd1.collect.foreach&#123;</span><br><span class="line">      <span class="keyword">case</span> (s,t) =&gt; println((s,t._1*<span class="number">1.0</span>/t._2))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>join算子效率不高。</p>
<p>aggregate（）算子中的零值在分区内可以使用，在分区间计算也会使用。</p>
<p>saveAsObjectFile是序列化到磁盘上</p>
<p>saveAsSequenceFile需要k-v对数据才能保存到磁盘上</p>
<p>countByValue（）将数据作为值，进行计数</p>
<p>rdd.collect()会将executor执行后返回的结果放入到数组中；rdd.foreach会将计算逻辑发送给executor，由executor执行。</p>
<p>序列化形成字节码才能网络传输。</p>
<p>算子外的代码在Driver端执行，算子内的代码在Executor中执行。所以执行任务时需要注意传输的信息是否可序列化，如果不能序列化则会抛异常。</p>
<p>14:51  网络传输</p>
<p>spark在执行作业之前，会先进行闭包（closure）检测，同时检查闭包的变量是否可以序列化用于网络传输。</p>
<p><strong>源码:视频69</strong></p>
<p>toDebugString：查看rdd的血缘关系</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(2) MapPartitionsRDD[2] at mapPartitionsWithIndex at AggregateByKey.scala:20 []</span><br><span class="line"> |  ShuffledRDD[1] at aggregateByKey at AggregateByKey.scala:14 []</span><br><span class="line"> +-(2) ParallelCollectionRDD[0] at parallelize at AggregateByKey.scala:11 []</span><br></pre></td></tr></table></figure>

<pre><code>继承了NarrowDependency的就是窄依赖，如MapPartitionsRDD最终会继承RDD(oneParent.context, List(new OneToOneDependency(oneParent))),所以窄依赖的rdd调用getDependencies方法，会调用父类的方法，返回其上一个RDD的对象。OneToOneDependency类是NarrowDependency的子类，所以称为窄依赖。
通过makeRDD等方法创建的rdd是ParallelCollectionRDD类型的，rdd.getDependencies会返回Nil空集合。
所有的会进行shuffle的算子最终会返回ShuffleRDD类对象，拥有自己的getDependencies方法，返回一个集合，集合中只有一个ShuffleDependency对象。



shuffleRDD就是宽依赖
</code></pre>
<p><img src="F:/Typora/图片/222.PNG" alt="222"></p>
<p>persist&#x2F;cache作用：①提高容错率②提升效率</p>
<pre><code>缓存会保存到血缘关系中，如果有血缘关系，直接调用缓存；但是不会切断前面的血缘关系，因为缓存可能丢失，丢失后重新根据血缘进行查找。血缘关系保存在RDD中。
</code></pre>
<h2 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h2><p><strong>源码:视频62</strong></p>
<p>**App-&gt;job:**Application任务会根据行动算子产生对应的job，如行动算子collect（），会将rdd作为参数调用sc.runJob方法，最终在DAGScheduler类中调用submitJob方法提交rdd。在handleJobSubmitted方法中生成ResultStage对象并作为参数创建一个ActiveJob作业对象。所以一个作业中包含多个stage。</p>
<p><strong>源码:视频71、72</strong></p>
<p>**job-&gt;stage:**在handleJobSubmitted方法中，通过血缘获取shuffleDep类的算子并生成对应的stage，将stage放入数组中作为参数创建ResultStage，并作为参数创建ActiveJob对象，所以一个job中包含了多个stage。调用submitStage方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ResultStage</span>和<span class="type">ShuffleMapStage</span></span><br><span class="line">源码：</span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">    <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">      listener.jobFailed(e)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">  ........</span><br><span class="line">  submitStage(finalStage)	<span class="comment">//将ResultStage作为参数调用submitStage方法</span></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line"><span class="comment">//通过createResultStage方法完成stage的切分并作为参数创建ActiveJob对象。</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">      rdd: <span class="type">RDD</span>[_],</span><br><span class="line">      func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">      partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">      jobId: <span class="type">Int</span>,</span><br><span class="line">      callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)<span class="comment">//查找当前的rdd的血缘，返回HashSet数组</span></span><br><span class="line">    <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">    <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)<span class="comment">//将所有的stage包含在ResultStage对象中。</span></span><br><span class="line">    stageIdToStage(id) = stage</span><br><span class="line">    updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">    stage</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//查找血缘中shuffleDep类，并且每个shuffle都创建并返回包含所有stage的集合</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(rdd: <span class="type">RDD</span>[_], firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">    getShuffleDependencies(rdd).map &#123; shuffleDep =&gt;</span><br><span class="line">      getOrCreateShuffleMapStage(shuffleDep, firstJobId)<span class="comment">//getOrCreateShuffleMapStage方法返回了createShuffleMapStage对象，相当于每个shuffle创建了一个阶段。</span></span><br><span class="line">    &#125;.toList</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//getShuffleDependencies方法查找shuffleDep类型的依赖加入到parents中并返回，交给map方法遍历</span></span><br><span class="line">  <span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">getShuffleDependencies</span></span>(</span><br><span class="line">      rdd: <span class="type">RDD</span>[_]): <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]]</span><br><span class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">    waitingForVisit.push(rdd)</span><br><span class="line">    <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> toVisit = waitingForVisit.pop()</span><br><span class="line">      <span class="keyword">if</span> (!visited(toVisit)) &#123;</span><br><span class="line">        visited += toVisit</span><br><span class="line">        toVisit.dependencies.foreach &#123;</span><br><span class="line">          <span class="keyword">case</span> shuffleDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">            parents += shuffleDep<span class="comment">//将shuffleDep类型的血缘加入到parents中</span></span><br><span class="line">          <span class="keyword">case</span> dependency =&gt;</span><br><span class="line">            waitingForVisit.push(dependency.rdd)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    parents</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>stage-&gt;task：疑问？？？？？？？？？？？？？？？？？？？？？？？ 怎么从前往后提交的？？？？？？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">通过递归从后往前遍历<span class="type">ResultStage</span>中的stage，从前往后提交stage</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;submitStage(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)<span class="comment">//查找ResultStage的上一级</span></span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)<span class="comment">//没有上一级则提交任务</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)<span class="comment">//有上一级则递归调用此方法，直到递归边界，即第一个stage。</span></span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">提交没有上级stage的任务（对单个stage进行处理）</span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">    logDebug(<span class="string">&quot;submitMissingTasks(&quot;</span> + stage + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">   	 <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">          stage <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">              partitionsToCompute.map &#123; id =&gt;	<span class="comment">//如果是ShuffleMapStage，则计算分区</span></span><br><span class="line">                <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">                <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">                <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,<span class="comment">//每个分区一个Task</span></span><br><span class="line">                  taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, <span class="type">Option</span>(jobId),</span><br><span class="line">                  <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">              partitionsToCompute.map &#123; id =&gt;	<span class="comment">//如果是ResultStage，则计算分区</span></span><br><span class="line">                <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">                <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">                <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">                <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,<span class="comment">//每个分区生成一个Task</span></span><br><span class="line">                  taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</span><br><span class="line">                  <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">             &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          logInfo(<span class="string">&quot;Submitting &quot;</span> + tasks.size + <span class="string">&quot; missing tasks from &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">          stage.pendingPartitions ++= tasks.map(_.partitionId)</span><br><span class="line">          logDebug(<span class="string">&quot;New pending partitions: &quot;</span> + stage.pendingPartitions)</span><br><span class="line">          taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">            tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))<span class="comment">//将得到的tasks放入TaskSet集合中，一个stage对应一个TaskSet</span></span><br><span class="line">          stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">    &#125;</span><br><span class="line">         </span><br><span class="line"> 调用submitTasks方法后，会调用backend.reviveOffers()方法，reviveOffers（），<span class="type">ReviveOffers</span>，最后调用executor.launchTask方法，启动线程池threadPool.execute，将<span class="type">TaskRunner</span>对象作为参数，所以该对象中有run方法。在launchTask时会调用task.serializedTask序列化方法和ser.serialize方法进行序列化，之后调用task.run方法开启。见视频<span class="number">87</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>任务的传递：视频88</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Driver</span>发送任务： </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</span><br><span class="line">      <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">        <span class="keyword">val</span> serializedTask = ser.serialize(task)<span class="comment">//序列化任务</span></span><br><span class="line">      ...</span><br><span class="line">      &#125;</span><br><span class="line">   executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</span><br><span class="line">   <span class="comment">//Driver端启动序列化，将序列化的任务发送给executor</span></span><br><span class="line"> &#125;</span><br><span class="line"><span class="type">Executor</span>接收任务</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">        exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> taskDesc = ser.deserialize[<span class="type">TaskDescription</span>](data.value)<span class="comment">//进行反序列化</span></span><br><span class="line">        logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">        executor.launchTask(<span class="keyword">this</span>, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,<span class="comment">//executor（计算对象）启动任务</span></span><br><span class="line">          taskDesc.name, taskDesc.serializedTask)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>堆外内存是jvm外的内存，由OS管理，需要自己控制分配和释放（spark封装了对外内存的控制和释放功能）。</p>
<p><strong>cms（面向单核）和G1（面向多核）以及jvm面试重点。HBase面试重点，rowkey设计，分区原理！线程池，preparedstatement与。。的区别  等相关问题。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">带有不同参数的同一SQL语句被多次执行的时候。PreparedStatement对象允许数据库预编译SQL语句</span><br><span class="line">1.PreparedStatement是预编译的,对于批量处理可以大大提高效率.也叫JDBC存储过程</span><br><span class="line"></span><br><span class="line">3.statement每次执行sql语句，相关数据库都要执行sql语句的编译，preparedstatement是预编译得,preparedstatement支持批处理</span><br><span class="line"></span><br><span class="line">1.在Web环境中，有恶意的用户会利用那些设计不完善的、不能正确处理字符串的应用程序。特别是在公共Web站点上,在没有首先通过PreparedStatement对象处理的情况下，所有的用户输入都不应该传递给SQL语句。此外，在用户有机会修改SQL语句的地方，如HTML的隐藏区域或一个查询字符串上，SQL语句都不应该被显示出来。</span><br><span class="line">2.在执行SQL命令时，我们有二种选择：可以使用PreparedStatement对象，也可以使用Statement对象。无论多少次地使用同一个SQL命令，PreparedStatement都只对它解析和编译一次。当使用Statement对象时，每次执行一个SQL命令时，都会对它进行解析和编译。</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<pre><code>checkpoint检查点保存时间:在执行行动算子时，才会进行checkpoint操作。因为RDD没有做缓存，所以需要重新计算RDD，然后写出到磁盘。因此可以在checkpoint的点上先进行persist/cache。

spark的checkpoint可以切断血缘关系，一般存储在hdfs上，原则上是不会丢失的；而persist/cache不会切断血缘。



groupBy（）底层也是调用groupByKey（），将groupBy的值通过函数处理后作为key转换为对偶元组，再调用groupByKey（）。



HashPartitioner分区器用到比较多，因为RangePartitioner耦合性太强，对数据源有要求，需要数据可以比较。
</code></pre>
<p>9:52  spark切片会调用hadoop的getSplit方法，也有1.1倍数。</p>
<p>9:56行号</p>
<p>10:25   json①传输数据小②JavaScript是函数式语言，json可以在js中直接使用③mysql从7.0开始支持json存储</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">spark读取json格式的数据解析并存储为<span class="type">Map</span>数组，并封装到<span class="type">Some</span>类的对象中（一般用sparkSQL读取json文件）</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MysqlAndHBase</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> scala.util.parsing.json.<span class="type">JSON</span><span class="comment">//导入scala的json解析包</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;input/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Option</span>[<span class="type">Any</span>]] = rdd.map(<span class="type">JSON</span>.parseFull)</span><br><span class="line">    rdd1.collect.foreach(println)</span><br><span class="line">      </span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>获取mysql中的数据和将数据写入mysql</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">获取mysql数据</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySQL</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> username = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;abc123&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jdbcRDD = <span class="keyword">new</span> <span class="type">JdbcRDD</span>(</span><br><span class="line">      sc,</span><br><span class="line">      () =&gt; &#123;</span><br><span class="line">        <span class="type">Class</span>.forName(driver)</span><br><span class="line">        <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url, username, password)</span><br><span class="line">        connection</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">&quot;select * from rdd where i &gt;= ? and i &lt;= ?&quot;</span>,</span><br><span class="line">      <span class="number">1</span>,</span><br><span class="line">      <span class="number">3</span>,</span><br><span class="line">      <span class="number">3</span>,</span><br><span class="line">      resultSet =&gt; &#123;println(<span class="string">&quot;***&quot;</span>);println(resultSet.getInt(<span class="number">1</span>) + <span class="string">&quot;,&quot;</span> + resultSet.getString(<span class="number">2</span>) + <span class="string">&quot;,&quot;</span> + resultSet.getInt(<span class="number">3</span>))&#125;</span><br><span class="line">    )</span><br><span class="line">    jdbcRDD.collect()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">写入mysql</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MySQLWrite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span></span><br><span class="line">    <span class="keyword">val</span> url = <span class="string">&quot;jdbc:mysql://hadoop102:3306/rdd&quot;</span></span><br><span class="line">    <span class="keyword">val</span> username = <span class="string">&quot;root&quot;</span></span><br><span class="line">    <span class="keyword">val</span> password = <span class="string">&quot;abc123&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;six&quot;</span>,<span class="number">16</span>),(<span class="string">&quot;six&quot;</span>,<span class="number">15</span>)))</span><br><span class="line">    <span class="comment">//每个元素都遍历会导致创建大量的连接导致数据库崩溃，而连接类都不支持序列化；所以有以下优化</span></span><br><span class="line"><span class="comment">/*    rdd.foreach&#123;</span></span><br><span class="line"><span class="comment">      case (name,age) =&gt; &#123;</span></span><br><span class="line"><span class="comment">        Class.forName(driver)</span></span><br><span class="line"><span class="comment">        val connection: Connection = DriverManager.getConnection(url,username,password)</span></span><br><span class="line"><span class="comment">        var sql = &quot;insert into rdd(name,age) value(?,?)&quot;</span></span><br><span class="line"><span class="comment">        val statement: PreparedStatement = connection.prepareStatement(sql)</span></span><br><span class="line"><span class="comment">        statement.setObject(1,name)</span></span><br><span class="line"><span class="comment">        statement.setObject(2,age)</span></span><br><span class="line"><span class="comment">        statement.executeUpdate()</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">        statement.close()</span></span><br><span class="line"><span class="comment">        connection.close()</span></span><br><span class="line"><span class="comment">      &#125;</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">    <span class="comment">//优化：将每个分区的数据作为整体发到executor，每个executor创建一个连接，连接数最少。</span></span><br><span class="line">    rdd.foreachPartition&#123;</span><br><span class="line">      <span class="keyword">case</span> partition =&gt; &#123;</span><br><span class="line">        <span class="type">Class</span>.forName(driver)</span><br><span class="line">        <span class="keyword">val</span> connection: <span class="type">Connection</span> = <span class="type">DriverManager</span>.getConnection(url,username,password)</span><br><span class="line">        <span class="keyword">var</span> sql = <span class="string">&quot;insert into rdd(name,age) value(?,?)&quot;</span></span><br><span class="line">        <span class="keyword">val</span> statement: <span class="type">PreparedStatement</span> = connection.prepareStatement(sql)</span><br><span class="line"></span><br><span class="line">        partition.foreach&#123;</span><br><span class="line">          <span class="keyword">case</span> (name,age) =&gt; &#123;</span><br><span class="line">            statement.setObject(<span class="number">1</span>,name)</span><br><span class="line">            statement.setObject(<span class="number">2</span>,age)</span><br><span class="line">            statement.executeUpdate()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        statement.close()</span><br><span class="line">        connection.close()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">addbatch将一组参数添加到此 <span class="type">PreparedStatement</span>.executeBatch方法批处理命令。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从HBase中读取和写入数据：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseReader</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> configuration: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()<span class="comment">//创建HBase配置类对象，对象中会加入类加载器，加载配置文件等信息。所以需要将HBase-site.xml文件放在加载路径上。</span></span><br><span class="line">    configuration.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>,<span class="string">&quot;fruit&quot;</span>)	<span class="comment">//指定读取的hbase表</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = sc.newAPIHadoopRDD(<span class="comment">//通过此类读取HBase</span></span><br><span class="line">      configuration,				<span class="comment">//传入参数配置，</span></span><br><span class="line">      classOf[<span class="type">TableInputFormat</span>],	<span class="comment">//指定表的输入类型</span></span><br><span class="line">      classOf[<span class="type">ImmutableBytesWritable</span>],<span class="comment">//指定rowkey的类型</span></span><br><span class="line">      classOf[<span class="type">Result</span>]				<span class="comment">//指定value的类型</span></span><br><span class="line">    )</span><br><span class="line">    rdd.foreach&#123;</span><br><span class="line">      <span class="keyword">case</span> (rowkey,result) =&gt; &#123;	<span class="comment">//获取的rdd是（rowkey，result）类型</span></span><br><span class="line">        <span class="keyword">for</span>(cell &lt;- result.rawCells())&#123;</span><br><span class="line">          println(<span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneRow(cell)) + <span class="string">&quot;,&quot;</span> + <span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneValue(cell)))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HBaseWrite</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> configuration: <span class="type">Configuration</span> = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>,<span class="type">String</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;1005&quot;</span>,<span class="string">&quot;name&quot;</span>,<span class="string">&quot;mango&quot;</span>),(<span class="string">&quot;1005&quot;</span>,<span class="string">&quot;color&quot;</span>,<span class="string">&quot;yellow&quot;</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> putRDD: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Put</span>)] = rdd.map &#123;<span class="comment">//转化为规定的格式</span></span><br><span class="line">      <span class="keyword">case</span> (id,color, name) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> rowkey = <span class="type">Bytes</span>.toBytes(id)</span><br><span class="line">        <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(rowkey)</span><br><span class="line">        put.addColumn(<span class="type">Bytes</span>.toBytes(<span class="string">&quot;info&quot;</span>), <span class="type">Bytes</span>.toBytes(color), <span class="type">Bytes</span>.toBytes(name))<span class="comment">//获取put对象，并加入需要的加入的value信息</span></span><br><span class="line">        (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(rowkey), put)<span class="comment">//转化为规定的格式类型</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(configuration)	<span class="comment">//新建一个带有HBase信息的配置文件</span></span><br><span class="line">    jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])<span class="comment">//设置输出表格式类型</span></span><br><span class="line">    jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>,<span class="string">&quot;fruit&quot;</span>)<span class="comment">//设置输出到哪个表</span></span><br><span class="line"></span><br><span class="line">    putRDD.saveAsHadoopDataset(jobConf)<span class="comment">//此方法只支持k-v类型的rdd，所以除了put，还要写上rowkey</span></span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//创建Hbase表</span></span><br><span class="line">  <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">    admin.disableTable(fruitTable)</span><br><span class="line">    admin.deleteTable(fruitTable)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.createTable(tableDescr)</span><br><span class="line"></span><br></pre></td></tr></table></figure>





<p>foreachPartition算子以分区为单位。但是也可能出现OOM。</p>
<p>tableInputFormat、ImmutableBytesWritable–Result  11:39</p>
<p>ajex需要有回调函数，在请求相应后执行。JdbcRDD中的参数中也有回调函数。</p>
<p>spark三大数据结构：</p>
<pre><code>RDD： 弹性分布式数据集

累加器：分布式共享只写数据（只有Driver端能读到全部数据），如果执行多次行动算子，会出现多加的情况（可以用cache）；如果不执行行动算子，那么不会触发累加器

广播变量：分布式共享只读数据（将数据发往executor，所有task从executor中获取数据,减少了数据冗余，加快数据获取。val breadcast = sc.broadcast（List（））生成广播数据对象，broadcast.value获取数据）
</code></pre>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">spark自带的累加器<span class="type">LongAccumulator</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Accumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> accumulator = sc.longAccumulator(<span class="string">&quot;acc&quot;</span>)</span><br><span class="line">    rdd.foreach&#123;</span><br><span class="line">      num =&gt; accumulator.add(num)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(accumulator.value)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">自定义累加器（继承<span class="type">AccumulatorV2</span>类，并<span class="number">6</span>个重写方法）</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MyAccumulator</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;xxx&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> accumulator = <span class="keyword">new</span> <span class="type">BlackListAccumulator</span></span><br><span class="line">    sc.register(accumulator,<span class="string">&quot;acc&quot;</span>)	<span class="comment">//注册累加器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;hive&quot;</span>,<span class="string">&quot;hadoop&quot;</span>,<span class="string">&quot;scale&quot;</span>,<span class="string">&quot;spark&quot;</span>))</span><br><span class="line">    rdd.foreach&#123;</span><br><span class="line">      word =&gt; accumulator.add(word)<span class="comment">//会对accumulator进行闭包检测，调用writeReplace方法</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    println(accumulator.value)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlackListAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">HashSet</span>[<span class="type">String</span>]]</span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> set = <span class="keyword">new</span> util.<span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = set.isEmpty<span class="comment">//判断累加器是否为空，不为空则抛异常</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, util.<span class="type">HashSet</span>[<span class="type">String</span>]] = <span class="keyword">new</span> <span class="type">BlackListAccumulator</span>		<span class="comment">//新建累加器，执行reset和isZero方法，之后发往executor</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = set.clear()	<span class="comment">//将新建的累加器清空，然后调用isZero方法</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = <span class="keyword">if</span>(v.contains(<span class="string">&quot;h&quot;</span>)) set.add(v)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, util.<span class="type">HashSet</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = set.addAll(other.value)	<span class="comment">//在Driver归并</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: util.<span class="type">HashSet</span>[<span class="type">String</span>] = set	<span class="comment">//获取结果</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>**闭包检测的源码见视频86**：底层在系列化对象时（闭包检测时），会调用AccumylatorV2类中的writeReplace方法。clean是闭包检测的方法，通过反射获取函数（函数最终会变成类）的类名，判断类名是否包含$anonfun$字段检测该类是否闭包；如果闭包，则checkSerializable方法检查是否可以序列化；最后在java的IO类的writeObject0方法中调用writeReplace方法。

kafkaProducer包含Sender对象，Sender类包含RecordAccumulator类属性对象，RecordAccumulator类包含Deque(双端队列）类属性对象.
</code></pre>
<p>自定义分区需要继承Partitioner类，重写numPartitions和getPartition方法。</p>
<p>Spark中读取的json文件是一行一个json对象，与普通的json对象不同。因为Spark按行读取文件。</p>
<p>tasknotserializable</p>
<p>11:37？？ 任务是在stage中划分的，一个stage根据分区划分对应数量的task；所以只有shuffle会产生新的stage，才会生成新的task。</p>
<pre><code>一个节点可以对应多个executor，一个executor可以有多个core，一个core可以执行一个task。

一般task任务数量是核心数量的2~3倍最好，因为有数据本地化原则，将任务分配到数据所在的节点，移动数据不如移动任务。
</code></pre>
<p><img src="F:/Typora/图片/333.PNG" alt="333"></p>
<p>管理项目打包方式是pom，子项目打包方式是jar</p>
<p>jackson		gson</p>
<p>在对数据进行累加的情况下，用累加器可以不用进行shuffle，提高了效率。累加器只有在执行行动算子时，才会执行。所以要避免出现没有执行行动算子，拿不到值的情况，也要避免多次使用行动算子，导致累加器重复执行，数据重复的问题。可以在累加器之后执行persist&#x2F;cache进行缓存，这样就不会重复执行累加器。</p>
<p>10:32为什么不重载，scala支持多态么？</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">CJ</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark/">http://example.com/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Hexo</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/%E9%A1%B9%E7%9B%AE%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/" title="项目知识笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">项目知识笔记</div></div></a></div><div class="next-post pull-right"><a href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E6%97%B6/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/" title="Spark内核解析"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark内核解析</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">CJ</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">419</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">38</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop%E5%8E%86%E5%8F%B2%EF%BC%9A"><span class="toc-number">1.</span> <span class="toc-text">Hadoop历史：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-core"><span class="toc-number">2.</span> <span class="toc-text">spark-core</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E5%AD%90%E5%92%8C%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">算子和分区器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">源码解析</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/MySQL/%E6%B3%A8%E8%A7%A3@Select%E5%92%8C@Insert/" title="注解@Select和@Insert">注解@Select和@Insert</a><time datetime="2023-05-06T05:48:28.906Z" title="发表于 2023-05-06 13:48:28">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%90%8E%E7%AB%AF%E6%A1%86%E6%9E%B6/%E6%B3%A8%E8%A7%A3@EnableAutoConfiguration/" title="注解@EnableAutoConfiguration">注解@EnableAutoConfiguration</a><time datetime="2023-05-06T05:48:06.027Z" title="发表于 2023-05-06 13:48:06">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A6%BB%E7%BA%BF/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E6%A1%86%E6%9E%B6/" title="大数据集群监控框架">大数据集群监控框架</a><time datetime="2023-05-06T05:42:56.298Z" title="发表于 2023-05-06 13:42:56">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/HashMap%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98%E5%8F%8AConcurrentHashMap%E5%8E%9F%E7%90%86/" title="HashMap并发问题及ConcurrentHashMap原理">HashMap并发问题及ConcurrentHashMap原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/05/06/%E9%AB%98%E5%B9%B6%E5%8F%91/Stream%E5%8E%9F%E7%90%86/" title="Stream原理">Stream原理</a><time datetime="2023-05-06T05:31:21.103Z" title="发表于 2023-05-06 13:31:21">2023-05-06</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By CJ</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>