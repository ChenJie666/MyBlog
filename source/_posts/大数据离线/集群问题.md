---
title: 集群问题
categories:
- 大数据离线
---
### 问题一
- 现象：Grafana触发了报警，但是没有发送报警信息，journalctl -u grafana -n 20 查看日志如下
```
11月 01 13:54:01 cos-bigdata-mysql grafana-server[21140]: t=2021-11-01T13:54:01+0800 lvl=info msg=Rendering logger=rendering path="d-solo/Jui9KoK7z/bigdata-server-process-alert?orgId=1&panelId=2"
11月 01 13:54:01 cos-bigdata-mysql grafana-server[21140]: t=2021-11-01T13:54:01+0800 lvl=eror msg="Alert Panic" logger=alerting.engine error="runtime error: invalid memory address or nil pointer dereference" stack="[engine.go:181 panic.go:1038 panic.go:221 signal_unix.go:735 rendering.go:216 rendering.go:178 notifier.go:218 notifier.go:117 result_handler.go:102 engine.go:224]"
```
- 原因：不知道什么原因，网上查询不到。
- 解决：重启grafana即可

### 问题二
- 现象：Grafana Alert推送告警图片失败，日志如下
```
11月 01 14:08:01 cos-bigdata-mysql grafana-server[29048]: t=2021-11-01T14:08:01+0800 lvl=info msg=Rendering logger=rendering renderer=plugin path="d-solo/Jui9KoK7z/bigdata-server-process-alert?orgId=1&panelId=2"
11月 01 14:08:01 cos-bigdata-mysql grafana-server[29048]: t=2021-11-01T14:08:01+0800 lvl=eror msg="Render request failed" logger=plugins.backend pluginId=grafana-image-renderer url="http://localhost:3000/d-solo/Jui9KoK7z/bigdata-server-process-alert?orgId=1&panelId=2&render=1" error="Error: Failed to launch the browser process!
/opt/module/grafana-8.2.2/data/plugins/grafana-image-renderer/chrome-linux/chrome: error while loading shared libraries: libatk-1.0.so.0: cannot open shared object file: No such file or directory


TROUBLESHOOTING: https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md
"
11月 01 14:08:01 cos-bigdata-mysql grafana-server[29048]: t=2021-11-01T14:08:01+0800 lvl=eror msg="Failed to render and upload alert panel image." logger=alerting.notifier ruleId=2 error="rendering failed: Error: Failed to launch the browser process!
/opt/module/grafana-8.2.2/data/plugins/grafana-image-renderer/chrome-linux/chrome: error while loading shared libraries: libatk-1.0.so.0: cannot open shared object file: No such file or directory


TROUBLESHOOTING: https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md
"
```
- 原因：环境缺少部分依赖包，需要进行安装
- 解决：通过命令`ldd grafana-8.2.2/data/plugins/grafana-image-renderer/chrome-linux/chrome`查看缺失的环境依赖并安装。
```
yum install -y libatk-1.0.so.0 libatk-bridge-2.0.so.0 libcups.so.2 libxcb.so.1 libxkbcommon.so.0 libX11.so.6 libXcomposite.so.1 libXdamage.so.1 libXext.so.6 libXfixes.so.3 libXrandr.so.2 libgbm.so.1 libpango-1.0.so.0 libcairo.so.2 libatspi.so.0 libxshmfence.so.1
```
重新安装插件 grafana-image-renderer。
又出现了新问题
```
11月 01 14:45:00 cos-bigdata-mysql grafana-server[30508]: t=2021-11-01T14:45:00+0800 lvl=info msg=Rendering logger=rendering renderer=plugin path="d-solo/Jui9KoK7z/bigdata-server-process-alert?orgId=1&from=1635747295710&to=1635749095710&panelId=2&width=1000&height=500&tz=Asia%2FShanghai"
11月 01 14:45:00 cos-bigdata-mysql grafana-server[30508]: t=2021-11-01T14:45:00+0800 lvl=eror msg="Rendering failed." logger=context userId=1 orgId=1 uname=admin error="rpc error: code = Canceled desc = grpc: the client connection is closing"
11月 01 14:45:00 cos-bigdata-mysql grafana-server[30508]: t=2021-11-01T14:45:00+0800 lvl=eror msg="Request Completed" logger=context userId=1 orgId=1 uname=admin method=GET path=/render/d-solo/Jui9KoK7z/bigdata-server-process-alert status=500 remote_addr=192.168.32.56 time_ms=5 size=1344 referer=
```
部署完成之后运行任务，又出来一个错
```
17-11-2021 15:07:18 CST customer INFO - com.alibaba.datax.common.exception.DataXException: Code:[Framework-03], Description:[DataX引擎配置错误，该问题通常是由于DataX安装错误引起，请联系您的运维解决 .].  - 在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数
```

### 问题三
- 现象：配置都一样，但是179节点的tez执行任务时报错如下
```
com.google.protobuf.ServiceException: java.io.IOException: DestHost:destPort bigdata3:43799 , LocalHost:localPort bigdata1/192.168.101.179:0. Failed on local exception: java.io.IOException: Connection reset by peer
Caused by: java.io.IOException: DestHost:destPort bigdata3:43799 , LocalHost:localPort bigdata1/192.168.101.179:0. Failed on local exception: java.io.IOException: Connection reset by peer
Caused by: java.io.IOException: Connection reset by peer
2022-04-19T16:43:25,448  INFO [f8b8e0df-ccd6-4f08-ab69-225144b91dc6 main] client.TezClient: Failed to retrieve AM Status via proxy
com.google.protobuf.ServiceException: java.io.EOFException: End of File Exception between local host is: "bigdata1/192.168.101.179"; destination host is: "bigdata2":38361; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
Caused by: java.io.EOFException: End of File Exception between local host is: "bigdata1/192.168.101.179"; destination host is: "bigdata2":38361; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
Caused by: java.io.EOFException]
```
- 原因：以上信息不准，需要在历史服务器上看具体的异常，可以发现缺少jar包导致
```
java.lang.NoClassDefFoundError: com/fasterxml/jackson/core/exc/InputCoercionException
Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.core.exc.InputCoercionException
```
- 解决：hive-env.sh文件中加载了Tez-0.10.1目录下的所有jar包，只需要将缺失的jackson-core-2.7.8.jarr和jackson-core-asl-1.9.13.jar放到Tez-0.10.1目录下即可。


### 问题四
- 现象：在执行任务中Datanode掉线，重启后报错如下
```
2022-06-06 00:40:43,940 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: bigdata1:9866:DataXceiver error processing WRITE_BLOCK operation  src: /192.168.101.180:50192 dst: /192.168.101.179:9866
java.io.IOException: Not ready to serve the block pool, BP-863358393-192.168.101.179-1626835277985.
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAndWaitForBP(DataXceiver.java:1412)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.checkAccess(DataXceiver.java:1428)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:717)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opWriteBlock(Receiver.java:173)
	at org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.processOp(Receiver.java:107)
	at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:292)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：字面理解为文件操作超租期，实际上就是data stream操作过程中文件被删掉了。之前也遇到过，通常是因为Mapred多个task操作同一个文件，一个task完成后删掉文件导致。
- 解决：过一会自己就好了。网上查询到的解决方法为
   1、datanode所在的linux服务器提高文件句柄参数；
   2、增加[HDFS](https://so.csdn.net/so/search?q=HDFS&spm=1001.2101.3001.7020)的datanode句柄参数：dfs.datanode.max.transfer.threads
```
<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>8192</value>
</property>
```





### 踩坑十六
- 现象：使用flume1.7.0向hadoop3.1.3导入数据，报如下错误
```
Exception in thread "SinkRunner-PollingRunner-DefaultSinkProcessor" java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1357)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1338)
	at org.apache.hadoop.conf.Configuration.setBoolean(Configuration.java:1679)
	at org.apache.flume.sink.hdfs.BucketWriter.open(BucketWriter.java:206)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:504)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:406)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：${HADOOP_HOME}/share/hadoop/common/lib下的guava包版本为guava-27.0-jre.jar，而flume-1.7.0使用的是低版本guava。版本不一致导致的错误。
- 解决：复制guava-27.0-jre.jar替换flume中低版本的guava包。


### 踩坑四十八
- 现象：Flume同步全量数据是报错如下
```
22 三月 2022 21:12:47,055 INFO  [hdfs-k1-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.close:357)  - Closing /origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp
22 三月 2022 21:13:00,650 WARN  [hdfs-k1-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.close:364)  - failed to close() HDFSWriter for file (/origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp). Exception follows.
java.io.IOException: Callable timed out after 10000 ms on file: /origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:682)
	at org.apache.flume.sink.hdfs.BucketWriter.close(BucketWriter.java:361)
	at org.apache.flume.sink.hdfs.BucketWriter$2.call(BucketWriter.java:275)
	at org.apache.flume.sink.hdfs.BucketWriter$2.call(BucketWriter.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:675)
	... 9 more
22 三月 2022 21:13:16,423 WARN  [hdfs-k1-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter.close:390)  - failed to rename() file (/origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp). Exception follows.
java.io.IOException: Callable timed out after 10000 ms on file: /origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:682)
	at org.apache.flume.sink.hdfs.BucketWriter.renameBucket(BucketWriter.java:614)
	at org.apache.flume.sink.hdfs.BucketWriter.close(BucketWriter.java:388)
	at org.apache.flume.sink.hdfs.BucketWriter$2.call(BucketWriter.java:275)
	at org.apache.flume.sink.hdfs.BucketWriter$2.call(BucketWriter.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:675)
	... 10 more
22 三月 2022 21:13:23,577 INFO  [hdfs-k1-roll-timer-0] (org.apache.flume.sink.hdfs.HDFSEventSink$1.run:382)  - Writer callback called.
22 三月 2022 21:16:33,706 WARN  [hdfs-k1-roll-timer-0] (org.apache.flume.sink.hdfs.BucketWriter$4.call:332)  - Renaming file: /origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp failed. Will retry again in 180 seconds.
java.io.IOException: Callable timed out after 10000 ms on file: /origin_data/compass/edb/edb_order/1970-01-01/edb.1647953566601.lzo.tmp
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:682)
	at org.apache.flume.sink.hdfs.BucketWriter.renameBucket(BucketWriter.java:614)
	at org.apache.flume.sink.hdfs.BucketWriter.access$1000(BucketWriter.java:58)
	at org.apache.flume.sink.hdfs.BucketWriter$4.call(BucketWriter.java:330)
	at org.apache.flume.sink.hdfs.BucketWriter$4.call(BucketWriter.java:315)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at org.apache.flume.sink.hdfs.BucketWriter.callWithTimeout(BucketWriter.java:675)
	... 10 more
22 三月 2022 21:19:21,821 INFO  [PollableSourceRunner-KafkaSource-r1] (org.apache.kafka.clients.consumer.internals.AbstractCoordinator.coordinatorDead:529)  - Marking the coordinator 2147483646 dead.
22 三月 2022 21:19:22,049 ERROR [PollableSourceRunner-KafkaSource-r1] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle:550)  - Error UNKNOWN_MEMBER_ID occurred while committing offsets for group edb_order_consumer
22 三月 2022 21:19:22,049 ERROR [PollableSourceRunner-KafkaSource-r1] (org.apache.flume.source.kafka.KafkaSource.doProcess:314)  - KafkaSource EXCEPTION, {}
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed due to group rebalance
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:552)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator$OffsetCommitResponseHandler.handle(ConsumerCoordinator.java:493)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:665)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:644)
	at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:358)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:968)
	at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:304)
	at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)
	at java.lang.Thread.run(Thread.java:748)
22 三月 2022 21:19:22,494 INFO  [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSCompressedDataStream.configure:64)  - Serializer = TEXT, UseRawLocalFileSystem = false
```
```
23 三月 2022 10:15:59,718 INFO  [agent-shutdown-hook] (org.apache.flume.instrumentation.MonitoredCounterGroup.stop:177)  - Shutdown Metric for type: SINK, name: k1. sink.event.drain.sucess == 2299667
23 三月 2022 10:15:59,735 WARN  [agent-shutdown-hook] (org.apache.kafka.common.network.Selector.poll:309)  - Unexpected error from cos-bigdata-test-hadoop-01/192.168.101.184; closing connection
java.lang.NullPointerException
	at org.apache.kafka.common.network.NetworkReceive.complete(NetworkReceive.java:67)
	at org.apache.kafka.common.network.KafkaChannel.read(KafkaChannel.java:135)
	at org.apache.kafka.common.network.Selector.poll(Selector.java:286)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:256)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:178)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.sendLeaveGroupRequest(AbstractCoordinator.java:575)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.maybeLeaveGroup(AbstractCoordinator.java:552)
	at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.close(AbstractCoordinator.java:541)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.close(ConsumerCoordinator.java:321)
	at org.apache.kafka.clients.ClientUtils.closeQuietly(ClientUtils.java:63)
	at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1277)
	at org.apache.kafka.clients.consumer.KafkaConsumer.close(KafkaConsumer.java:1258)
	at org.apache.flume.source.kafka.KafkaSource.doStop(KafkaSource.java:528)
	at org.apache.flume.source.BasicSourceSemantics.stop(BasicSourceSemantics.java:96)
	at org.apache.flume.source.PollableSourceRunner.stop(PollableSourceRunner.java:101)
	at org.apache.flume.lifecycle.LifecycleSupervisor.stop(LifecycleSupervisor.java:104)
	at org.apache.flume.node.Application.stop(Application.java:92)
	at org.apache.flume.node.Application$1.run(Application.java:343)
23 三月 2022 10:16:00,124 INFO  [agent-shutdown-hook] (org.apache.kafka.clients.consumer.internals.AbstractCoordinator.coordinatorDead:529)  - Marking the coordinator 2147483646 dead.
```
- 原因：估计是azkaban任务运行导致的内存和cpu占用。
- 解决：

### 踩坑四十九
- 问题：启动Flume后消费Kafka的数据一直没有反应，查看启动Flume时的日志，报错如下
```
Exception in thread "PollableSourceRunner-KafkaSource-r1" java.lang.OutOfMemoryError: Java heap space
	at org.apache.kafka.common.utils.Utils.toArray(Utils.java:268)
	at org.apache.kafka.common.utils.Utils.toArray(Utils.java:261)
	at org.apache.kafka.clients.consumer.internals.Fetcher.parseRecord(Fetcher.java:622)
	at org.apache.kafka.clients.consumer.internals.Fetcher.handleFetchResponse(Fetcher.java:566)
	at org.apache.kafka.clients.consumer.internals.Fetcher.access$000(Fetcher.java:69)
	at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:139)
	at org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:136)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)
	at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:380)
	at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:274)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:320)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:213)
	at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:193)
	at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:908)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:853)
	at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:200)
	at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：Flume的JVM配置的内存过小
- 解决：修改conf/flume-env.sh文件
```
export JAVA_OPTS="-Xms512m -Xmx1024m -Dcom.sun.management.jmxremote"
```


### 踩坑五十九
- 问题：Flume中的hdfs sink配置了回滚条件，但是实际罗盘文件却都是小文件。在flume的日志配置文件中配置日志存储位置，查看日志后发现如下异常
```
17 六月 2022 09:43:44,663 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.BucketWriter.append:516)  - Hit max consecutive under-replication rotations (30); will not continue rolling files under this path due to under-replication
```
- 原因：如果感知到文件正在进行块复制，那么Flume会滚动该文件并产生新的文件。而集群中有一个节点的Datanode进程掉线了(配置副本数为3且只有3个Datanode节点)，导致该路径被不同重试复制。
- 解决： 启动该掉线的节点  
或 配置让程序感知不到写的文件所在块正在复制，在任务文件中配置
```
a1.sinks.k1.hdfs.minBlockReplicas = 1
```

### 踩坑六十
- 问题：Flume启动kafka source任务，如果Kafka没有到达新数据，Flume消费不到任何数据，指定的消费者组没有消费者
```
Consumer group 'youmeng3-active' has no active members
```
- 原因：未添加消费者
- 解决：添加消费者


### 踩坑六十六
- 问题：Flume从Kafka读取飞燕的数据落盘到hdfs中，到13:30左右时报错如下
```
23 六月 2022 15:46:22,807 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: org.apache.flume.auth.SecurityException: Privileged action failed
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:451)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.flume.auth.SecurityException: Privileged action failed
	at org.apache.flume.auth.UGIExecutor.execute(UGIExecutor.java:49)
	at org.apache.flume.auth.KerberosAuthenticator.execute(KerberosAuthenticator.java:64)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:665)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: java.io.IOException: write beyond end of stream
	at com.hadoop.compression.lzo.LzopOutputStream.write(LzopOutputStream.java:134)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.flume.serialization.BodyTextEventSerializer.write(BodyTextEventSerializer.java:70)
	at org.apache.flume.sink.hdfs.HDFSCompressedDataStream.append(HDFSCompressedDataStream.java:125)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:540)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:537)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:668)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.flume.auth.UGIExecutor.execute(UGIExecutor.java:47)
	... 6 more
23 六月 2022 15:46:22,805 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.AbstractHDFSWriter.isUnderReplicated:99)  - Unexpected error while checking replication factor
java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.getNumCurrentReplicas(AbstractHDFSWriter.java:166)
	at org.apache.flume.sink.hdfs.AbstractHDFSWriter.isUnderReplicated(AbstractHDFSWriter.java:85)
	at org.apache.flume.sink.hdfs.BucketWriter.shouldRotate(BucketWriter.java:573)
	at org.apache.flume.sink.hdfs.BucketWriter.append(BucketWriter.java:508)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:406)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[192.168.101.184:9866,DS-8687386a-b9ab-4c73-84ed-9a9dac98662e,DISK], DatanodeInfoWithStorage[192.168.101.185:9866,DS-564db0fe-1d53-418f-937f-470faf52b712,DISK]], original=[DatanodeInfoWithStorage[192.168.101.184:9866,DS-8687386a-b9ab-4c73-84ed-9a9dac98662e,DISK], DatanodeInfoWithStorage[192.168.101.185:9866,DS-564db0fe-1d53-418f-937f-470faf52b712,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)
23 六月 2022 15:46:22,808 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:447)  - process failed
org.apache.flume.auth.SecurityException: Privileged action failed
	at org.apache.flume.auth.UGIExecutor.execute(UGIExecutor.java:49)
	at org.apache.flume.auth.KerberosAuthenticator.execute(KerberosAuthenticator.java:64)
	at org.apache.flume.sink.hdfs.BucketWriter$9.call(BucketWriter.java:665)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: write beyond end of stream
	at com.hadoop.compression.lzo.LzopOutputStream.write(LzopOutputStream.java:134)
	at java.io.OutputStream.write(OutputStream.java:75)
	at org.apache.flume.serialization.BodyTextEventSerializer.write(BodyTextEventSerializer.java:70)
	at org.apache.flume.sink.hdfs.HDFSCompressedDataStream.append(HDFSCompressedDataStream.java:125)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:540)
	at org.apache.flume.sink.hdfs.BucketWriter$7.call(BucketWriter.java:537)
	at org.apache.flume.sink.hdfs.BucketWriter$9$1.run(BucketWriter.java:668)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.flume.auth.UGIExecutor.execute(UGIExecutor.java:47)
	... 6 more
```
- 原因：这里涉及到hdfs的两个配置
   - `dfs.client.block.write.replace-datanode-on-failure.enable`：如果在写入管道中存在一个DataNode或者网络故障时，那么DFSClient将尝试从管道中删除失败的DataNode，然后继续尝试剩下的DataNodes进行写入。结果，管道中的DataNodes的数量在减少。(enable ：启用特性，disable：禁用特性) 该特性是在管道中添加新的DataNodes。当集群规模非常小时，例如3个节点或更少时，集群管理员可能希望将策略在默认配置文件里面设置为NEVER或者禁用该特性。否则，因为找不到新的DataNode来替换，用户可能会经历异常高的管道写入错误,导致追加文件操作失败。
   - `dfs.client.block.write.replace-datanode-on-failure.policy`：这个属性只有在dfs.client.block.write.replace-datanode-on-failure.enable设置true时有效：(**ALWAYS**：当一个存在的DataNode被删除时，总是添加一个新的DataNode; **NEVER**  ：永远不添加新的DataNode; **DEFAULT**：副本数是r，DataNode的数时n，只要r >= 3时，或者floor(r/2)大于等于n时，或 r>n时再添加一个新的DataNode，并且这个块是hflushed/appended)
- 解决：设置`dfs.client.block.write.replace-datanode-on-failure.policy=NEVER`

### 踩坑六十七
- 问题：Flume启动报错如下
```
org.apache.flume.ChannelException: Cannot commit transaction. Byte capacity allocated to store event body 640000.0reached. Please increase heap space/byte capacity allocated to the channel as the sinks may not be keeping up with the sources
	at org.apache.flume.channel.MemoryChannel$MemoryTransaction.doCommit(MemoryChannel.java:120)
	at org.apache.flume.channel.BasicTransactionSemantics.commit(BasicTransactionSemantics.java:151)
	at org.apache.flume.channel.ChannelProcessor.processEventBatch(ChannelProcessor.java:194)
	at org.apache.flume.source.kafka.KafkaSource.doProcess(KafkaSource.java:295)
	at org.apache.flume.source.AbstractPollableSource.process(AbstractPollableSource.java:60)
	at org.apache.flume.source.PollableSourceRunner$PollingRunner.run(PollableSourceRunner.java:133)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因： source 接收端的容量达到上线，无法再接收数据
- 解决：


### 踩坑六十八
- 问题：Flume在运行中报错如下
```
23 六月 2022 19:02:49,798 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.sink.hdfs.HDFSEventSink.process:447)  - process failed
java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:649)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:409)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:430)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
23 六月 2022 19:02:49,799 ERROR [SinkRunner-PollingRunner-DefaultSinkProcessor] (org.apache.flume.SinkRunner$PollingRunner.run:158)  - Unable to deliver event. Exception follows.
org.apache.flume.EventDeliveryException: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:451)
	at org.apache.flume.sink.DefaultSinkProcessor.process(DefaultSinkProcessor.java:67)
	at org.apache.flume.SinkRunner$PollingRunner.run(SinkRunner.java:145)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException: Timed out before HDFS call was made. Your hdfs.callTimeout might be set too low or HDFS calls are taking too long.
	at org.apache.flume.sink.hdfs.BucketWriter.checkAndThrowInterruptedException(BucketWriter.java:649)
	at org.apache.flume.sink.hdfs.BucketWriter.flush(BucketWriter.java:409)
	at org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:430)
	... 3 more
```
- 原因：
- 解决：


### 踩坑六十九
- 现象：Datax从hdfs往mysql导数据，但是卡在加载驱动这一步不动了
```
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.653 [main] WARN  Engine - prioriy set to 0, because NumberFormatException, the value is: null
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.655 [main] INFO  PerfTrace - PerfTrace traceId=job_-1, isEnable=false, priority=0
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.655 [main] INFO  JobContainer - DataX jobContainer starts job.
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.657 [main] INFO  JobContainer - Set jobId = 0
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.669 [job-0] INFO  HdfsReader$Job - init() begin...
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.927 [job-0] INFO  HdfsReader$Job - hadoopConfig details:{"finalParameters":[]}
28-06-2022 09:22:03 CST cb_sales2mysql INFO - 2022-06-28 09:22:03.927 [job-0] INFO  HdfsReader$Job - init() ok and end...
28-06-2022 09:22:03 CST cb_sales2mysql INFO - Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
```
- 原因：mysql表中出现了死锁，导致整张表锁死，无法写入数据
- 解决：解决mysql表的死锁问题


### 踩坑七十
- 现象：使用Fair Scheduler调度Tez任务时，经常发生异常如下
```
2022-06-29 16:32:37,779 [FATAL] [HistoryEventHandlingThread] |yarn.YarnUncaughtExceptionHandler|: Thread Thread[HistoryEventHandlingThread,5,main] threw an Error.  Shutting down now...
java.lang.VerifyError: Stack map does not match the one at exception handler 77
Exception Details:
  Location:
    com/fasterxml/jackson/databind/deser/std/StdDeserializer._parseDate(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/util/Date; @77: astore
  Reason:
    Type 'com/fasterxml/jackson/core/JsonParseException' (current frame, stack[0]) is not assignable to 'com/fasterxml/jackson/core/exc/StreamReadException' (stack map, stack[0])
  Current Frame:
    bci: @69
    flags: { }
    locals: { 'com/fasterxml/jackson/databind/deser/std/StdDeserializer', 'com/fasterxml/jackson/core/JsonParser', 'com/fasterxml/jackson/databind/DeserializationContext' }
    stack: { 'com/fasterxml/jackson/core/JsonParseException' }
  Stackmap Frame:
    bci: @77
    flags: { }
    locals: { 'com/fasterxml/jackson/databind/deser/std/StdDeserializer', 'com/fasterxml/jackson/core/JsonParser', 'com/fasterxml/jackson/databind/DeserializationContext' }
    stack: { 'com/fasterxml/jackson/core/exc/StreamReadException' }
  Bytecode:
    0x0000000: 2bb6 0035 aa00 0000 0000 0081 0000 0003
    0x0000010: 0000 000b 0000 007a 0000 0081 0000 0081
    0x0000020: 0000 0034 0000 0041 0000 0081 0000 0081
    0x0000030: 0000 0081 0000 0071 2a2b b600 11b6 0012
    0x0000040: 2cb6 006b b02b b600 4742 a700 223a 052c
    0x0000050: 2ab4 0002 2bb6 006e 126f 03bd 0004 b600
    0x0000060: 70c0 002d 3a06 1906 b600 4c42 bb00 7159
    0x0000070: 21b7 0072 b02a 2cb6 0073 c000 71b0 2a2b
    0x0000080: 2cb6 0074 b02c 2ab4 0002 2bb6 0025 c000
    0x0000090: 71b0                                   
  Exception Handler Table:
    bci [69, 74] => handler: 77
    bci [69, 74] => handler: 77
  Stackmap Table:
    same_frame(@56)
    same_frame(@69)
    same_locals_1_stack_item_frame(@77,Object[#359])
    append_frame(@108,Long)
    chop_frame(@117,1)
    same_frame(@126)
    same_frame(@133)

	at com.fasterxml.jackson.databind.deser.std.JdkDeserializers.<clinit>(JdkDeserializers.java:26)
	at com.fasterxml.jackson.databind.deser.BasicDeserializerFactory.findDefaultDeserializer(BasicDeserializerFactory.java:1852)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.findStdDeserializer(BeanDeserializerFactory.java:167)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerFactory.createBeanDeserializer(BeanDeserializerFactory.java:131)
	at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer2(DeserializerCache.java:411)
	at com.fasterxml.jackson.databind.deser.DeserializerCache._createDeserializer(DeserializerCache.java:349)
	at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCache2(DeserializerCache.java:264)
	at com.fasterxml.jackson.databind.deser.DeserializerCache._createAndCacheValueDeserializer(DeserializerCache.java:244)
	at com.fasterxml.jackson.databind.deser.DeserializerCache.findValueDeserializer(DeserializerCache.java:142)
	at com.fasterxml.jackson.databind.DeserializationContext.findRootValueDeserializer(DeserializationContext.java:476)
	at com.fasterxml.jackson.databind.ObjectReader._prefetchRootDeserializer(ObjectReader.java:2094)
	at com.fasterxml.jackson.databind.ObjectReader.forType(ObjectReader.java:681)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:799)
	at com.sun.jersey.api.client.ClientResponse.getEntity(ClientResponse.java:634)
	at com.sun.jersey.api.client.ClientResponse.getEntity(ClientResponse.java:586)
	at org.apache.hadoop.yarn.client.api.impl.TimelineWriter.putEntities(TimelineWriter.java:93)
	at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:177)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.handleEvents(ATSHistoryLoggingService.java:354)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService.access$600(ATSHistoryLoggingService.java:59)
	at org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService$1.run(ATSHistoryLoggingService.java:206)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：java.lang.VerifyError 是说 JVM 在加载一个类时，会去校验类的正确性，只有类文件不合法才会报这个Error。可能是tez中的jackson包的版本(2.10.0)与hadoop中的jackson包版本(2.7.8)不一致导致的。在2.10.0版本中存在com/fasterxml/jackson/core/exc/StreamReadException类，但是2.7.8中不存在，找不到类会导致编译不通过。
   而集群中只有179节点的tez中添加了jackson-core-2.10.0，所以azkaban分配到该节点就会发生这个错误。
- 解决：替换tez中的jackson包的版本与hadoop中的一致(hadoop3.1.3的jackson版本为2.7.8)。
- 参考：[Android 不想和你说话，抛了个 java.lang.VerifyError - Yrom's](https://yrom.net/blog/2016/08/22/java-lang-verifyerror-on-android/)

### 踩坑七十一
- 现象：Kafka的topic有两个副本，但是缺失了一个副本同步。
```
[root@cos-bigdata-hadoop-01 kafka-2.11]# bin/kafka-topics.sh --zookeeper 192.168.101.179:2181 --topic status-model --describe
Topic:status-model	PartitionCount:3	ReplicationFactor:2	Configs:
	Topic: status-model	Partition: 0	Leader: 0	Replicas: 0,2	Isr: 2,0
	Topic: status-model	Partition: 1	Leader: 1	Replicas: 1,0	Isr: 0,1
	Topic: status-model	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2
```
- 原因：ISR丢失(抖动)
- 解决：

### 踩坑七十二
- 现象：使用其他集群的kafka客户端来访问本集群的topic。如果该topic只有一个分区，那么可以成功访问，如果该topic有多个分区，那么就会报错如下
```
[root@cos-bigdata-test-hadoop-02 kafka_2.11]# bin/kafka-console-consumer.sh --bootstrap-server 192.168.101.181:9092 --topic compass-edb-sales-prod --from-beginning
[2022-08-16 11:32:12,460] WARN Received unknown topic or partition error in ListOffset request for partition compass-edb-sales-prod-1. The topic/partition may not exist or the user may not have Describe access to it. (org.apache.kafka.clients.consumer.internals.Fetcher)
[2022-08-16 11:32:12,462] WARN Received unknown topic or partition error in ListOffset request for partition compass-edb-sales-prod-0. The topic/partition may not exist or the user may not have Describe access to it. (org.apache.kafka.clients.consumer.internals.Fetcher)
[2022-08-16 11:32:12,462] WARN Received unknown topic or partition error in ListOffset request for partition compass-edb-sales-prod-2. The topic/partition may not exist or the user may not have Describe access to it. (org.apache.kafka.clients.consumer.internals.Fetcher)
```
- 原因：猜测是读取使用的命令中指定的是Kafka集群，而没有指定zookeeper，所以找不到topic对应的多个分区所在的节点。在Zookeeper的节点/borkers/topics/[topic_name]/partitions/0/state中记录了谁是leader，有哪些服务器可用；![image.png](集群问题.assets54318a17d494c2c94ffef958765315a.png)
- 解决：

### 踩坑七十三
- 设置spark.executor.instances配置时，报错
```
[2022-09-17 09:59:04] Error while processing statement: Cannot modify spark.executor.cores at runtime. It is not in list of params that are allowed to be modified at runtime
```
- 原因：
- 解决：hive-site.conf添加如下
```
    <property>
       <name>hive.security.authorization.sqlstd.confwhitelist</name>        <value>mapred.*|hive.*|mapreduce.*|spark.*|tez.*</value>
    </property>
    <property>
        <name>hive.security.authorization.sqlstd.confwhitelist.append</name>
        <value>mapred.*|hive.*|mapreduce.*|spark.*|tez.*</value>
    </property>
```


### 踩坑七十四
- 问题：namenode高可用配置后，运行了6个小时，active namenode转为standby，但是没有选举出新的active namenode节点
```
2023-04-21 09:15:34,484 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2023-04-21 09:15:34,493 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn2, ipcAddress=cos-bigdata-test-flink-02/192.168.101.194:8020, httpAddress=http://cos-bigdata-test-flink-02:9870], try next.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error
2023-04-21 09:15:34,497 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn3, ipcAddress=cos-bigdata-test-flink-03/192.168.101.195:8020, httpAddress=http://cos-bigdata-test-flink-03:9870], try next.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error
2023-04-21 09:15:34,504 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:416)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:475)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$300(EditLogTailer.java:441)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:458)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:454)
Caused by: java.io.IOException: Cannot find any valid remote NN to service request!
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:573)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：未找到原因
- 解决：关闭hdfs集群`stop-dfs.sh`，格式化zookeeper `hdfs zkfc -formatZK`，重启hdfs集群 `start-dfs.sh`后正常。此时再kill掉namenode后可以正常进行故障转移。
