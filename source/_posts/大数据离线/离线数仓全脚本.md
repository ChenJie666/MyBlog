---
title: 离线数仓全脚本
categories:
- 大数据离线
---
flume
=====================================================
file-kafka
a1.sources = r1
a1.channels = c1 c2

a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume-1.7.0/taildir_position.json
a1.sources.r1.filegroups = f1		//监控组，多个组以空格分隔
a1.sources.r1.filegroups.f1 = /tmp/logs/app.*  	//
a1.sources.r1.fileHeader = false

a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = com.atguigu.interceptor.ETLInterceptor$ETLBuilder
a1.sources.r1.interceptors.i2.type = com.atguigu.interceptor.LogInterceptor$LogBuilder


a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.topic_start = c1
a1.sources.r1.selector.mapping.topic_event = c2



a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c1.kafka.topic = topic_start
a1.channels.c1.parseAsFlumeEvent = false

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092
a1.channels.c2.kafka.topic = topic_event
a1.channels.c2.parseAsFlumeEvent = false

a1.sources.r1.channels = c1 c2



-----------------------------
file-kafka-hdfs.conf

# agent
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# source
a1.sources.r1.channels = c1
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume-1.7.0/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/q6/*.log
a1.sources.r1.fileHeader = false
a1.sources.ri.maxBatchCount = 1000

# interceptor
a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = com.hxr.flume.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.hxr.flume.LogTypeInterceptor$Builder

# multiplexing selector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.Log_Q6 = c1

# channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
a1.channels.c1.kafka.topic = log_q6
a1.channels.c1.parseAsFlumeEvent = false


# sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/device/logs/%Y-%m-%d/
a1.sinks.k1.hdfs.filePrefix = q6-
# a1.sinks.k1.hdfs.round = true
# a1.sinks.k1.hdfs.roundValue = 2
# a1.sinks.k1.hdfs.roundUnit = hour
a1.sinks.k1.hdfs.rollInterval = 3600
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdsf.rollCount = 0
a1.channels.c1.hdfs.useLocalTimeStamp = true

# codec LZOP
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k1.hdfs.fileType = CompressedStream

# 拼装
a1.sinks.k1.channel = c1
--------------------


sqoop导入业务数据
用户行为数据仓库需求完成
===========================================================================================================================
从mysql中导入八张表的数据的脚本
```sh
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'


if [ -n "$2" ];then
    do_date=$2
fi

import_data(){
/opt/module/sqoop-1.4.6/bin/sqoop import \
--connect jdbc:mysql://hadoop102:3306/$db \
--username root \
--password abc123 \
--target-dir /origin_data/gmall/db/$1/$do_date \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by '	' \
--query "$2 and \$CONDITIONS"
--compress \
--compression-codec lzop \
--null-string '\N' \
--null-non-string '\N'
}

import_order_info(){
import_data order_info "select 
                        id,
                        total_amount,
                        order_status,
                        user_id,
                        payment_way,
                        out_trade_no,
                        create_time,
                        operate_time
                        from order_info
                        where (date_format(create_time,'%Y-%m-%d')='$do_date') or (date_format(operate_time,'%Y-%m-%d')='$do_date')"
}

import_order_detail(){
import_data order_detail "select
                        od.id, 
                        order_id, 
                        user_id, 
                        sku_id, 
                        sku_name, 
                        order_price, 
                        sku_num, 
                        oi.create_time
                        from order_detail od join order_info oi on od.order_id=oi.id
                        where date_format(oi.create_time,'%Y-%m-%d')='$do_date'
"
}

import_user_info(){
import_data user_info "select
                        id, name, birthday, gender, email, user_level, 
                        create_time
                        from user_info 
                        where 1=1
                        "
}

import_sku_info(){
  import_data "sku_info" "select 
id, spu_id, price, sku_name, sku_desc, weight, tm_id,
category3_id, create_time
  from sku_info where 1=1"
}

import_base_category1(){
  import_data "base_category1" "select 
id, name from base_category1 where 1=1"
}

import_base_category2(){
  import_data "base_category2" "select 
id, name, category1_id from base_category2 where 1=1"
}

import_base_category3(){
  import_data "base_category3" "select id, name, category2_id from base_category3 where 1=1"
}

import_payment_info(){
  import_data "payment_info"   "select 
    id,  
    out_trade_no, 
    order_id, 
    user_id, 
    alipay_trade_no, 
    total_amount,  
    subject, 
    payment_type, 
    payment_time 
  from payment_info 
  where DATE_FORMAT(payment_time,'%Y-%m-%d')='$do_date'"
}


case $1 in
order_info)import_order_info;;
order_detail)import_order_detail;;
user_info)import_user_info;;
sku_info)import_sku_info;;
base_category1)import_base_category1;;
base_category2)import_base_category2;;
base_category3)import_base_category3;;
payment_info)import_payment_info;;
all)
import_order_info
import_order_detail
import_user_info
import_sku_info
import_base_category1
import_base_category2
import_base_category3
import_payment_info;;
esac
```



ods
======================================================
建库语句
create database gmall;


ods_start_log
******************************************************
建表语句（start表）
create external table ods_start_log(
line string
)
partitioned by (dt string)
stored as inputformat "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
location '/warehouse/gmall/ods/ods_start_log';

装载数据
load data inpath '/origin_data/gmall/log/topic_start/2019-07-05' overwrite into table ods_start_log partition(dt='2019-07-05');

装载脚本(ods_start_log)
#!/bin/bash
db=gmall
do_date=`date -d 'yesterday' + %F`
hive=/opt/module/hive-1.2.1/bin/hive

if [ -n "$1" ];then
    do_date=$1
fi

sql="
load data inpath '/origin_data/gmall/log/topic_start/$do_date' overwrite into table ${db}.ods_start_log partition(dt='$do_date');
"

$hive -e "$sql"




ods_event_log
*************************************************
建表语句（event表）
use gmall;
create external table ods_event_log(
line string
)
partitioned by (dt string)
stored as inputformat "com.hadoop.mapred.DeprecatedLzoTextInputFormat"
OUTPUTFORMAT "org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat"
location '/warehouse/gmall/ods/ods_event_log';

装载数据
load data inpath '/origin_data/gmall/log/topic_event/2019-07-05' overwrite into table ods_event_log partition(dt='2019-07-05');

#装载脚本(ods_event_log)
#!/bin/bash
db=gmall
do_date=`date -d '-1 day' +%F`
hive=/opt/module/hive-1.2.1/bin/hive

if [ -n "$1" ];then
    do_date=$1
fi

sql="
load data inpath '/origin_data/gmall/log/topic_event/$do_date' overwrite into table ${db}.ods_event_log partition(dt='$do_date');
"

$hive -e "$sql";




dwd
======================================================
dwd_start_log
********************************************************
建表语句（dwd_start_log）
use gmall;
create external table dwd_start_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`entry` string, 
`open_ad_type` string, 
`action` string, 
`loading_time` string, 
`detail` string, 
`extend1` string
)
partitioned by (dt string)
location '/warehouse/gmall/dwd/dwd_start_log';


装载数据
insert overwrite table dwd_start_log partition(dt='2019-07-05')
select 
    get_json_object(line,'$.mid') mid_id,
    get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t')  $db_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
from ods_start_log 
where dt='2019-07-05';


#装载脚本(dwd_start_log)
#!/bin/bash
db=gmall
do_date=`date -d '-1 day' +%F`
hive=/opt/module/hive-1.2.1/bin/hive

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dwd_start_log partition(dt='$do_date')
select 
    get_json_object(line,'$.mid') mid_id,
    get_json_object(line,'$.uid') user_id,
    get_json_object(line,'$.vc') version_code,
    get_json_object(line,'$.vn') version_name,
    get_json_object(line,'$.l') lang,
    get_json_object(line,'$.sr') source,
    get_json_object(line,'$.os') os,
    get_json_object(line,'$.ar') area,
    get_json_object(line,'$.md') model,
    get_json_object(line,'$.ba') brand,
    get_json_object(line,'$.sv') sdk_version,
    get_json_object(line,'$.g') gmail,
    get_json_object(line,'$.hw') height_width,
    get_json_object(line,'$.t')  $db_time,
    get_json_object(line,'$.nw') network,
    get_json_object(line,'$.ln') lng,
    get_json_object(line,'$.la') lat,
    get_json_object(line,'$.entry') entry,
    get_json_object(line,'$.open_ad_type') open_ad_type,
    get_json_object(line,'$.action') action,
    get_json_object(line,'$.loading_time') loading_time,
    get_json_object(line,'$.detail') detail,
    get_json_object(line,'$.extend1') extend1
from ${db}.ods_start_log 
where dt='$do_date';
"

$hive -e "$sql"


dwd_event_log
*************************************************************
创建UDF和UDTF函数，打包后放到hive的lib库中
add jar /opt/module/hive-1.2.1/lib/gmall-hive-1.0-SNAPSHOT.jar;
create temporary function base_analizer as "com.atguigu.gmall.hive.udf.BaseEventUDF";
create temporary function flat_analizer as "com.atguigu.gmall.hive.udf.BaseEventUDTF";

建表语句(中间表)
create table dwd_base_event_log(
    `mid_id` string,
    `user_id` string, 
    `version_code` string, 
    `version_name` string, 
    `lang` string, 
    `source` string, 
    `os` string, 
    `area` string, 
    `model` string,
    `brand` string, 
    `sdk_version` string, 
    `gmail` string, 
    `height_width` string, 
    ` $db_time` string, 
    `network` string, 
    `lng` string, 
    `lat` string,
    event_name string,
    event_json string,
    server_time string
)
partitioned by (dt string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_base_event_log';


装载数据（中间表）
insert overwrite table dwd_base_event_log partition(dt="2019-07-05")
select 
    base_analizer(line,'mid') as mid_id,
    base_analizer(line,'uid') as user_id,
    base_analizer(line,'vc') as version_code,
    base_analizer(line,'vn') as version_name,
    base_analizer(line,'l') as lang,
    base_analizer(line,'sr') as source,
    base_analizer(line,'os') as os,
    base_analizer(line,'ar') as area,
    base_analizer(line,'md') as model,
    base_analizer(line,'ba') as brand,
    base_analizer(line,'sv') as sdk_version,
    base_analizer(line,'g') as gmail,
    base_analizer(line,'hw') as height_width,
    base_analizer(line,'t') as  $db_time,
    base_analizer(line,'nw') as network,
    base_analizer(line,'ln') as lng,
    base_analizer(line,'la') as lat,
    event_name,
    event_json,
    base_analizer(line,'st') as server_time
from ods_event_log lateral view flat_analizer(base_analizer(line,"et")) tmp_table as event_name,event_json
where dt="2019-07-05";


数据脚本
#！/bin/hive
do_date=`date -d '-1 day' +%F`
db=gmall
hive=/opt/module/hive-1.2.1/bin/hive

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dwd_base_event_log partition(dt='$do_date')
select 
    base_analizer(line,'mid') as mid_id,
    base_analizer(line,'uid') as user_id,
    base_analizer(line,'vc') as version_code,
    base_analizer(line,'vn') as version_name,
    base_analizer(line,'l') as lang,
    base_analizer(line,'sr') as source,
    base_analizer(line,'os') as os,
    base_analizer(line,'ar') as area,
    base_analizer(line,'md') as model,
    base_analizer(line,'ba') as brand,
    base_analizer(line,'sv') as sdk_version,
    base_analizer(line,'g') as gmail,
    base_analizer(line,'hw') as height_width,
    base_analizer(line,'t') as  $db_time,
    base_analizer(line,'nw') as network,
    base_analizer(line,'ln') as lng,
    base_analizer(line,'la') as lat,
    event_name,
    event_json,
    base_analizer(line,'st') as server_time
from ${db}.ods_event_log lateral view flat_analizer(base_analizer(line,"et")) tmp_table as event_name,event_json
where dt='$do_date';
"

$hive -e "$sql"


dwd层的事件表
****************************************************
建表语句
CREATE EXTERNAL TABLE dwd_display_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
` $db_time` string,
`network` string,
`lng` string,
`lat` string,
`action` string,
`goodsid` string,
`place` string,
`extend1` string,
`category` string,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_display_log/';

CREATE EXTERNAL TABLE dwd_newsdetail_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string, 
` $db_time` string,  
`network` string, 
`lng` string, 
`lat` string, 
`entry` string,
`action` string,
`goodsid` string,
`showtype` string,
`news_staytime` string,
`loading_time` string,
`type1` string,
`category` string,
`server_time` string)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_newsdetail_log/';

CREATE EXTERNAL TABLE dwd_loading_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string,
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`action` string,
`loading_time` string,
`loading_way` string,
`extend1` string,
`extend2` string,
`type` string,
`type1` string,
`server_time` string)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_loading_log/';

CREATE EXTERNAL TABLE dwd_ad_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`entry` string,
`action` string,
`content` string,
`detail` string,
`ad_source` string,
`behavior` string,
`newstype` string,
`show_style` string,
`server_time` string)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_ad_log/';

CREATE EXTERNAL TABLE dwd_notification_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string,
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`action` string,
`noti_type` string,
`ap_time` string,
`content` string,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_notification_log/';

CREATE EXTERNAL TABLE dwd_active_foreground_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
` $db_time` string,
`network` string,
`lng` string,
`lat` string,
`push_id` string,
`access` string,
`server_time` string)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_foreground_log/';

CREATE EXTERNAL TABLE dwd_active_background_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
 `height_width` string,
` $db_time` string,
`network` string,
`lng` string,
`lat` string,
`active_source` string,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_background_log/';

CREATE EXTERNAL TABLE dwd_comment_log(
`mid_id` string,
`user_id` string,
`version_code` string,
`version_name` string,
`lang` string,
`source` string,
`os` string,
`area` string,
`model` string,
`brand` string,
`sdk_version` string,
`gmail` string,
`height_width` string,
` $db_time` string,
`network` string,
`lng` string,
`lat` string,
`comment_id` int,
`userid` int,
`p_comment_id` int, 
`content` string,
`addtime` string,
`other_id` int,
`praise_count` int,
`reply_count` int,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_comment_log/';

CREATE EXTERNAL TABLE dwd_favorites_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`id` int, 
`course_id` int, 
`userid` int,
`add_time` string,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_favorites_log/';

CREATE EXTERNAL TABLE dwd_praise_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`id` string, 
`userid` string, 
`target_id` string,
`type` string,
`add_time` string,
`server_time` string
)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_praise_log/';

CREATE EXTERNAL TABLE dwd_error_log(
`mid_id` string,
`user_id` string, 
`version_code` string, 
`version_name` string, 
`lang` string, 
`source` string, 
`os` string, 
`area` string, 
`model` string,
`brand` string, 
`sdk_version` string, 
`gmail` string, 
`height_width` string,  
` $db_time` string,
`network` string, 
`lng` string, 
`lat` string, 
`errorBrief` string, 
`errorDetail` string, 
`server_time` string)
PARTITIONED BY (dt string)
location '/warehouse/gmall/dwd/dwd_error_log/';

装载数据脚本：
***************************************************************
#!/bin/bash
do_date=`date -d '-1 day' +%F`
hive='/opt/module/hive-1.2.1/bin/hive'
db='gmall'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dwd_display_log partition(dt='$do_date')
select 
    mid_id,
    user_id, 
    version_code, 
    version_name, 
    lang, 
    source, 
    os, 
    area, 
    model,
    brand, 
    sdk_version, 
    gmail, 
    height_width,  
     $db_time,
    network, 
    lng, 
    lat, 
    get_json_object(event_json,'$.kv.action'),
    get_json_object(event_json,'$.kv.goodsid'),
    get_json_object(event_json,'$.kv.place'),
    get_json_object(event_json,'$.kv.extend1'),
    get_json_object(event_json,'$.kv.category'),
    server_time
from ${db}.dwd_base_event_log
where dt='2019-07-05'
and event_name='display';

insert overwrite table ${db}.dwd_newsdetail_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.entry') entry,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.goodsid') goodsid,
	get_json_object(event_json,'$.kv.showtype') showtype,
	get_json_object(event_json,'$.kv.news_staytime') news_staytime,
	get_json_object(event_json,'$.kv.loading_time') loading_time,
	get_json_object(event_json,'$.kv.type1') type1,
	get_json_object(event_json,'$.kv.category') category,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='newsdetail';


insert overwrite table ${db}.dwd_loading_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.loading_time') loading_time,
	get_json_object(event_json,'$.kv.loading_way') loading_way,
	get_json_object(event_json,'$.kv.extend1') extend1,
	get_json_object(event_json,'$.kv.extend2') extend2,
	get_json_object(event_json,'$.kv.type') type,
	get_json_object(event_json,'$.kv.type1') type1,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='loading';


insert overwrite table ${db}.dwd_ad_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.entry') entry,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.content') content,
	get_json_object(event_json,'$.kv.detail') detail,
	get_json_object(event_json,'$.kv.source') ad_source,
	get_json_object(event_json,'$.kv.behavior') behavior,
	get_json_object(event_json,'$.kv.newstype') newstype,
	get_json_object(event_json,'$.kv.show_style') show_style,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='ad';


insert overwrite table ${db}.dwd_notification_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.action') action,
	get_json_object(event_json,'$.kv.noti_type') noti_type,
	get_json_object(event_json,'$.kv.ap_time') ap_time,
	get_json_object(event_json,'$.kv.content') content,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='notification';


insert overwrite table ${db}.dwd_active_foreground_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
get_json_object(event_json,'$.kv.push_id') push_id,
get_json_object(event_json,'$.kv.access') access,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='active_foreground';


insert overwrite table ${db}.dwd_active_background_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.active_source') active_source,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='active_background';


insert overwrite table ${db}.dwd_comment_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.comment_id') comment_id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.p_comment_id') p_comment_id,
	get_json_object(event_json,'$.kv.content') content,
	get_json_object(event_json,'$.kv.addtime') addtime,
	get_json_object(event_json,'$.kv.other_id') other_id,
	get_json_object(event_json,'$.kv.praise_count') praise_count,
	get_json_object(event_json,'$.kv.reply_count') reply_count,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='comment';


insert overwrite table ${db}.dwd_favorites_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.id') id,
	get_json_object(event_json,'$.kv.course_id') course_id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.add_time') add_time,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='favorites';


insert overwrite table ${db}.dwd_praise_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.id') id,
	get_json_object(event_json,'$.kv.userid') userid,
	get_json_object(event_json,'$.kv.target_id') target_id,
	get_json_object(event_json,'$.kv.type') type,
	get_json_object(event_json,'$.kv.add_time') add_time,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='praise';


insert overwrite table ${db}.dwd_error_log
PARTITION (dt='$do_date')
select 
	mid_id,
	user_id,
	version_code,
	version_name,
	lang,
	source,
	os,
	area,
	model,
	brand,
	sdk_version,
	gmail,
	height_width,
	 $db_time,
	network,
	lng,
	lat,
	get_json_object(event_json,'$.kv.errorBrief') errorBrief,
	get_json_object(event_json,'$.kv.errorDetail') errorDetail,
	server_time
from ${db}.dwd_base_event_log 
where dt='$do_date' and event_name='error';
"

$hive -e "$sql"








需求一：日周月活跃用户
================================================================
创建表（dws_uv_detail_day）
create external table dws_uv_detail_day(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    ` $db_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度'
)
partitioned by (dt string)
stored as parquet
location '/warehouse/gmall/dws/dws_uv_detail_day';

数据导入
***************************************************************
insert overwrite table dws_uv_detail_day partition(dt='2019-07-05')
select 
    mid_id,
    concat_ws("|",collect_set(user_id)) user_id,
    concat_ws("|",collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat
from dwd_start_log
where dt='2019-07-05'
group by mid_id;



载入脚本
************************************************************
#!bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n $1 ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dws_uv_detail_day partition(dt='$do_date')
select 
    mid_id,
    concat_ws('|',collect_set(user_id)) user_id,
    concat_ws('|',collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat
from ${db}.dwd_start_log
where dt='$do_date'
group by mid_id;
"

$hive -e "$sql"





创建表（dws_uv_detail_wk）
create external table dws_uv_detail_wk(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    ` $db_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度',
    monday_date string COMMENT '周一日期',
    sunday_date string COMMENT  '周日日期'
)
partitioned by (dt string)
stored as parquet
location '/warehouse/gmall/dws/dws_uv_detail_wk/';

装载数据(动态分组)
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table dws_uv_detail_wk partition(dt)
select 
    mid_id,
    concat_ws('|',collect_set(user_id)) user_id,
    concat_ws('|',collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_add(next_day('2019-07-05','MO'),-7) monday_date,
    date_add(next_day('2019-07-05','MO'),-1) sunday_date,
    concat(date_add(next_day('2019-07-05','MO'),-7),'_',date_add(next_day('2019-07-05','MO'),-1))
from dws_uv_detail_day
where dt between date_add(next_day('2019-07-05','MO'),-7) and date_add(next_day('2019-07-05','MO'),-1)
group by mid_id;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite ${db}.table dws_uv_detail_wk partition(dt)
select 
    mid_id,
    concat_ws('|',collect_set(user_id)) user_id,
    concat_ws('|',collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_add(next_day('$date','MO'),-7) monday_date,
    date_add(next_day('$date','MO'),-1) sunday_date,
    concat(date_add(next_day('$date','MO'),-7),'_',date_add(next_day('$date','MO'),-1))
from ${db}.dws_uv_detail_day
where dt between date_add(next_day('$date','MO'),-7) and date_add(next_day('$date','MO'),-1)
group by mid_id;
"

$hive -e "$sql"



创建表（dws_uv_detail_mn）
create external table dws_uv_detail_mn(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    ` $db_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度'
) COMMENT '活跃用户按月明细'
partitioned by (dt string)
stored as parquet
location '/warehouse/gmall/dws/dws_uv_detail_mn';


装载数据
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table dws_uv_detail_mn partition(dt)
select 
    mid_id ,
    concat_ws("|",collect_set(user_id)) user_id,
    concat_ws('|',collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_format('2019-07-05','yyyy-MM')
from dws_uv_detail_day
where date_format('2019-07-05','yyyy-MM')=date_format(dt,'yyyy-MM')
group by mid_id;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
insert overwrite table ${db}.dws_uv_detail_mn partition(dt)
select 
    mid_id ,
    concat_ws('|',collect_set(user_id)) user_id,
    concat_ws('|',collect_set(version_code)) version_code,
    concat_ws('|', collect_set(version_name)) version_name,
    concat_ws('|', collect_set(lang))lang,
    concat_ws('|', collect_set(source)) source,
    concat_ws('|', collect_set(os)) os,
    concat_ws('|', collect_set(area)) area, 
    concat_ws('|', collect_set(model)) model,
    concat_ws('|', collect_set(brand)) brand,
    concat_ws('|', collect_set(sdk_version)) sdk_version,
    concat_ws('|', collect_set(gmail)) gmail,
    concat_ws('|', collect_set(height_width)) height_width,
    concat_ws('|', collect_set( $db_time))  $db_time,
    concat_ws('|', collect_set(network)) network,
    concat_ws('|', collect_set(lng)) lng,
    concat_ws('|', collect_set(lat)) lat,
    date_format('$do_date','yyyy-MM')
from ${db}.dws_uv_detail_day
where date_format('$do_date','yyyy-MM')=date_format(dt,'yyyy-MM')
group by mid_id;
"

$hive -e "$sql"




需求一：活跃设备数
===========================================================
创建表
create external table ads_uv_count(
    dt string COMMENT '统计日期',
    day_count bigint COMMENT '当日用户数量',
    wk_count bigint COMMENT '当周用户数量',
    mn_count bigint COMMENT '当月用户数量',
    is_weekend string COMMENT 'Y,N是否是周末,用于得到本周最终结果',
    is_monthend string COMMENT 'Y,N是否是月末,用于得到本月最终结果'
) COMMENT '设备活跃数'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_uv_count';

装载数据
insert into table ads_uv_count 
select '2019-07-05',daycount,wkcount,mncount,if(next_day('2019-07-05','SU')='2019-07-05','Y','N'),case when last_day('2019-07-05')='2019-07-05' then 'Y' else 'N' end
from 
(select 
    count(*) daycount
from dws_uv_detail_day
where dt='2019-07-05'
) a1,
(select 
    count(*) wkcount
from dws_uv_detail_wk
where dt=concat(date_add(next_day('2019-07-05','MO'),-7),'_',date_add(next_day('2019-07-05','MO'),-1))
) a2,
(select 
    count(*) mncount
from dws_uv_detail_mn
where dt=date_format('2019-07-05','yyyy-MM')
) a3;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_uv_count 
select '$do_date',daycount,wkcount,mncount,if(next_day('$do_date','SU')='$do_date','Y','N'),case when last_day('$do_date')='$do_date' then 'Y' else 'N' end
from 
(select 
    count(*) daycount
from ${db}.dws_uv_detail_day
where dt='$do_date'
) a1,
(select 
    count(*) wkcount
from ${db}.dws_uv_detail_wk
where dt=concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1))
) a2,
(select 
    count(*) mncount
from ${db}.dws_uv_detail_mn
where dt=date_format('$do_date','yyyy-MM')
) a3;
"

$hive -e "$sql"












需求二：用户新增主体
==================================================================================
创建表（dws_new_mid_day）每日新增的mid
create external table dws_new_mid_day(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    ` $db_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度',
    `create_date`  string  comment '创建时间'
) COMMENT '每日新增设备信息'
stored as parquet
location '/warehouse/gmall/dws/dws_new_mid_day';

装载数据
insert into table dws_new_mid_day
select 
    ud.mid_id,
    ud.user_id , 
    ud.version_code , 
    ud.version_name , 
    ud.lang , 
    ud.source, 
    ud.os, 
    ud.area, 
    ud.model, 
    ud.brand, 
    ud.sdk_version, 
    ud.gmail, 
    ud.height_width,
    ud. $db_time,
    ud.network,
    ud.lng,
    ud.lat,
    '2019-07-05'
from dws_uv_detail_day ud left join dws_new_mid_day nm
on ud.mid_id=nm.mid_id
where nm.mid_id is null and ud.dt='2019-07-05';

装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.dws_new_mid_day
select 
    ud.mid_id,
    ud.user_id , 
    ud.version_code , 
    ud.version_name , 
    ud.lang , 
    ud.source, 
    ud.os, 
    ud.area, 
    ud.model, 
    ud.brand, 
    ud.sdk_version, 
    ud.gmail, 
    ud.height_width,
    ud. $db_time,
    ud.network,
    ud.lng,
    ud.lat,
    '$do_date'
from ${db}.dws_uv_detail_day ud left join ${db}.dws_new_mid_day nm
on ud.mid_id=nm.mid_id
where nm.mid_id is null and ud.dt='$do_date';
"

$hive -e "$sql"


===========================================================
建表语句（ads_new_mid_count）每日新增的设备数
create external table ads_new_mid_count(
    create_date string comment "创建时间" ,
    new_mid_count   BIGINT comment '新增设备数量'
) COMMENT '每日新增设备信息数量'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_new_mid_count';

装载数据
insert into table ads_new_mid_count
select 
    create_date,
    count(*)
from dws_new_mid_day
where create_date='2019-07-05'
group by create_date;

装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_new_mid_count
select 
    create_date,
    count(*)
from ${db}.dws_new_mid_day
where create_date='$do_date'
group by create_date;
"

$hive -e "$sql"








需求三：用户留存主题
===================================================================
建表语句（）
create external table dws_user_retention_day
(
    `mid_id` string COMMENT '设备唯一标识',
    `user_id` string COMMENT '用户标识', 
    `version_code` string COMMENT '程序版本号', 
    `version_name` string COMMENT '程序版本名', 
    `lang` string COMMENT '系统语言', 
    `source` string COMMENT '渠道号', 
    `os` string COMMENT '安卓系统版本', 
    `area` string COMMENT '区域', 
    `model` string COMMENT '手机型号', 
    `brand` string COMMENT '手机品牌', 
    `sdk_version` string COMMENT 'sdkVersion', 
    `gmail` string COMMENT 'gmail', 
    `height_width` string COMMENT '屏幕宽高',
    ` $db_time` string COMMENT '客户端日志产生时的时间',
    `network` string COMMENT '网络模式',
    `lng` string COMMENT '经度',
    `lat` string COMMENT '纬度',
   `create_date`    string  comment '设备新增时间',
   `retention_day`  int comment '截止当前日期留存天数'
)  COMMENT '每日用户留存情况'
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dws/dws_user_retention_day/'
;



装载数据
***********************************************
#7月6日计算任务
#即
#计算7月3日的3日留存
#计算7月4日的2日留存
#计算7月5日的1日留存
insert overwrite table dws_user_retention_day partition(dt='2019-07-08')
select
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('2019-07-08',-3) create_date,
    3 retention_day
from dws_uv_detail_day ud join dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='2019-07-08' and nm.create_date=date_add('2019-07-08',-3)

union all

select 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('2019-07-08',-2) create_date,
    2 retention_day
from dws_uv_detail_day ud join dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='2019-07-08' and nm.create_date=date_add('2019-07-08',-2)

union all


select 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('2019-07-08',-1) create_date,
    1 retention_day
from dws_uv_detail_day ud join dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='2019-07-08' and nm.create_date=date_add('2019-07-08',-1);

装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dws_user_retention_day partition(dt='$do_date')
select
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('$do_date',-3) create_date,
    3 retention_day
from ${db}.dws_uv_detail_day ud join ${db}.dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='$do_date' and nm.create_date=date_add('$do_date',-3)

union all

select 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('$do_date',-2) create_date,
    2 retention_day
from ${db}.dws_uv_detail_day ud join ${db}.dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='$do_date' and nm.create_date=date_add('$do_date',-2)

union all


select 
    nm.mid_id,
    nm.user_id , 
    nm.version_code , 
    nm.version_name , 
    nm.lang , 
    nm.source, 
    nm.os, 
    nm.area, 
    nm.model, 
    nm.brand, 
    nm.sdk_version, 
    nm.gmail, 
    nm.height_width,
    nm. $db_time,
    nm.network,
    nm.lng,
    nm.lat,
    date_add('$do_date',-1) create_date,
    1 retention_day
from ${db}.dws_uv_detail_day ud join ${db}.dws_new_mid_day nm on ud.mid_id=nm.mid_id
where ud.dt='$do_date' and nm.create_date=date_add('$do_date',-1);
"

$hive -e "$sql"




建表语句（ads_user_retention_day_count）留存用户数
***********************************************************
create external table ads_user_retention_day_count 
(
    create_date string  comment '设备新增日期',
    retention_day   int comment '截止当前日期留存天数',
    retention_count bigint comment  '留存数量'
)  COMMENT '每日用户留存情况'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_user_retention_day_count/';


数据装载
insert into table ads_user_retention_day_count
select 
    create_date,
    retention_day,
    count(*)
from dws_user_retention_day
where dt='2019-07-07'
group by create_date,retention_day;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_user_retention_day_count
select 
    create_date,
    retention_day,
    count(*)
from ${db}.dws_user_retention_day
where dt='$do_date'
group by create_date,retention_day;
"

$hive -e "$sql"



建表语句
******************************************************************
create external table ads_user_retention_day_rate 
(
    stat_date          string comment '统计日期',
    create_date    string  comment '设备新增日期',
    retention_day  int comment '截止当前日期留存天数',
    retention_count    bigint comment  '留存数量',
    new_mid_count  bigint comment '当日设备新增数量',
    retention_ratio    decimal(10,2) comment '留存率'
)  COMMENT '每日用户留存情况'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_user_retention_day_rate/';


装载数据
insert overwrite table ads_user_retention_day_rate      //会把前面的所有数据都进行计算，所以需要用overwrite
select 
    '2019-07-07',
    nm.create_date,
    retention_day,
    retention_count,
    new_mid_count,
    cast(retention_count/new_mid_count*100 as decimal(10,2))
from ads_new_mid_count nm join ads_user_retention_day_count ur
on nm.create_date=ur.create_date;
<!--where nm.create_date between date_add('2019-07-07',-3) and '2019-07-07';-->


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.ads_user_retention_day_rate
select 
    '$do_date',
    nm.create_date,
    retention_day,
    retention_count,
    new_mid_count,
    cast(retention_count/new_mid_count*100 as demical(10,2))
from ${db}.ads_new_mid_count nm join ${db}.ads_user_retention_day_count ur
on nm.create_date=ur.create_date;
"

$hive -e "$sql"





需求四：沉默用户数(沉默用户：指的是只在安装当天启动过，且启动时间是在一周前)
===================================================================================
建表语句
create external table ads_silent_count( 
    `dt` string COMMENT '统计日期',
    `silent_count` bigint COMMENT '沉默设备数'
) 
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_silent_count';


装载数据
insert into table ads_silent_count
select 
    dt,
    count(*)
from dws_uv_detail_day
group by mid_id
having count(*)=1 and max(dt)<date_add('2019-07-07',-7)

装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_silent_count
select 
    dt,
    count(*)
from ${db}.dws_uv_detail_day
group by mid_id
having count(*)=1 and max(dt)<date_add('$do_date',-7)
"

$hive -e "$sql"


需求五：本周回流用户数(本周回流=本周活跃-本周新增-上周活跃)
=======================================================================
建表语句
create external table ads_back_count( 
    `dt` string COMMENT '统计日期',
    `wk_dt` string COMMENT '统计日期所在周',
    `wastage_count` bigint COMMENT '回流设备数'
) 
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_back_count';


装载数据
insert into table ads_back_count
select 
    '$do_date',
    dt,
    count(*)
from (
    (
    select 
        mid_id,dt
    from dws_uv_detail_day
    where dt=concat(date_add(next_day('2019-07-07','MO'),-7),'_',date_add(next_day('2019-07-07','MO'),-1))
    ) t1
    left join
    (
    select 
        mid_id
    from dws_new_mid_day
    where create_date between date_add(next_day('2019-07-07','MO'),-7) and date_add(next_day('2019-07-07','MO'),-1)
    ) t2 on t1.mid_id=t2.mid_id
    left join 
    (
    select 
        mid_id
    from dws_uv_detail_day
    where dt=concat(date_add(next_day('2019-07-07','MO'),-14),'_',date_add(next_day('2019-07-07','MO'),-8))
    ) t3 on t1.mid_id=t2.mid_id
    where t2.mid_id is null and t3.mid_id is null
) t4
;

装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_back_count
select 
    '$do_date',
    dt,
    count(*)
from (
    (
    select 
        mid_id,dt
    from ${db}.dws_uv_detail_day
    where dt=concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1))
    ) t1
    left join
    (
    select 
        mid_id
    from ${db}.dws_new_mid_day
    where create_date between date_add(next_day('$do_date','MO'),-7) and date_add(next_day('$do_date','MO'),-1)
    ) t2 on t1.mid_id=t2.mid_id
    left join 
    (
    select 
        mid_id
    from ${db}.dws_uv_detail_day
    where dt=concat(date_add(next_day('$do_date','MO'),-14),'_',date_add(next_day('$do_date','MO'),-8))
    ) t3 on t1.mid_id=t2.mid_id
    where t2.mid_id is null and t3.mid_id is null
) t4
;
"

$hive -e "$sql"



需求六：流逝用户数(流失用户：最近7天未登录我们称之为流失用户)
==============================================================================================
建表语句
create external table ads_wastage_count( 
    `dt` string COMMENT '统计日期',
    `wastage_count` bigint COMMENT '流失设备数'
) 
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_wastage_count';


装载数据
insert into table ads_wastage_count
select 
    '2019-07-07',
    count(*)
from (
    select 
        mid_id
    from dws_uv_detail_day
    group by mid_id,dt
    having max(dt)<date_add('2019-07-07',-7)
    ) t1;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_wastage_count
select 
    '$do_date',
    count(*)
from (
    select 
        mid_id
    from ${db}.dws_uv_detail_day
    group by mid_id
    having max(dt)<date_add('$do_date',-7)
    ) t1;
"

$hive -e "$sql"



需求七：最近连续三周活跃用户(将最近三周的mid_id分组count，如果等于3，则说明连续三周活跃)
=========================================================
建表语句
create external table ads_continuity_wk_count( 
    `dt` string COMMENT '统计日期,一般用结束周周日日期,如果每天计算一次,可用当天日期',
    `wk_dt` string COMMENT '持续时间',
    `continuity_count` bigint
) 
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_continuity_wk_count';

装载数据
insert into table ads_continuity_wk_count
select 
    '2019-07-07',
    concat(date_add(next_day('$do_date','MO'),-21),'_',date_add(next_day('$do_date','MO'),-15)) and concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1)),
    count(*)
from (
    select 
        mid_id
    from dws_uv_detail_wk
    where dt between concat(date_add(next_day('$do_date','MO'),-21),'_',date_add(next_day('$do_date','MO'),-15)) and concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1))
    group by mid_id
    having count(*)=3
) t1;



装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_continuity_wk_count
select 
    '$do_date',
    concat(date_add(next_day('$do_date','MO'),-21),'_',date_add(next_day('$do_date','MO'),-15)) and concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1)),
    count(*)
from (
    select 
        mid_id
    from ${db}.dws_uv_detail_wk
    where dt between concat(date_add(next_day('$do_date','MO'),-21),'_',date_add(next_day('$do_date','MO'),-15)) and concat(date_add(next_day('$do_date','MO'),-7),'_',date_add(next_day('$do_date','MO'),-1))
    group by mid_id
    having count(*)=3
) t1;
"

$hive -e "$sql"



需求八：最近七天，连续三天登陆的用户数
========================================================================
建表语句
create external table ads_continuity_uv_count( 
    `dt` string COMMENT '统计日期',
    `wk_dt` string COMMENT '最近7天日期',
    `continuity_count` bigint
) COMMENT '连续活跃设备数'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_continuity_uv_count';

装载数据
insert into table ads_continuity_uv_count
select 
    '2019-07-07',
    concat(date_add('2019-07-07',-7),'_','2019-07-07',-7),
    count(*)
from (
    select 
        mid_id
    from (
        select 
            mid_id
        from (
            select 
                mid_id,
                date_add(dt,-rk) diff
            from 
                (
                select 
                    mid_id,
                    dt,
                    rank() over(partition by mid_id order by dt) rk
                from dws_uv_detail_day
                where dt>date_add('2019-07-07',-7)
                ) t1
            ) t2
        group by mid_id,diff
        having count(*)>=3
        ) t3
    group by mid_id
    ) t4;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_continuity_uv_count
select 
    '$date',
    concat(date_add('$date',-7),'_','$date',-7),
    count(*)
from (
    select 
        mid_id
    from (
        select 
            mid_id
        from (
            select 
                mid_id,
                date_add(dt,-rk) diff
            from 
                (
                select 
                    mid_id,
                    dt,
                    rank() over(partition by mid_id order by dt) rk
                from ${db}.dws_uv_detail_day
                where dt>date_add('$date',-7)
                ) t1
            ) t2
        group by mid_id,diff
        having count(*)>=3
        ) t3
    group by mid_id
    ) t4;
"

$hive -e "$sql"







从hdfs到ods层
=====================================================================
八张表建表语句
create external table ods_order_info (
    `id` string COMMENT '订单编号',
    `total_amount` decimal(10,2) COMMENT '订单金额',
    `order_status` string COMMENT '订单状态',
    `user_id` string COMMENT '用户id',
    `payment_way` string COMMENT '支付方式',
    `out_trade_no` string COMMENT '支付流水号',
    `create_time` string COMMENT '创建时间',
    `operate_time` string COMMENT '操作时间'
) COMMENT '订单表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_order_info/'
;

create external table ods_order_detail( 
    `id` string COMMENT '订单编号',
    `order_id` string  COMMENT '订单号', 
    `user_id` string COMMENT '用户id',
    `sku_id` string COMMENT '商品id',
    `sku_name` string COMMENT '商品名称',
    `order_price` string COMMENT '商品价格',
    `sku_num` string COMMENT '商品数量',
    `create_time` string COMMENT '创建时间'
) COMMENT '订单明细表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	' 
location '/warehouse/gmall/ods/ods_order_detail/'
;

create external table ods_sku_info( 
    `id` string COMMENT 'skuId',
    `spu_id` string   COMMENT 'spuid', 
    `price` decimal(10,2) COMMENT '价格',
    `sku_name` string COMMENT '商品名称',
    `sku_desc` string COMMENT '商品描述',
    `weight` string COMMENT '重量',
    `tm_id` string COMMENT '品牌id',
    `category3_id` string COMMENT '品类id',
    `create_time` string COMMENT '创建时间'
) COMMENT '商品表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_sku_info/'
;

create external table ods_user_info( 
    `id` string COMMENT '用户id',
    `name`  string COMMENT '姓名',
    `birthday` string COMMENT '生日',
    `gender` string COMMENT '性别',
    `email` string COMMENT '邮箱',
    `user_level` string COMMENT '用户等级',
    `create_time` string COMMENT '创建时间'
) COMMENT '用户信息'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_user_info/'
;

create external table ods_base_category1( 
    `id` string COMMENT 'id',
    `name`  string COMMENT '名称'
) COMMENT '商品一级分类'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_base_category1/'
;

create external table ods_base_category2( 
    `id` string COMMENT ' id',
    `name` string COMMENT '名称',
    category1_id string COMMENT '一级品类id'
) COMMENT '商品二级分类'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_base_category2/'
;

create external table ods_base_category3(
    `id` string COMMENT ' id',
    `name`  string COMMENT '名称',
    category2_id string COMMENT '二级品类id'
) COMMENT '商品三级分类'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_base_category3/'
;

create external table ods_payment_info(
    `id`   bigint COMMENT '编号',
    `out_trade_no`    string COMMENT '对外业务编号',
    `order_id`        string COMMENT '订单编号',
    `user_id`         string COMMENT '用户编号',
    `alipay_trade_no` string COMMENT '支付宝交易流水编号',
    `total_amount`    decimal(16,2) COMMENT '支付金额',
    `subject`         string COMMENT '交易内容',
    `payment_type`    string COMMENT '支付类型',
    `payment_time`    string COMMENT '支付时间'
   )  COMMENT '支付流水表'
PARTITIONED BY (`dt` string)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ods/ods_payment_info/'
;



装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
load data inpath '/origin_data/$db/db/order_info/$do_date' overwrite into table ${db}.ods_order_info partition(dt='$do_date');

load data inpath '/origin_data/$db/db/order_detail/$do_date' OVERWRITE into table ${db}.ods_order_detail partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/sku_info/$do_date' OVERWRITE into table ${db}.ods_sku_info partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/user_info/$do_date' OVERWRITE into table ${db}.ods_user_info partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/payment_info/$do_date' OVERWRITE into table ${db}.ods_payment_info partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/base_category1/$do_date' OVERWRITE into table ${db}.ods_base_category1 partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/base_category2/$do_date' OVERWRITE into table ${db}.ods_base_category2 partition(dt='$do_date');
                               
load data inpath '/origin_data/$db/db/base_category3/$do_date' OVERWRITE into table ${db}.ods_base_category3 partition(dt='$do_date');
"

$hive -e "$sql"



ods层到dwd层
========================================================================================
五张表的建表语句（将三张品类表合并到商品表中）
create external table dwd_order_info (
    `id` string COMMENT '',
    `total_amount` decimal(10,2) COMMENT '',
    `order_status` string COMMENT ' 1 2 3 4 5',
    `user_id` string COMMENT 'id',
    `payment_way` string COMMENT '',
    `out_trade_no` string COMMENT '',
    `create_time` string COMMENT '',
    `operate_time` string COMMENT ''
) 
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_order_info/'
tblproperties ("parquet.compression"="snappy")
;

create external table dwd_order_detail( 
    `id` string COMMENT '',
    `order_id` decimal(10,2) COMMENT '', 
    `user_id` string COMMENT 'id',
    `sku_id` string COMMENT 'id',
    `sku_name` string COMMENT '',
    `order_price` string COMMENT '',
    `sku_num` string COMMENT '',
    `create_time` string COMMENT ''
)
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_order_detail/'
tblproperties ("parquet.compression"="snappy")
;

create external table dwd_user_info( 
    `id` string COMMENT 'id',
    `name` string COMMENT '', 
    `birthday` string COMMENT '',
    `gender` string COMMENT '',
    `email` string COMMENT '',
    `user_level` string COMMENT '',
    `create_time` string COMMENT ''
) 
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_user_info/'
tblproperties ("parquet.compression"="snappy")
;

create external table dwd_payment_info(
    `id`   bigint COMMENT '',
    `out_trade_no`    string COMMENT '',
    `order_id`        string COMMENT '',
    `user_id`         string COMMENT '',
    `alipay_trade_no` string COMMENT '',
    `total_amount`    decimal(16,2) COMMENT '',
    `subject`         string COMMENT '',
    `payment_type`    string COMMENT '',
    `payment_time`    string COMMENT ''
   )  
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_payment_info/'
tblproperties ("parquet.compression"="snappy")
;

create external table dwd_sku_info(
    `id` string COMMENT 'skuId',
    `spu_id` string COMMENT 'spuid',
    `price` decimal(10,2) COMMENT '',
    `sku_name` string COMMENT '',
    `sku_desc` string COMMENT '',
    `weight` string COMMENT '',
    `tm_id` string COMMENT 'id',
    `category3_id` string COMMENT '1id',
    `category2_id` string COMMENT '2id',
    `category1_id` string COMMENT '3id',
    `category3_name` string COMMENT '3',
    `category2_name` string COMMENT '2',
    `category1_name` string COMMENT '1',
    `create_time` string COMMENT ''
) 
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dwd/dwd_sku_info/'
tblproperties ("parquet.compression"="snappy")
;


装载脚本
#!/bin/bash

# 定义变量方便修改
db=gmall
hive=/opt/module/hive-1.2.1/bin/hive

# 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天
if [ -n "$1" ] ;then
    do_date=$1
else 
    do_date=`date -d "-1 day" +%F`  
fi

sql="

set hive.exec.dynamic.partition.mode=nonstrict;

insert overwrite table "$db".dwd_order_info partition(dt)
select * from "$db".ods_order_info 
where dt='$do_date' and id is not null;
 
insert overwrite table "$db".dwd_order_detail partition(dt)
select * from "$db".ods_order_detail 
where dt='$do_date'   and id is not null;

insert overwrite table "$db".dwd_user_info partition(dt)
select * from "$db".ods_user_info
where dt='$do_date' and id is not null;
 
insert overwrite table "$db".dwd_payment_info partition(dt)
select * from "$db".ods_payment_info
where dt='$do_date' and id is not null;

insert overwrite table "$db".dwd_sku_info partition(dt)
select  
    sku.id,
    sku.spu_id,
    sku.price,
    sku.sku_name,
    sku.sku_desc,
    sku.weight,
    sku.tm_id,
    sku.category3_id,
    c2.id category2_id,
    c1.id category1_id,
    c3.name category3_name,
    c2.name category2_name,
    c1.name category1_name,
    sku.create_time,
    sku.dt
from
    "$db".ods_sku_info sku
    join "$db".ods_base_category3 c3 on sku.category3_id=c3.id 
    join "$db".ods_base_category2 c2 on c3.category2_id=c2.id 
    join "$db".ods_base_category1 c1 on c2.category1_id=c1.id 
    where sku.dt='$do_date' and c3.dt='$do_date' and c2.dt='$do_date' and c1.dt='$do_date'
    and sku.id is not null;
"

$hive -e "$sql"


dws层用户行为宽表
=========================================================================
建表语句
create external table dws_user_action 
(   
    user_id          string      comment '用户 id',
    order_count     bigint      comment '下单次数 ',
    order_amount    decimal(16,2)  comment '下单金额 ',
    payment_count   bigint      comment '支付次数',
    payment_amount  decimal(16,2) comment '支付金额 ',
    comment_count   bigint      comment '评论次数'
) COMMENT '每日用户行为宽表'
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dws/dws_user_action/'
tblproperties ("parquet.compression"="snappy");


装载数据
insert overwrite table dws_user_action partition(dt='2019-07-05')
select 
    user_id,
    sum(order_count) order_count,
    sum(order_amount) order_amount,
    sum(payment_count) payment_count,
    sum(payment_amount) payment_amount,
    sum(comment_count) comment_count
from 
    (
    select
        user_id,
        count(*) order_count,
        sum(total_amount) order_amount,
        0 payment_count,
        0 payment_amount,
        0 comment_count
    from dwd_order_info oi 
    where dt='2019-07-05'
    group by user_id
    
    union all
    
    select
        user_id,
        0 order_count,
        0 order_amount,
        count(*) payment_count,
        sum(total_amount) payment_amount,
        0 comment_count
    from dwd_payment_info
    where date_format(payment_time,'yyyy-MM-dd')='2019-07-05'
    group by user_id
    
    union all
    
    select 
        user_id,
        0 order_count,
        0 order_amount,
        0 payment_count,
        0 payment_amount,
        count(*) comment_count
    from dwd_comment_log
    where dt='2019-07-05'
    group by user_id
    ) tmp
group by user_id;



装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert overwrite table ${db}.dws_user_action partition(dt='$do_date')
select 
    user_id,
    sum(order_count) order_count,
    sum(order_amount) order_amount,
    sum(payment_count) payment_count,
    sum(payment_amount) payment_amount,
    sum(comment_count) comment_count
from 
    (
    select
        user_id,
        count(*) order_count,
        sum(total_amount) order_amount,
        0 payment_count,
        0 payment_amount,
        0 comment_count
    from ${db}.dwd_order_info
    where dt='$do_date'
    group by user_id
    
    union all
    
    select
        user_id,
        0 order_count,
        0 order_amount,
        count(*) payment_count,
        sum(total_amount) payment_amount,
        0 comment_count
    from ${db}.dwd_payment_info
    where date_format(payment_time,'yyyy-MM-dd')='$do_date'
    group by user_id
    
    union all
    
    select 
        user_id,
        0 order_count,
        0 order_amount,
        0 payment_count,
        0 payment_amount,
        count(*) comment_count
    from ${db}.dwd_comment_log
    where dt='$do_date'
    group by user_id
    ) tmp
group by user_id;
"

$hive -e "$sql"




需求一：GMV（GROSS MERCHANDISE VOLUME）成交总额
==========================================================================================
建表语句
create external table ads_gmv_sum_day(
    dt string COMMENT '统计日期',
    gmv_count  bigint COMMENT '当日gmv订单个数',
    gmv_amount  decimal(16,2) COMMENT '当日gmv订单总金额',
    gmv_payment  decimal(16,2) COMMENT '当日支付金额'
) COMMENT 'GMV'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_gmv_sum_day/'
;


装载数据
insert into table ads_gmv_sum_day 
select 
    '2019-07-05',
    sum(gmv_count) gmv_count,
    sum(gmv_amount) gmv_amount,
    sum(gmv_payment) gmv_payment
from (
    select 
        '2019-07-05',
        cast(count(*) as decimal(16,2)) gmv_count,
        cast(sum(total_amount) as decimal(16,2)) gmv_amount,
        cast(0 as decimal(16,2)) gmv_payment
    from dwd_order_info
    where dt='2019-07-05' and date_format(create_time,'yyyy-MM-dd')='2019-07-05'
    
    union all
    
    select 
        '2019-07-05',
        cast(0 as decimal(16,2)) gmv_count,
        cast(0 as decimal(16,2)) gmv_amount,
        cast(sum(total_amount) as decimal(16,2)) gmv_payment
    from dwd_payment_info
    where dt='2019-07-05'
    ) tmp;
    
此处直接从dws层的dws_user_action直接拿数据即可
insert into table ads_gmv_sum_day 
select 
    '2019-07-05',
    sum(order_count) gmv_count,
    sum(order_amount) gmv_amount,
    sum(payment_amount) gmv_payment
from dws_user_action 
where dt='2019-07-05';



装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_gmv_sum_day 
select 
    '$do_date',
    sum(order_count) gmv_count,
    sum(order_amount) gmv_amount,
    sum(payment_amount) gmv_payment
from ${db}.dws_user_action 
where dt='$do_date';
"

$hive -e "$sql"




需求二：转化率
===================================================================
需求2.1：新鲜度
建表语句
create external table ads_user_convert_day( 
    `dt` string COMMENT '统计日期',
    `uv_m_count`  bigint COMMENT '当日活跃设备',
    `new_m_count`  bigint COMMENT '当日新增设备',
    `new_m_ratio`   decimal(10,2) COMMENT '当日新增占日活的比率'
) COMMENT '转化率'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_user_convert_day/'
;


装载数据
insert into table ads_user_convert_day
select 
    '2019-07-05' dt,
    sum(uv_m_count) uv_m_count,
    sum(new_m_count) new_m_count,
    sum(uv_m_count)/sum(new_m_count) new_m_ratio
from (
    select 
        '2019-07-05' dt,
        day_count uv_m_count,
        0 new_m_count
    from ads_uv_count
    where dt='2019-07-05'
    
    union all
    
    select 
        '2019-07-05' dt,
        0 uv_m_count,
        new_mid_count new_m_count
    from ads_new_mid_count
    where create_date='2019-07-05'
    ) tmp
group by dt;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_user_convert_day
select 
    '$do_date' dt,
    sum(uv_m_count) uv_m_count,
    sum(new_m_count) new_m_count,
    sum(uv_m_count)/sum(new_m_count) new_m_ratio
from (
    select 
        '$do_date' dt,
        day_count uv_m_count,
        0 new_m_count
    from ${db}.ads_uv_count
    where dt='$do_date'
    
    union all
    
    select 
        '$do_date' dt,
        0 uv_m_count,
        new_mid_count new_m_count
    from ${db}.ads_new_mid_count
    where create_date='$do_date'
    ) tmp
group by dt;
"

$hive -e "$sql"


需求2.2：漏斗分析
建表语句
create external  table ads_user_action_convert_day(
    `dt` string COMMENT '统计日期',
    `total_visitor_m_count`  bigint COMMENT '总访问人数',
    `order_u_count` bigint     COMMENT '下单人数',
    `visitor2order_convert_ratio`  decimal(10,2) COMMENT '访问到下单转化率',
    `payment_u_count` bigint     COMMENT '支付人数',
    `order2payment_convert_ratio` decimal(10,2) COMMENT '下单到支付的转化率'
 ) COMMENT '用户行为漏斗分析'
row format delimited  fields terminated by '	'
location '/warehouse/gmall/ads/ads_user_action_convert_day/'
;

装载数据：
insert into table ads_user_action_convert_day
select
    '2019-07-05' dt,
    uv.day_count,
    ua.order_count,
    cast(ua.order_count/uv.day_count as decimal(10,2)),
    ua.payment_count,
    cast(ua.payment_count/ua.order_count as decimal(10,2))
from (
    select 
        '2019-07-05' dt,
        sum(if(order_count>=1,1,0)) order_count,
        sum(if(payment_count>=1,1,0)) payment_count
        
    from dws_user_action 
    where dt='2019-07-05'
    ) ua 
join ads_uv_count uv on ua.dt=uv.dt;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi

sql="
insert into table ${db}.ads_user_action_convert_day
select
    '$do_date' dt,
    uv.day_count,
    ua.order_count,
    cast(ua.order_count/uv.day_count as decimal(10,2)),
    ua.payment_count,
    cast(ua.payment_count/ua.order_count as decimal(10,2))
from (
    select 
        '$do_date' dt,
        sum(if(order_count>=1,1,0)) order_count,
        sum(if(payment_count>=1,1,0)) payment_count
        
    from ${db}.dws_user_action 
    where dt='$do_date'
    ) ua 
join ${db}.ads_uv_count uv on ua.dt=uv.dt;
"

$hive -e "$sql"



需求三：品牌复购率
=======================================================================================================
DWS层：用户购买商品明细宽表
====================================================================
建表语句
create external table dws_sale_detail_daycount
(   
    user_id   string  comment '用户 id',
    sku_id    string comment '商品 Id',
    user_gender  string comment '用户性别',
    user_age string  comment '用户年龄',
    user_level string comment '用户等级',
    order_price decimal(10,2) comment '商品价格',
    sku_name string   comment '商品名称',
    sku_tm_id string   comment '品牌id',
    sku_category3_id string comment '商品三级品类id',
    sku_category2_id string comment '商品二级品类id',
    sku_category1_id string comment '商品一级品类id',
    sku_category3_name string comment '商品三级品类名称',
    sku_category2_name string comment '商品二级品类名称',
    sku_category1_name string comment '商品一级品类名称',
    spu_id  string comment '商品 spu',
    sku_num  int comment '购买个数',
    order_count string comment '当日下单单数',
    order_amount string comment '当日下单金额'
) COMMENT '用户购买商品明细表'
PARTITIONED BY (`dt` string)
stored as parquet
location '/warehouse/gmall/dws/dws_user_sale_detail_daycount/'
tblproperties ("parquet.compression"="snappy");

数据装载
insert overwrite table dws_sale_detail_daycount partition(dt='2019-07-05')
select 
    od.user_id,
    od.sku_id,
    gender,
    cast(months_between('2019-07-05',date_format(birthday,'yyyy-MM-dd'))/12 as int),
    user_level,
    cast(price as decimal(10,2)) order_price,
    sku_name,
    tm_id,
    category3_id,
    category2_id,
    category1_id,
    category3_name,
    category2_name,
    category1_name,
    si.spu_id,
    sku_num,
    order_count,
    order_amount
from (
    select
        user_id,
        sku_id,
        sum(sku_num) sku_num,
        count(*) order_count,
        sum(order_price*sku_num) order_amount
    from dwd_order_detail
    where dt='2019-07-05'
    group by user_id,sku_id
    ) od 
join dwd_user_info ui on od.user_id=ui.id
join dwd_sku_info si on od.sku_id=si.id
where ui.dt='2019-07-05' and si.dt='2019-07-05';


载入脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi;

sql="
insert overwrite table ${db}.dws_sale_detail_daycount partition(dt='$do_date')
select 
    od.user_id,
    od.sku_id,
    gender,
    cast(months_between('$do_date',date_format(birthday,'yyyy-MM-dd'))/12 as int),
    user_level,
    cast(price as decimal(10,2)) order_price,
    sku_name,
    tm_id,
    category3_id,
    category2_id,
    category1_id,
    category3_name,
    category2_name,
    category1_name,
    si.spu_id,
    sku_num,
    order_count,
    order_amount
from (
    select
        user_id,
        sku_id,
        sum(sku_num) sku_num,
        count(*) order_count,
        sum(order_price*sku_num) order_amount
    from ${db}.dwd_order_detail
    where dt='$do_date'
    group by user_id,sku_id
    ) od 
join ${db}.dwd_user_info ui on od.user_id=ui.id
join ${db}.dwd_sku_info si on od.sku_id=si.id
where ui.dt='$do_date' and si.dt='$do_date';
"

$hive -e "$sql"


ads层：品牌复购率
=======================================================
建表语句
create external table ads_sale_tm_category1_stat_mn
(   
    tm_id string comment '品牌id',
    category1_id string comment '1级品类id ',
    category1_name string comment '1级品类名称 ',
    buycount   bigint comment  '购买人数',
    buy_twice_last bigint  comment '两次以上购买人数',
    buy_twice_last_ratio decimal(10,2)  comment  '单次复购率',
    buy_3times_last   bigint comment   '三次以上购买人数',
    buy_3times_last_ratio decimal(10,2)  comment  '多次复购率',
    stat_mn string comment '统计月份',
    stat_date string comment '统计日期' 
)   COMMENT '复购率统计'
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_sale_tm_category1_stat_mn/'
;


数据装载
insert into table ads_sale_tm_category1_stat_mn
select 
    tm_id,
    category1_id,
    category1_name,
    sum(if(order_count>=1,1,0)) buycount,
    sum(if(order_count>=2,1,0)) buy_twice_last,
    cast(sum(if(order_count>=2,1,0))/sum(if(order_count>=1,1,0)) as decimal(10,2)) buy_twice_last_ratio,
    sum(if(order_count>=3,1,0)) buy_3times_last,
    cast(sum(if(order_count>=3,1,0))/sum(if(order_count>=1,1,0)) as decimal(10,2)) buy_3times_last_ratio,
    date_format('2019-07-05','yyyy-MM') stat_mn,
    '2019-07-05' stat_date
from (
    select 
        user_id,
        sku_tm_id tm_id,
        sku_category1_id category1_id,
        sku_category1_name category1_name,
        count(*) order_count
    from dws_sale_detail_daycount
    where date_format(dt,'yyyy-MM')=date_format('2019-07-05','yyyy-MM')
    group by user_id,sku_tm_id,sku_category1_id,sku_category1_name
    ) tmp
group by tm_id,category1_id,category1_name;


装载脚本
#!/bin/bash
do_date=`date -d '-1 day' +%F`
db='gmall'
hive='/opt/module/hive-1.2.1/bin/hive'

if [ -n "$1" ];then
    do_date=$1
fi;

sql="
insert into table ${db}.ads_sale_tm_category1_stat_mn
select 
    tm_id,
    category1_id,
    category1_name,
    sum(if(order_count>=1,1,0)) buycount,
    sum(if(order_count>=2,1,0)) buy_twice_last,
    cast(sum(if(order_count>=2,1,0))/sum(if(order_count>=1,1,0)) as decimal(10,2)) buy_twice_last_ratio,
    sum(if(order_count>=3,1,0)) buy_3times_last,
    cast(sum(if(order_count>=3,1,0))/sum(if(order_count>=1,1,0)) as decimal(10,2)) buy_3times_last_ratio,
    date_format('$do_date','yyyy-MM') stat_mn,
    '$do_date' stat_date
from (
    select 
        user_id,
        sku_tm_id tm_id,
        sku_category1_id category1_id,
        sku_category1_name category1_name,
        count(*) order_count
    from ${db}.dws_sale_detail_daycount
    where date_format(dt,'yyyy-MM')=date_format('$do_date','yyyy-MM')
    group by user_id,sku_tm_id,sku_category1_id,sku_category1_name
    ) tmp
group by tm_id,category1_id,category1_name;
"

$hive -e "$sql"


练习：求每个等级的用户对应的复购率前十的商品排行
================================================================================
建表语句
create external table ads_sale_tm_stat_rank(
    user_level string,
    sku_id string,
    buy_count,
    buy_twice_last,
    buy_twice_last_ratio,
    rk int
)
row format delimited fields terminated by '	'
location '/warehouse/gmall/ads/ads_sale_rank';


装载数据
insert into table ads_sale_tm_stat_rank
select
    *
from (
    select
        user_level,
        sku_id,
        buy_count,
        buy_twice_last,
        buy_twice_last_ratio,
        row_number() over(partition by user_level order by buy_twice_last_ratio desc) rn
    from (
        select 
            user_level,
            sku_id,
            sum(if(order_count>=1,1,0)) buy_count,
            sum(if(order_count>=2,1,0)) buy_twice_last,
            sum(if(order_count>=2,1,0))/sum(if(order_count>=1,1,0)) buy_twice_last_ratio
        from (
            select 
                user_level,
                user_id,
                sku_id,
                count(*) order_count
            from dws_sale_detail_daycount
            group by user_level,user_id,sku_id
            ) t1
        group by user_level,sku_id
        ) t2
    ) t3 where rn<=10;


sqoop导出脚本
==============================================================================================
#/bin/bash
db='gmall'

export_data(){
/opt/module/sqoop-1.4.6/bin/sqoop export \
--connect "jdbc:mysql://hadoop102:3306/$db?useUnicode=true&characterEncoding=utf-8" \
--username root \
--password abc123 \
--table $1 \
--num-mappers 1 \
--export-dir /warehouse/$db/ads/$1 \
--input-fields-terminated-by '	' \
--update-mode allowinsert \
--update-key 'tm_id,category1_id,stat_mn,stat_date' \
--input-null-string '\N' \
--input-null-non-string '\N'
}

case $1 in
"ads_uv_count")
    export_data ads_uv_count;;
"ads_user_action_convert_day")
    export_data ads_user_action_convert_day;;
"ads_gmv_sum_day")
    export_data ads_gmv_sum_day;;
"all")
    export_data ads_uv_count
    export_data ads_user_action_convert_day
    export_data ads_gmv_sum_day;;
esac


在mysql中创建表
======================================================================================
活跃用户统计
CREATE TABLE `ads_uv_count`  (
  `dt` varchar(255) DEFAULT NULL COMMENT '统计日期',
  `day_count` bigint(200) DEFAULT NULL COMMENT '当日用户数量',
  `wk_count` bigint(200) DEFAULT NULL COMMENT '当周用户数量',
  `mn_count` bigint(200) DEFAULT NULL COMMENT '当月用户数量',
  `is_weekend` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT 'Y,N是否是周末,用于得到本周最终结果',
  `is_monthend` varchar(200) CHARACTER SET utf8 COLLATE utf8_general_ci DEFAULT NULL COMMENT 'Y,N是否是月末,用于得到本月最终结果'
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日活跃用户数量' ROW_FORMAT = Dynamic;

留存率统计
CREATE TABLE `ads_user_retention_day_rate`  (
  `stat_date` varchar(255)  DEFAULT NULL COMMENT '统计日期',
  `create_date` varchar(255) DEFAULT NULL COMMENT '设备新增日期',
  `retention_day` bigint(200) DEFAULT NULL COMMENT '截止当前日期留存天数',
  `retention_count` bigint(200) DEFAULT NULL COMMENT '留存数量',
  `new_mid_count` bigint(200) DEFAULT NULL COMMENT '当日设备新增数量',
  `retention_ratio` decimal(10, 2) DEFAULT NULL COMMENT '留存率'
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日用户留存情况' ROW_FORMAT = Dynamic;

漏斗分析
CREATE TABLE `ads_user_action_convert_day`  (
  `dt` varchar(200) DEFAULT NULL COMMENT '统计日期',
  `total_visitor_m_count` bigint(20) DEFAULT NULL COMMENT '总访问人数',
  `order_u_count` bigint(20) DEFAULT NULL COMMENT '下单人数',
  `visitor2order_convert_ratio` decimal(10, 2) DEFAULT NULL COMMENT '购物车到下单转化率',
  `payment_u_count` bigint(20) DEFAULT NULL COMMENT '支付人数',
  `order2payment_convert_ratio` decimal(10, 2) DEFAULT NULL COMMENT '下单到支付的转化率'
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日用户行为转化率统计' ROW_FORMAT = Dynamic;

GMV统计
CREATE TABLE ads_gmv_sum_day(
  `dt` varchar(200) DEFAULT NULL COMMENT '统计日期',
  `gmv_count` bigint(20) DEFAULT NULL COMMENT '当日gmv订单个数',
  `gmv_amount` decimal(16, 2) DEFAULT NULL COMMENT '当日gmv订单总金额',
  `gmv_payment` decimal(16, 2) DEFAULT NULL COMMENT '当日支付金额'
) ENGINE = InnoDB CHARACTER SET = utf8 COLLATE = utf8_general_ci COMMENT = '每日活跃用户数量' ROW_FORMAT = Dynamic;




azkaban调度
==================================================================================
sqoop_import.job文件：
type=command
command=sh sqoop_import.sh all ${dt}
dependencies=ads_db_gmv

ods_db.job文件：
type=command
command=sh ods_db.sh ${dt}
dependencies=sqoop_import

dwd_db.job文件：
type=command
command=sh dwd_db.sh ${dt}
dependencies=ods_db

dws_db_wide.job文件：
type=command
command=sh dws_db_wide.sh ${dt}
dependencies=dwd_db

ads_db_gmv.job文件：
type=command
command=sh ads_db_gmv.sh ${dt}
dependencies=dws_db_wide

sqoop_export.job文件：
type=command
command=sh sqoop_export.sh all
dependencies=ads_db_gmv


拉链表制作：
======================================================================================
建表语句
create external table dwd_order_info_his(
?   `id` string COMMENT '订单编号',
?   `total_amount` decimal(10,2) COMMENT '订单金额',
?   `order_status` string COMMENT '订单状态',
??  `user_id` string COMMENT '用户id' ,
??? `payment_way` string COMMENT '支付方式',
??? `out_trade_no` string COMMENT '支付流水号',
??? `create_time` string COMMENT '创建时间',
??? `operate_time` string COMMENT '操作时间',
??? `start_date`  string COMMENT '有效开始日期',
??? `end_date`  string COMMENT '有效结束日期'
) COMMENT '订单拉链表'
stored as parquet
location '/warehouse/gmall/dwd/dwd_order_info_his/'
tblproperties ("parquet.compression"="snappy");


临时拉链表建表语句
create external table dwd_order_info_his_tmp (
    id string COMMENT '订单编号',
    total_amount decimal(10,2) COMMENT '订单金额',
    order_status string COMMENT '订单状态',
    user_id string COMMENT '用户id' ,
    payment_way string COMMENT '支付方式',
    out_trade_no string COMMENT '支付流水号',
    create_time string COMMENT '创建时间',
    operate_time string COMMENT '操作时间',
    start_date  string COMMENT '有效开始日期',
    end_date  string COMMENT '有效结束日期'
) COMMENT '订单拉链表'
stored as parquet
location '/warehouse/gmall/dwd/dwd_order_info_his/'
tblproperties ("parquet.compression"="snappy");


数据装载(拉链表即是将新增数据插入，将变更数据的有效结束日期该为前一天)
insert overwrite table dwd_order_info_his_tmp
select 
        id,
        total_amount,
        order_status,
        user_id,
        payment_way,
        out_trade_no,
        create_time,
        operate_time,
        start_date,
        end_date
from 
    (
    select 
        id,
        total_amount,
        order_status,
        user_id,
        payment_way,
        out_trade_no,
        create_time,
        operate_time,
        '2019-07-06' start_date,
        '9999-99-99' end_date
    from dwd_order_info
    where dt='2019-07-06'
    
    union all
    
    select 
        oih.id,
        oih.total_amount,
        oih.order_status,
        oih.user_id,
        oih.payment_way,
        oih.out_trade_no,
        oih.create_time,
        oih.operate_time,
        oih.start_date start_date,
        if(oi.id is null,end_date,date_add('2019-07-06',-1)) end_date
    from dwd_order_info_his_tmp oih left join (select * from dwd_order_info where dt='2019-07-06') oi 
    on oih.id=oi.id and oih.end_date='9999-99-99'
    ) tmp;
