---
title: 分层建模
categories:
- 大数据离线
---
**脚本细节：**
1. 日志从hdfs中load到ods中需要创建lzo索引，业务通过sqoop导入时也要创建lzo索引。
2. 从ods到dwd需要指定inputformat，不能将lzo索引当作小文件进行合并 set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat
3. 如果创建了多队列，hive脚本中还需要指定使用的容量队列set mapreduce.job.queuename=hive

###分层
- ODS（operation data store）：存放原始数据。保持数据原貌不变；创建分区表，防止后续的全表扫描；`时间分区`，采用`LZO压缩，需要建索引`，`指定输入输出格式`；创建外部表；
- DWD（data warehouse detail）：结构粒度与ODS层保持一致，对ODS层数据进行清洗（去除无效数据、脏数据，数据脱敏，`维度退化`，`数据转换`等）。ETL数据清洗，用hive sql、MR、Python、Kettle、SparkSQL；`时间分区`，采用`LZO压缩(parquet支持切片)，不需要建索引`，采用`parquet格式`存储；创建外部表；
- DWS（data warehouse service）：在DWD基础上，按天进行轻度汇总。`时间分区`，采用`parquet格式`存储；创建外部表；
- DWT（data warehouse topic）：在DWS基础上，按主题进行汇总。采用`parquet格式`存储；创建外部表，时间分区；
- ADS（Application Data Store）：为统计报表提供数据。`明确字段分割符`，与sqoop对应；创建外部表，时间分区。

在ODS层对原始数据进行存储，需要采用压缩（一般lzo为10倍压缩），创建分区表，防止后续的全表扫描。lzop需要建索引才能进行分片，如果时parquet格式+lzo压缩，则不需要切片，因为parquet支持切片。

在DWD层进行维度建模
步骤：
1.选择业务线
2.声明粒度（越细越好，一般以条为单位）
3.确定事实表和维度表
![image.png](分层建模.assets\843eab302168475b94c7685f8625e350.png)

   需要将满足三范式的维度表进行维度退化变为一张表，减少join次数，提高效率。

![image.png](分层建模.assetse356c3fb2b441c28f38a02c5dd39c67.png)

在DWS层统计每日的数据，如每日会员行为，统计了每个用户登录次数，加购次数，加购金额，下单次数，下单金额，支付次数，支付金额等行为。

在DWT层根据主题对每日数据进行计算统计，如统计每个用户的首次登录时间，末次登录时间，累计登录天数，最近30日登录天数，首次下单时间，末次下单时间，累计下单次数，累计下单金额，最近30日下单次数，最近30日下单金额，首次支付时间，末次支付时间，累计支付次数，累计支付金额，最近30日支付次数，最近30日支付金额等字段的用户主题宽表。

在ADS层对各大主题指标分别进行分析。



**数据流向图**
![image.png](分层建模.assets