---
title: 2023-05-05
categories:
- 大数据离线
---
# 五、部分框架升级

## 5.1 Hadoop 3.1.3

### 5.1.1 安装

安装方式同旧版本

### 5.1.2 配置文件

core-site.xml

```
<configuration>
    <!-- 指定NameNode地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://bigdata1:9820</value>
    </property>
    <!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
    </property>
    
    <!-- 配置HDFS网页登陆使用的静态用户为hxr -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>hxr</value>
    </property>
    <!-- 配置hxr允许通过代理访问主机节点 -->
    <property>
        <name>hadoop.proxyuser.hxr.hosts</name>
        <value>*</value>
    </property>
    <!-- 配置hxr允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.hxr.groups</name>
        <value>*</value>
    </property>

    <property>
        <name>io.compression.codecs</name>
        <value>
        org.apache.hadoop.io.compress.GzipCodec,
        org.apache.hadoop.io.compress.DefaultCodec,
        org.apache.hadoop.io.compress.BZip2Codec,
        org.apache.hadoop.io.compress.SnappyCodec,
        com.hadoop.compression.lzo.LzoCodec,
        com.hadoop.compression.lzo.LzopCodec
        </value>
    </property>
    <property>
        <name>io.compression.codec.lzo.class</name>
        <value>com.hadoop.compression.lzo.LzoCodec</value>
    </property>

</configuration>
```

> 集群支持lzo压缩，还需要将lzo包**hadoop-lzo-0.4.20.jar**放到路径**/opt/module/hadoop-2.7.2/share/hadoop/common**下，分发到其他节点后重启hadoop集群。

hdfs-site.xml

```
<configuration>
    <!-- nn web端访问地址 -->
    <property>
        <name>dfs.namenode.http-address</name>
        <value>bigdata1:9870</value>
    </property>
    <!-- 2nn web端访问地址 -->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>bigdata3:9868</value>
    </property>
    <!-- 挂载目录配置 -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/data,file:///dfs/data1,file:///home/dfs/data2</value>
    </property>

    <!-- 测试环境指定HDFS副本数量 -->
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
</configuration>
```

> 如果是新添加的硬盘，为了防止原有的数据丢失，需要将原来的数据也进行挂载，默认路径是`file://${hadoop.tmp.dir}/dfs/data`。
> 启动时，所有挂载的路径下，不能存在一个以上的current文件夹，否则会导致错误
>
> ```
> org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/dfs/data2 is in an inconsistent state: Root /home/dfs/data2: DatanodeUuid=a6602160-4a1b-445a-946c-e525affe6b42, does not match 13905291-f3df-41f3-a4ed-c7c2b5a5c63d from other StorageDirectory.
> ```

yarn-site.xml

```
<configuration>
    <!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>bigdata2</value>
    </property>
    <!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
    
    <!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
    </property>

    <!-- 虚拟核数和物理核数乘数 -->
    <property>
        <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
        <value>1</value>
    </property>

    <!-- 该节点上YARN可使用的物理内存总量 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>30720</value>
    </property>

    
    <!-- resourcemanager可以分配给容器的核数 -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
    </property>
    
    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
    
    <!-- 开启日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <!-- 设置日志聚集服务器地址 -->
    <property>  
        <name>yarn.log.server.url</name>  
        <value>http://bigdata3:19888/jobhistory/logs</value>
    </property>
    <!-- 设置日志保留时间为7天 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
</configuration>
```

mapred-site.xml

```
<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>bigdata1:10020</value>
    </property>

    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>bigdata1:19888</value>
    </property>
</configuration>
```

workers

```
bigdata1
bigdata2
bigdata3
```

在/etc/profile.d/env.sh中添加pid存储位置变量：

```
# hdfs pid location
export HADOOP_PID_DIR=${HADOOP_HOME}/pids/hdfs
# jobhistoryserver pid location
export HADOOP_MAPRED_PID_DIR=${HADOOP_HOME}/pids/mapred
# yarn pid location
export YARN_PID_DIR=${HADOOP_HOME}/pids/yarn
```

在log4j.properties中配置hadoop日志输出位置

```
hadoop.log.dir=/opt/module/hadoop-3.1.3/logs
```

### 5.1.3 端口

| 组件 | 端口 |
| namenode-web | 9870/9871 |
| namenode通讯端口 | 9820 |
| resourcemanager-web | 8088 |
| resourcemanager通讯端口 | 8031 |
| nodemanager-http端口 | 8042 |

### 5.1.4 高可用

#### 5.1.4.1 namenode高可用

[Apache Hadoop 3.3.5 – HDFS High Availability Using the Quorum Journal Manager](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithQJM.html)

> HA通过配置Active/Standby两个NameNode实现在集群中对NameNode的热备份来解决上述问题。
>
> 1. 在一个典型的HA集群中，使用两台单独的机器配置为NameNodes。在任何时间点，确保NameNodes中只有一个处于Active状态，其他的处在Standby状态。其中Active对外提供服务，负责集群中的所有客户端操作，Standby仅仅充当备机，仅同步Active NameNode的状态，以便能够在它失败时快速进行切换。
> 2. 为了能够实时同步Active和Standby两个NameNode的元数据信息（editlog），需提供一个SharedStorage（共享存储系统），可以是NFS或QJM，Active将数据写入共享存储系统，而Standby监听该系统，一旦发现有日志变更，则读取这些数据并加载到自己内存中，以保证自己内存状态与Active保持一致，则在当failover(失效转移)发生时standby便可快速切为active。
> 3. 为了实现快速切换，Standby节点获取集群中blocks的最新位置是非常必要的。为了实现这一目标，DataNodes上需要同时配置这两个Namenode的地址，并同时给他们发送文件块信息以及心跳检测。
> 4. 为了实现热备，增加FailoverController（故障转移控制器）和Zookeeper，FailoverController与Zookeeper通信，通过Zookeeper选举机制，FailoverController通过RPC让NameNode转换为Active或Standby。
>
> 【备注】standby代替SNN的功能（edits-----fsimage）
> 启动了hadoop2.0的HA机制之后，secondarynamenode，checkpointnode，buckcupnode这些都不需要了。

> Hadoop 高可用性有两种实现方式：QJM 和 NFS。QJM 通过多个 JournalNode 保证了系统的可靠性，消耗的资源很少，不需要额外的机器来启动 JournalNode 进程，只需从 Hadoop 集群中选择几个节点启动 JournalNode 即可。NFS 则是通过共享存储来实现数据共享，但是 NFS 存在单点故障问题，不如 QJM 可靠。这里使用QJM 方式实现HA。

hdfs-site.xml添加如下配置

```
    <!-- NameNode高可用 -->
    <property>
        <name>dfs.nameservices</name>
        <value>hdfscluster</value>
    </property>
    <property>
        <name>dfs.ha.namenodes.hdfscluster</name>
        <value>nn1,nn2,nn3</value>
    </property>
    
    <property>
        <name>dfs.namenode.rpc-address.hdfscluster.nn1</name>
        <value>cos-bigdata-test-flink-01:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.hdfscluster.nn2</name>
        <value>cos-bigdata-test-flink-02:8020</value>
    </property>
    <property>
        <name>dfs.namenode.rpc-address.hdfscluster.nn3</name>
        <value>cos-bigdata-test-flink-03:8020</value>
    </property>
    
    <property>
        <name>dfs.namenode.http-address.hdfscluster.nn1</name>
        <value>cos-bigdata-test-flink-01:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.hdfscluster.nn2</name>
        <value>cos-bigdata-test-flink-02:9870</value>
    </property>
    <property>
        <name>dfs.namenode.http-address.hdfscluster.nn3</name>
        <value>cos-bigdata-test-flink-03:9870</value>
    </property>
    
    <property>
        <name>dfs.namenode.shared.edits.dir</name>
        <value>qjournal://cos-bigdata-test-flink-01:8485;cos-bigdata-test-flink-02:8485;cos-bigdata-test-flink-03:8485/hdfscluster</value>
    </property>
    
    <property>
        <name>dfs.client.failover.proxy.provider.hdfscluster</name>
        <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>shell(/bin/true)</value>
    </property>
    
<!--
    <property>
        <name>dfs.ha.fencing.methods</name>
        <value>sshfence</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value>/root/.ssh/id_rsa</value>
    </property>
    -->
    
    <property>
        <name>dfs.journalnode.edits.dir</name>
        <value>/opt/module/hadoop/journal</value>
    </property>
    
    <property>
        <name>dfs.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property> 
```

在core-site.xml中配置zk

```
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>cos-bigdata-test-flink-01:2181,cos-bigdata-test-flink-02:2181,cos-bigdata-test-flink-03:2181</value>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfscluster</value>
    </property>
```

在hadoop-env.sh中配置用户

```
export HDFS_JOURNALNODE_USER=root
export HDFS_ZKFC_USER=root
```

启动每个节点上的journalnode服务 `hdfs --daemon start journalnode`
运行命令清空zookeeper中namenode的注册信息 `hdfs zkfc -formatZK`
将所有journalnode启动后，在其中一台namenode下执行命令 `hdfs namenode -initializeSharedEdits`
然后启动服务`start-dfs.sh`
此时配置的另外两台的namenode会报错启动失败，执行命令 `hdfs namenode -bootstrapStandby`
然后重启集群 `stop-dfs.sh`  `start-dfs.sh`
最后检查各个namenode的状态 `bin/hdfs haadmin -getServiceState nn1/nn2/nn2`

相比namenode单节点，多了两个进程

| 进程                    | 描述                                     |
| ----------------------- | ---------------------------------------- |
| JournalNode             | 用于存储namenode的数据                   |
| DFSZKFailoverController | 用于在发生namenode宕机是进行自动故障转移 |

#### 5.1.4.2 resourcemanger高可用

yarn-site.xml中添加配置

```
    <!-- Yarn高可用 -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarncluster</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2,rm3</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>cos-bigdata-test-flink-01</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>cos-bigdata-test-flink-02</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm3</name>
        <value>cos-bigdata-test-flink-03</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>cos-bigdata-test-flink-01:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>cos-bigdata-test-flink-02:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm3</name>
        <value>cos-bigdata-test-flink-03:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm1</name>
        <value>cos-bigdata-test-flink-01:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm2</name>
        <value>cos-bigdata-test-flink-02:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm3</name>
        <value>cos-bigdata-test-flink-03:8032</value>
    </property>
    <!-- 指定 AM 向 rm1 申请资源的地址 -->
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm1</name> 
        <value>cos-bigdata-test-flink-01:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm2</name> 
        <value>cos-bigdata-test-flink-02:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm3</name> 
        <value>cos-bigdata-test-flink-03:8030</value>
    </property>
    <!-- 指定供 NM 连接的地址 --> 
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
        <value>cos-bigdata-test-flink-01:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
        <value>cos-bigdata-test-flink-02:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm3</name>
        <value>cos-bigdata-test-flink-03:8031</value>
    </property>
    <!-- 启用自动恢复 --> 
    <property>
        <name>yarn.resourcemanager.recovery.enabled</name>
        <value>true</value>
    </property>
<!-- 指定 resourcemanager 的状态信息存储在 zookeeper 集群 --> 
    <property>
        <name>yarn.resourcemanager.store.class</name> 
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
<!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
<!--     <property>
        <name>yarn.client.failover-proxy-provider</name>
        <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>
    </property> -->
    <property>
        <name>hadoop.zk.address</name>
        <value>cos-bigdata-test-flink-01:2181,cos-bigdata-test-flink-02:2181,cos-bigdata-test-flink-03:2181</value>
    </property>
```

启动yarn `start-yarn.sh`，访问任何一个resourcemanager都会重定向到activate的那个resourcemanager上。
查看resourcemanager状态 `yarn rmadmin -getServiceState rm1/rm2`

<br>
**启动Flink-on-yarn任务**
因为namenode和resourcemanager都是集群，Flink-on-yarn任务无法指定其中一个节点的地址。此时可以将hadoop的配置文件引入，在flink-conf.yml中添加配置；如果配置了flink-ha，则high-availability.storageDir也需要修改为namenode集群地址。

```
fs.hdfs.hadoopconf: $HADOOP_HOME/etc/hadoop

yarn.application-attempts: 2
high-availability: zookeeper
high-availability.storageDir: hdfs:///flink/yarn/ha
high-availability.zookeeper.quorum: bigdata1:2181,bigdata2:2181,bigdata3:2181
high-availability.zookeeper.path.root: /flink-yarn
```

可以将hadoop集群的地址和端口省略，例如
`./flink run -s hdfs:///flink/checkpoint/msas/msas_device_exceptions/0bbbf051f47e795067032e7aa612aecd/chk-9884/_metadata --allowNonRestoredState -m yarn-cluster -ynm ADS_DEVICE_MSAS_EXCEPTIONS_test -p 2 -ys 2 -yjm 1024 -ytm 2048m -d -c com.iotmars.compass.MsasDeviceExceptionsApp -yqu default -yD metrics.reporter.promgateway.class=org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter -yD metrics.reporter.promgateway.host=192.168.101.174 -yD metrics.reporter.promgateway.port=9091 -yD metrics.reporter.promgateway.jobName=flink-metrics- -yD metrics.reporter.promgateway.randomJobNameSuffix=true -yD metrics.reporter.promgateway.deleteOnShutdown=false -yD metrics.reporter.promgateway.groupingKey="instance=ADS_DEVICE_MSAS_EXCEPTIONS_test" /opt/jar/test/ADS_DEVICE_MSAS_EXCEPTIONS-1.0-SNAPSHOT.jar`

> 启动时namenode和resourcemanager会进行故障转移，知道找到活跃的节点。在访问日志时，historyserver也会进行故障转移，找到可用的namenode节点。

<br>

## 5.2 Hive 3.1.2

1. 解压
2. 在lib中放入mysql驱动 mysql-connector-java-5.1.45.jar
3. conf中创建hive-site.xml配置文件

```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- 数据库配置 -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://192.168.101.174:3306/metastore?useSSL=false</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>Password@123</value>
    </property>

    <!-- 内部表元数据存储路径 -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <!-- Hive元数据存储版本的验证 -->
    <property>
        <name>hive.metastore.schema.verification</name>
        <value>false</value>
    </property>

    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>

    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>bigdata1</value>
    </property>

    <!-- 元数据授权存储 -->
    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
    </property>
    <!-- 显示查询头信息和当前数据库 -->    
    <property>
        <name>hive.cli.print.header</name>
        <value>true</value>
    </property>

    <property>
        <name>hive.cli.print.current.db</name>
        <value>true</value>
    </property>
</configuration>
```

> 这里没有配置hive.metastore.uris参数，所以metastore实际没有使用到，启动hiveserver2时不需要启动metastore，即hiveserver2直连数据库而不是通过metastore进行连接。实际使用中，直连mysql的任务过多会导致获取原数据失败，所以推荐使用metastore进行连接。

4. 修改/opt/module/hive3.1.2/conf/hive-log4j.properties.template文件名称为hive-log4j.properties，修改 log 存放位置hive.log.dir=/opt/module/hive3.1.2/logs
5. 创建数据库metastore，执行命令`schematool -initSchema -dbType mysql -verbose`，在数据库中创建表。
6. 修改/etc/profile.d/my_env.sh，添加环境变量

```
#HIVE_HOME
export HIVE_HOME=/opt/module/hive
export PATH=$PATH:$HIVE_HOME/bin
```

7. 运行bin/hive进入本地客户端。
   如果需要通过beeline进行远程连接，需要启动metastore服务和hiveserver2服务。

```
nohup ./hive --service metastore &
nohup ./hive --service hiveserver2 &
```

登陆用户在hadoop配置文件core-site.xml中配置

```
    <!-- 配置HDFS网页登陆使用的静态用户为hxr -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>hxr</value>
    </property>
    <!-- 配置hxr允许通过代理访问主机节点 -->
    <property>
        <name>hadoop.proxyuser.hxr.hosts</name>
        <value>*</value>
    </property>
```

<br>

## 5.2 Tez 0.10.1

因为tez-0.9.x版本过低，不能和hive3.1.2搭配使用，所以使用tez-0.10.1版本。

下载[Tez安装包](apache.org)](https://dlcdn.apache.org/tez/0.10.1/)
和[Tez-UI.war包](https://repository.apache.org/content/repositories/releases/org/apache/tez/tez-ui/0.10.1/)

### 5.2.1 Tez安装

1. 解压安装包到指定目录

```
tar –zxvf apache-tez-0.10.1-bin.tar.gz -C /opt/module/
```

重命名解压后的文件为tez-0.10.1。

2. 需要在hive的配置文件hive-env.sh中引入tez的所有jar包：

```
export TEZ_HOME=/opt/module/tez-0.10.1
export TEZ_JARS=""
for jar in `ls $TEZ_HOME | grep jar`;do
        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar
done

for jar in `ls $TEZ_HOME/lib`;do
        export TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar
done

export ATLAS_HOME=/opt/module/atlas-2.1.0
export ATLAS_JARS=""
for jar in `ls $ATLAS_HOME/hook/hive | grep jar`;do
        export ATLAS_JARS=$ATLAS_JARS:$ATLAS_HOME/hook/hive/$jar
done

for jar in `ls $ATLAS_HOME/hook/hive/atlas-hive-plugin-impl`;do
        export ATLAS_JARS=$ATLAS_JARS:$ATLAS_HOME/hook/hive/atlas-hive-plugin-impl/$jar
done

export HIVE_AUX_JARS_PATH=$HADOOP_HOME/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS$ATLAS_JARS
```

> 这里因为使用了Altas的钩子，需要添加相关的包。所以将JARS包加入到hive的读取路径中

3. 在Hive的/opt/module/hive/conf下面创建一个tez-site.xml文件，添加如下内容

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xs1" href="configuration.xsl"?>
<configuration>
    <property>
        <name>tez.lib.uris</name>
        <value>${fs.defaultFS}/tez/apache-tez-0.10.1-bin.tar.gz</value>
    </property>
    <property>
        <name>tez.use.cluster.hadoop-libs</name>
        <value>true</value>
    </property>
    <property>
        <name>tez.history.logging.service.class</name>
        <value>org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService</value>
    </property>

    <property>
        <name>tez.am.resource.memory.mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>hive.tez.cpu.vcores</name>
        <value>1</value>
    </property>
    <property>
        <name>hive.tez.container.size</name>
        <value>10240</value>
    </property>
    <property>
        <name>tez.runtime.io.sort.mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>tez.runtime.unordered.output.buffer.size-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>tez.am.container.reuse.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>tez.am.speculation.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>hive.tez.auto.reducer.parallelism</name>
        <value>true</value>
    </property>
</configuration>
```

4. 将/opt/module/tez-0.10.1上传到HDFS的/tez路径，使所有的hdfs节点都可以使用tez。

   ```
    hadoop fs -mkdir /tez
    hadoop fs -put /opt/software/apache-tez-0.10.1-bin.tar.gz /tez
    hadoop fs -ls /tez
   ```

   放置的路径需要与tez-site.xml中配置的路径对应.
