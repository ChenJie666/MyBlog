---
title: 框架开机自启配置
categories:
- 大数据离线
---
# 脚本
**hadoop.sh**
```
#!/bin/bash



```

**zookeeper.sh**
```
#!/bin/bash

case $1 in
"start")
    echo ----- 开启zookeeper集群 -----
    for host in bigdata1 bigdata2 bigdata3
    do
        ssh ${host} "/opt/module/zookeeper-3.4.10/bin/zkServer.sh start"
    done
;;
"stop")
    echo ----- 关闭zookeeper集群 -----
    for host in bigdata1 bigdata2 bigdata3
    do
        ssh ${host} "/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop"
    done
;;
"status")
    echo ----- 查看zookeeper集群状态 -----
    for host in bigdata1 bigdata2 bigdata3
    do
        ssh ${host} "/opt/module/zookeeper-3.4.10/bin/zkServer.sh status"
    done
;;
esac
```

**kafka.sh**
```
#!/bin/bash

case $1 in
"start")
    for host in bigdata1 bigdata2 bigdata3
    do
        ssh ${host} "source /etc/profile ; export JMX_PORT=9988 ; nohup /opt/module/kafka-2.11/bin/kafka-server-start.sh /opt/module/kafka-2.11/config/server.properties 1>/dev/null 2>&1 &"
        if [ $? -eq 0 ]
        then
            echo ----- ${host} kafka启动成功 -----
        fi
    done
;;
"stop")
    for host in bigdata1 bigdata2 bigdata3
    do
        ssh ${host} "source /etc/profile ; /opt/module/kafka-2.11/bin/kafka-server-stop.sh"
        if [ $? -eq 0 ]
        then
            echo ----- ${host} kafka关闭成功 -----
        fi
    done
;;
esac
```

**flume.sh**
```
#!/bin/bash

case $1 in
"start")
    for host in bigdata1 #bigdata2
    do
#       ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/file-kafka-hdfs.conf 1>/dev/null 2>&1 &"
        #ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/log-kafka.conf 1>/dev/null 2>&1 &"
        ssh bigdata1 "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/edb_kafka2hdfs.job 1>/dev/null 2>&1 &"
        #ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/kafka-hdfs.conf 1>/dev/null 2>&1 &"
        #ssh bigdata2 "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/kafka-hdfs.conf 1>/dev/null 2>&1 &"
        if [ $? -eq 0 ]
        then
            echo ----- ${host} flume启动成功 -----
        fi
    done
;;
"stop")
    for host in bigdata1 #bigdata2
    do
        ssh ${host} "source /etc/profile ; ps -ef | awk -F \" \" '/edb_kafka2hdfs.job/ && !/awk/{print \$2}' | xargs kill "
        #ssh ${host} "source /etc/profile ; ps -ef | awk -F \" \" '/kafka-hdfs.conf/ && !/awk/{print \$2}' | xargs kill "
        if [ $? -eq 0 ]
        then
            echo ----- ${host} flume关闭成功 -----
        fi
    done
;;
esac
```

**hiveserver2.sh**
```
sudo -i -u hxr nohup hiveserver2 >/opt/module/hive-3.1.2/logs/hive-on-spark.log  2>/opt/module/hive-3.1.2/logs/hive-on-spark.err &
```



**datax-web.sh**
```
/data/datax/bin/start-all.sh
```

**datax-executor.sh**
```

```

**prometheus**
```

```


**grafana**
```

```


# 部署开机自启
