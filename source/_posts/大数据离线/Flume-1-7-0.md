---
title: Flume-1-7-0
categories:
- å¤§æ•°æ®ç¦»çº¿
---
# ä¸€ã€ä»»åŠ¡æ–‡ä»¶
## 1.1 è¯´æ˜
flumeï¼š
sourceå¯ä»¥æ¥æ”¶netcatã€execã€spooldirã€taildirã€avroç­‰æ¥æºçš„æ•°æ®ã€‚
taildiræ”¯æŒç›‘æ§å¤šç›®å½•ï¼ˆfilegroupså‚æ•°ä»¥ç©ºæ ¼åˆ†éš”ï¼‰å¹¶å®ç°æ–­ç‚¹ç»­ä¼ ï¼Œtaildirçš„æ–­ç‚¹ä¿¡æ¯å­˜å‚¨åœ¨taildir_position.json æ–‡ä»¶ä¸­ã€‚fileHeaderå¯ä»¥ç”¨äºé‡‡é›†å¤šä¸ªæ–‡ä»¶å¹¶è¿›è¡ŒåŒºåˆ†ï¼Œè®¾ä¸ºtrueï¼Œåˆ™å¯ä»¥æŒ‡å®šheaderä¸­çš„keyçš„å€¼ï¼Œvalueé»˜è®¤ä¸ºæ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚

file channelï¼šå°†å†™åˆ°channelçš„eventä¿å­˜åˆ°ç£ç›˜çš„dataDirsæ–‡ä»¶ä¸­ï¼Œä¼šåœ¨å†…å­˜ä¸­ç»´æŠ¤ä¸€ä¸ªé˜Ÿåˆ—ï¼Œé˜Ÿåˆ—ä¸­å­˜å‚¨äº†å†™åˆ°æ–‡ä»¶ä¸­çš„è¿˜æœªè¢«sinkæ¶ˆè´¹çš„eventçš„ç‰©ç†åœ°å€ã€‚é˜²æ­¢å†…å­˜ä¸­çš„é˜Ÿåˆ—ä¿¡æ¯ä¸¢å¤±ï¼Œä¼šé—´éš”checkpointIntervalæ—¶é—´ä¿å­˜åˆ°æœ¬åœ°çš„checkpointDirä¸­ï¼Œè¿˜æœ‰ä¸€ä¸ªbackupCheckpointDirä½œä¸ºcheckpointDirçš„å¤‡ä»½ã€‚

sinkå¯ä»¥å°†æ•°æ®å‘å¾€hdfsã€hbaseã€loggerã€avroç­‰ç›®çš„åœ°

flumeä¸­å­˜åœ¨äº‹åŠ¡ï¼štransactionCapacity

![image.png](Flume-1-7-0.assets\d2c9138c58ea41c49cd97e279372ed29.png)


## 1.2 ä»»åŠ¡æ–‡ä»¶é…ç½®
ä¸»è¦å‚æ•°è¯´æ˜ï¼š

| å±æ€§åç§° |	é»˜è®¤å€¼ |	å«ä¹‰ |
|---|---|---|
|capacity|	100	|å­˜å‚¨åœ¨channelä¸­çš„æœ€å¤§eventæ•°é‡ï¼Œå³ queue çš„å¤§å°
|transactionCapacity|	100	|source å°† event put åˆ° channelï¼Œæˆ–è€… sink ä» channel take event çš„æ¯æ¬¡æœ€å¤§äº‹ç‰©æ•°ã€‚putListå’ŒtakeListçš„å¤§å°ã€‚transactionCapacity å¿…é¡»ä¸å¤§äº capacity|
|keep-alive|	3|	æ·»åŠ æˆ–ç§»é™¤ event çš„è¶…æ—¶æ—¶é—´|
|byteCapacityBufferPercentage|	20	|æ‰€æœ‰ event çš„ header å­—èŠ‚æ•°å  byteCapacity çš„ç™¾åˆ†æ¯”|
|byteCapacity	|JVMçš„80%|	channel ä¸­å…è®¸æ‰€æœ‰ event å­—èŠ‚æ•°ä¹‹å’Œçš„æœ€å¤§å€¼ï¼ˆåªè®¡ç®— event çš„ body å­—èŠ‚æ•°ï¼Œä¸åŒ…å« headerï¼‰ã€‚defaultByteCapacityç­‰äºJVMå¯ç”¨æœ€å¤§å†…å­˜çš„80ï¼…ï¼ˆ-Xmxçš„80%ï¼‰ã€‚å¦‚æœå®šä¹‰äº†byteCapacityï¼Œåˆ™ å®é™…byteCapacity = å®šä¹‰çš„byteCapacity * (1- Event headerç™¾åˆ†æ¯”) / byteCapacitySlotSize ã€‚byteCapacitySlotSize é»˜è®¤ç­‰äº100|
|flumeBatchSize|	100|	KafkaSink ä¸€æ¬¡äº‹åŠ¡å‘é€çš„ event æ•°|


**Sourceé…ç½®**
| å‚æ•° | è¯´æ˜ |
| --- | --- |
|  |  |

**Channelé…ç½®**
| å‚æ•° | è¯´æ˜ |
| --- | --- |
|  |  |

**HDFS Sinké…ç½®**
| å‚æ•° | è¯´æ˜ |
| --- | --- |
| a2.sinks.k1.hdfs.filePrefix | ä¸Šä¼ æ–‡ä»¶çš„å‰ç¼€  |
| a2.sinks.k1.hdfs.round | æ˜¯å¦æŒ‰ç…§æ—¶é—´æ»šåŠ¨æ–‡ä»¶å¤¹ |
| a2.sinks.k1.hdfs.roundValue | å¤šå°‘æ—¶é—´å•ä½åˆ›å»ºä¸€ä¸ªæ–°çš„æ–‡ä»¶å¤¹  |
| a2.sinks.k1.hdfs.roundUnit | é‡æ–°å®šä¹‰æ—¶é—´å•ä½  |
| a2.sinks.k1.hdfs.useLocalTimeStamp | æ˜¯å¦ä½¿ç”¨æœ¬åœ°æ—¶é—´æˆ³  |
| a2.sinks.k1.hdfs.fileType | è®¾ç½®æ–‡ä»¶ç±»å‹ï¼Œå¯æ”¯æŒå‹ç¼©  |
| a2.sinks.k1.hdfs.rollInterval | å¤šä¹…ç”Ÿæˆä¸€ä¸ªæ–°çš„æ–‡ä»¶ |
| a2.sinks.k1.hdfs.rollSize | è®¾ç½®æ¯ä¸ªæ–‡ä»¶çš„æ»šåŠ¨å¤§å° |
| a2.sinks.k1.hdfs.rollCount | æ–‡ä»¶çš„æ»šåŠ¨ä¸Eventæ•°é‡æ— å…³ |
| a2.sinks.k1.hdfs.batchSize | ç§¯æ”’å¤šå°‘ä¸ªEventæ‰flushåˆ°HDFSä¸€æ¬¡ |

hdfs.useLocalTimeStampé…ç½®è¯¦è§£ï¼š
hdfs.useLocalTimeStampä¸ºtrueæ—¶ï¼Œç›¸å½“äºå°†eventçš„headerçš„timestampå±æ€§è®¾ç½®ä¸ºå½“å‰æ—¶é—´çš„æ—¶é—´æˆ³ï¼ˆ13ä½ï¼‰ã€‚å¦‚æœå°†falseè®¾ç½®ä¸ºfalseï¼Œåˆ™æˆ‘ä»¬éœ€è¦è‡ªå·±è®¾ç½®headerä¸­çš„timestampå±æ€§ï¼Œè¿™æ ·flumeå°±ä¼šæ ¹æ®timestampä¸­çš„æ—¶é—´è¿›è¡Œåˆ†åŒºä¿å­˜ã€‚è¯¦è§ä¸‹æ–‡TimestampInterceptorã€‚

### 1.2.1 Kafkaä½œä¸ºChannelï¼Œç›´æ¥å­˜å‚¨åˆ°HDFS
**file-kafka-hdfsï¼š**
```
# agent
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# source
a1.sources.r1.channels = c1
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume-1.7.0/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/q6/.*log
a1.sources.r1.fileHeader = false
a1.sources.r1.maxBatchCount = 1000

# interceptor
a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = com.hxr.flume.LogETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.hxr.flume.LogTypeInterceptor$Builder

# multiplexing selector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.Log_Q6 = c1

# channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
a1.channels.c1.kafka.topic = log_q6
a1.channels.c1.parseAsFlumeEvent = false


# sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/device/logs/q6/%Y-%m-%d/
a1.sinks.k1.hdfs.filePrefix = q6-
#a1.sinks.k1.hdfs.round = true
#a1.sinks.k1.hdfs.roundValue = 10
#a1.sinks.k1.hdfs.roundUnit = minute
#a1.sinks.k1.hdfs.rollInterval = 3600
#a1.sinks.k1.hdfs.rollSize = 134217728
#a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.useLocalTimeStamp = true

a1.sinks.k1.hdfs.rollInterval = 3600
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

# codec LZOP
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k1.hdfs.fileType = CompressedStream

# æ‹¼è£…
a1.sinks.k1.channel = c1
```


### 1.2.2 å­˜å‚¨åˆ°Kafkaï¼Œå†ä»Kafkaè¾“å‡ºåˆ°HDFSï¼ˆåˆ†ä¸ºä¸¤ä¸ªä»»åŠ¡ï¼Œæ¨èï¼‰
**log-kafka.conf**
```
#define
a1.sources= r1
a1.channels= c1 c2

#source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume-1.7.0/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/logs/q6/.*log
a1.sources.r1.fileHeader = false
a1.sources.r1.maxBatchCount = 1000

#interceptors
#a1.sources.r1.interceptors = i1 i2
#a1.sources.r1.interceptors.i1.type = com.hxr.flume.LogETLInterceptor$Builder
a1.sources.r1.interceptors = i2
a1.sources.r1.interceptors.i2.type = com.hxr.flume.LogTypeInterceptor$Builder

#selector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.Log_Q6 = c1
a1.sources.r1.selector.mapping.Log_E5 = c2

#channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a1.channels.c1.kafka.topic = ModelLog_Q6
a1.channels.c1.parseAsFlumeEvent = false

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a1.channels.c2.kafka.topic = ModelLog_E5
a1.channels.c2.parseAsFlumeEvent = false

#combine
a1.sources.r1.channels = c1 c2
```

**kafka-hdfs.conf**
```
#define
a2.sources= r1 r2
a2.channels= c1 c2
a2.sinks = k1 k2

#source
a2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r1.batchSize = 5000
a2.sources.r1.batchDurationMillis = 2000
a2.sources.r1.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a2.sources.r1.kafka.topics = Log_Q6
a2.sources.r1.kafka.consumer.group.id = custom.q6

a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r2.batchSize = 5000
a2.sources.r2.batchDurationMillis = 2000
a2.sources.r2.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a2.sources.r2.kafka.topics = Log_E5
a2.sources.r2.kafka.consumer.group.id = custom.e5

#channels
a2.channels.c1.type = memory
a2.channels.c1.capacity = 10000
a2.channels.c1.transactionCapacity = 10000
a2.channels.c1.byteCapacityBufferPercentage = 20
a2.channels.c1.byteCapacity = 800000

a2.channels.c2.type = memory
a2.channels.c2.capacity = 10000
a2.channels.c2.transactionCapacity = 10000
a2.channels.c2.byteCapacityBufferPercentage = 20
a2.channels.c2.byteCapacity = 800000

#sink
a2.sinks.k1.type = hdfs
a2.sinks.k1.hdfs.path = /origin_data/device_model_log/logs/q6/%Y-%m-%d
a2.sinks.k1.hdfs.filePrefix = q6-
a2.sinks.k1.hdfs.rollInterval = 3600
a2.sinks.k1.hdfs.rollSize = 134217728
a2.sinks.k1.hdfs.rollCount = 0
a2.sinks.k1.hdfs.useLocalTimeStamp = true

a2.sinks.k2.type = hdfs
a2.sinks.k2.hdfs.path = /origin_data/device_model_log/logs/e5/%Y-%m-%d
a2.sinks.k2.hdfs.filePrefix = e5-
a2.sinks.k2.hdfs.rollInterval = 3600
a2.sinks.k2.hdfs.rollSize = 134217728
a2.sinks.k2.hdfs.rollCount = 0
a2.sinks.k2.hdfs.useLocalTimeStamp = true

#compress
a2.sinks.k1.hdfs.codeC = lzop
a2.sinks.k1.hdfs.fileType = CompressedStream

a2.sinks.k2.hdfs.codeC = lzop
a2.sinks.k2.hdfs.fileType = CompressedStream

#combine
a2.sources.r1.channels = c1
a2.sources.r2.channels = c2
a2.sinks.k1.channel = c1
a2.sinks.k2.channel = c2
```

### 1.2.3 é™¤äº†memory channelï¼Œè¿˜å¯ä»¥ä½¿ç”¨file channel
**ä½¿ç”¨file channel**
```
# agent 
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
a1.sources.r1.kafka.topics = compass-edb-sales-test
a1.sources.r1.kafka.consumer.group.id = edb_consumer

# channels
#a1.channels.c1.type = memory
#a1.channels.c1.capacity = 10000
#a1.channels.c1.transactionCapacity = 10000
#a1.channels.c1.byteCapacityBufferPercentage = 20
#a1.channels.c1.byteCapacity = 800000
a1.channels.c1.type = file 
a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/checkpoint/edb_file_channel
a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/data/edb_file_channel

# sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/compass/edb_test/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = edb
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.rollInterval = 3600
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.useLocalTimeStamp =true


# compress
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k1.hdfs.fileType = CompressedStream

# combine
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

## 1.3 ä»»åŠ¡å¯åŠ¨å’Œè„šæœ¬
**å¯åŠ¨ä»»åŠ¡å‘½ä»¤ï¼š**`/opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/log-kafka.conf  -Dflume.root.logger=DEBUG,FILE-Dflume.log.file=flume.log`

**å¯åŠ¨/å…³é—­è„šæœ¬**
```
#!/bin/bash

case $1 in
"start")
    for host in bigdata1 #bigdata2
    do
#       ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/file-kafka-hdfs.conf 1>/dev/null 2>&1 &"
        #ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/log-kafka.conf 1>/dev/null 2>&1 &"
        ssh bigdata1 "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/log-kafka.conf 1>/dev/null 2>&1 &"
        #ssh ${host} "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/kafka-hdfs.conf 1>/dev/null 2>&1 &"
        ssh bigdata2 "source /etc/profile ; nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/kafka-hdfs.conf 1>/dev/null 2>&1 &"
        if [ $? -eq 0 ]
        then
            echo ----- ${host} flumeå¯åŠ¨æˆåŠŸ -----
        fi
    done
;;
"stop")
    for host in bigdata1 #bigdata2
    do
        ssh ${host} "source /etc/profile ; ps -ef | awk -F \" \" '/log-kafka.conf/ && !/awk/{print \$2}' | xargs kill "
        ssh ${host} "source /etc/profile ; ps -ef | awk -F \" \" '/kafka-hdfs.conf/ && !/awk/{print \$2}' | xargs kill "
        if [ $? -eq 0 ]
        then
            echo ----- ${host} flumeå…³é—­æˆåŠŸ -----
        fi
    done
;;
esac
```

<br>
# äºŒã€Flumeç»„ä»¶

![image.png](Flume-1-7-0.assets819b1a830f2471ab59e420b127b109f.png)

## 2.1 ChannelSelector
ChannelSelectorçš„ä½œç”¨å°±æ˜¯é€‰å‡ºEventå°†è¦è¢«å‘å¾€å“ªä¸ªChannelã€‚å…¶å…±æœ‰ä¸¤ç§ç±»å‹ï¼Œåˆ†åˆ«æ˜¯Replicatingï¼ˆå¤åˆ¶ï¼‰å’ŒMultiplexingï¼ˆå¤šè·¯å¤ç”¨ï¼‰ã€‚
ReplicatingSelectorä¼šå°†åŒä¸€ä¸ªEventå‘å¾€æ‰€æœ‰çš„Channelï¼ŒMultiplexingä¼šæ ¹æ®ç›¸åº”çš„åŸåˆ™ï¼Œå°†ä¸åŒçš„Eventå‘å¾€ä¸åŒçš„Channelã€‚

## 2.2 SinkProcessor
SinkProcessorå…±æœ‰ä¸‰ç§ç±»å‹ï¼Œåˆ†åˆ«æ˜¯DefaultSinkProcessorã€LoadBalancingSinkProcessorå’ŒFailoverSinkProcessorã€‚
DefaultSinkProcessorå¯¹åº”çš„æ˜¯å•ä¸ªçš„Sinkï¼ŒLoadBalancingSinkProcessorå’ŒFailoverSinkProcessorå¯¹åº”çš„æ˜¯Sink Groupï¼ŒLoadBalancingSinkProcessorå¯ä»¥å®ç°è´Ÿè½½å‡è¡¡çš„åŠŸèƒ½ï¼ŒFailoverSinkProcessorå¯ä»¥é”™è¯¯æ¢å¤çš„åŠŸèƒ½ã€‚

## 2.3 è‡ªå®šä¹‰æ‹¦æˆªå™¨
è‡ªå®šä¹‰æ‹¦æˆªå™¨ä¸»è¦åˆ†ä¸¤ç§ï¼šETL æ‹¦æˆªå™¨ã€æ—¥å¿—ç±»å‹åŒºåˆ†æ‹¦æˆªå™¨ã€‚
- ETL æ‹¦æˆªå™¨ä¸»è¦ç”¨äºè¿‡æ»¤æ—¶é—´æˆ³ä¸åˆæ³•å’Œ Json æ•°æ®ä¸å®Œæ•´çš„æ—¥å¿—ã€‚
- æ—¥å¿—ç±»å‹åŒºåˆ†æ‹¦æˆªå™¨ä¸»è¦ç”¨äºï¼Œå°†å¯åŠ¨æ—¥å¿—å’Œäº‹ä»¶æ—¥å¿—åŒºåˆ†å¼€æ¥ï¼Œæ–¹ä¾¿å‘å¾€ Kafka çš„ä¸åŒTopicã€‚

### 2.3.1 æ‹¦æˆªå™¨è§„åˆ’
#### ä»Logåˆ°Kafka
![image.png](Flume-1-7-0.assets8f2481d535847d18d37d56e66d8e309.png)

æ—¥å¿—è¿‡æ»¤å™¨ï¼šå¯¹æ—¥å¿—æ ¼å¼è¿›è¡Œæ ¡éªŒã€‚

#### ä»Kafkaåˆ°HDFS
![image.png](Flume-1-7-0.assets\7eb7d3dfe7f248c8b1e4964fd5620245.png)

**æ—¶é—´è¿‡æ»¤å™¨ï¼š**
ç”±äºFlumeé»˜è®¤ä¼šç”¨Linuxç³»ç»Ÿæ—¶é—´ï¼Œä½œä¸ºè¾“å‡ºåˆ°HDFSè·¯å¾„çš„æ—¶é—´ã€‚å¦‚æœæ•°æ®æ˜¯23:59åˆ†äº§ç”Ÿçš„ã€‚Flumeæ¶ˆè´¹Kafkaé‡Œé¢çš„æ•°æ®æ—¶ï¼Œæœ‰å¯èƒ½å·²ç»æ˜¯ç¬¬äºŒå¤©äº†ï¼Œé‚£ä¹ˆè¿™éƒ¨é—¨æ•°æ®ä¼šè¢«å‘å¾€ç¬¬äºŒå¤©çš„HDFSè·¯å¾„ã€‚æˆ‘ä»¬å¸Œæœ›çš„æ˜¯æ ¹æ®æ—¥å¿—é‡Œé¢çš„å®é™…æ—¶é—´ï¼Œå‘å¾€HDFSçš„è·¯å¾„ï¼Œæ‰€ä»¥ä¸‹é¢æ‹¦æˆªå™¨ä½œç”¨æ˜¯è·å–æ—¥å¿—ä¸­çš„å®é™…æ—¶é—´ã€‚
è§£å†³çš„æ€è·¯ï¼šæ‹¦æˆªjsonæ—¥å¿—ï¼Œé€šè¿‡fastjsonæ¡†æ¶è§£æjsonï¼Œè·å–å®é™…æ—¶é—´tsã€‚å°†è·å–çš„tsæ—¶é—´å†™å…¥æ‹¦æˆªå™¨headerå¤´ï¼Œheaderçš„keyå¿…é¡»æ˜¯timestamp(æ¯«ç§’çº§)ï¼Œå› ä¸º**Flumeæ¡†æ¶ä¼šæ ¹æ®è¿™ä¸ªkeyçš„å€¼è¯†åˆ«ä¸ºæ—¶é—´ï¼Œå†™å…¥åˆ°HDFS**ã€‚


### 2.3.2 ä»£ç å®ç°
**ä¾èµ–**
```
<dependencies>
        <dependency>
            <groupId>org.apache.flume</groupId>
            <artifactId>flume-ng-core</artifactId>
            <version>1.7.0</version>
            <scope>provided</scope>
        </dependency>
        <dependency>
            <groupId>com.alibaba</groupId>
            <artifactId>fastjson</artifactId>
            <version>1.2.62</version>
        </dependency>

        <dependency>
            <groupId>com.microsoft.sqlserver</groupId>
            <artifactId>mssql-jdbc</artifactId>
            <version>7.0.0.jre8</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>2.3.2</version>
                <configuration>
                    <source>1.8</source>
                    <target>1.8</target>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>3.1.1</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
```

**ETLInterceptor**
```
public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }
}
```

**TimeStampInterceptor**
```
public class TimeStampInterceptor implements Interceptor {

    private ArrayList<Event> events = new ArrayList<>();

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

        JSONObject jsonObject = JSONObject.parseObject(log);

        String ts = jsonObject.getString("ts");
        headers.put("timestamp", ts);

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        events.clear();
        for (Event event : list) {
            events.add(intercept(event));
        }

        return events;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {
        @Override
        public Interceptor build() {
            return new TimeStampInterceptor();
        }

        @Override
        public void configure(Context context) {
        }
    }
}
```

```
# agent
a1.sources = r1
a1.channels = c1
a1.sinks = k1

# sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
a1.sources.r1.kafka.topics = compass-edb-returnSales-prod
a1.sources.r1.kafka.consumer.group.id = edb_returnSales_consumer

# interceptor
a1.sources.r1.interceptors = i1 i2
a1.sources.r1.interceptors.i1.type = com.iotmars.flume.ETLInterceptor$Builder
a1.sources.r1.interceptors.i2.type = com.iotmars.flume.TimeStampInterceptor$Builder

# channels
#a1.channels.c1.type = memory
#a1.channels.c1.capacity = 10000
#a1.channels.c1.transactionCapacity = 10000
#a1.channels.c1.byteCapacityBufferPercentage = 20
#a1.channels.c1.byteCapacity = 800000
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/checkpoint/edb_returnSales_channel
a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/data/edb_returnSales_channel

# sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/compass/edb_salesReturn/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = edb_returnSales
a1.sinks.k1.hdfs.batchSize = 100
a1.sinks.k1.hdfs.rollInterval = 3600
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
a1.sinks.k1.hdfs.useLocalTimeStamp = false


# compress
a1.sinks.k1.hdfs.codeC = lzop
a1.sinks.k1.hdfs.fileType = CompressedStream

# combine
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

### 2.3.3 é…ç½®æ‹¦æˆªå™¨
æ‰“åŒ…åå°† å¸¦æœ‰ä¾èµ–çš„jaråŒ… æ”¾åˆ°flumeçš„libç›®å½•ä¸‹ã€‚

åœ¨ä»»åŠ¡æ–‡ä»¶ä¸­é…ç½®æ‹¦æˆªå™¨
```
a1.sources.r1.interceptors =  i1
a1.sources.r1.interceptors.i1.type = com.atguigu.flume.interceptor.ETLInterceptor$Builder
```



<br>
# ä¸‰ã€ç›‘æ§
| å­—æ®µï¼ˆå›¾è¡¨åç§°ï¼‰	| å­—æ®µå«ä¹‰ |
|---|---|
|EventPutAttemptCount |	sourceå°è¯•å†™å…¥channelçš„äº‹ä»¶æ€»æ•°é‡ |
|EventPutSuccessCount |	æˆåŠŸå†™å…¥channelä¸”æäº¤çš„äº‹ä»¶æ€»æ•°é‡ |
|EventTakeAttemptCount |	sinkå°è¯•ä»channelæ‹‰å–äº‹ä»¶çš„æ€»æ•°é‡ã€‚ |
|EventTakeSuccessCount |	sinkæˆåŠŸè¯»å–çš„äº‹ä»¶çš„æ€»æ•°é‡ |
|StartTime |	channelå¯åŠ¨çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |
|StopTime |	channelåœæ­¢çš„æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ |
|ChannelSize |	ç›®å‰channelä¸­äº‹ä»¶çš„æ€»æ•°é‡ |
|ChannelFillPercentage |	channelå ç”¨ç™¾åˆ†æ¯” |
|ChannelCapacity |	channelçš„å®¹é‡ |


<br>
# å››ã€è°ƒä¼˜
ä¼˜åŒ–è¦ä»ä¸‰ä¸ªç»„ä»¶çš„è§’åº¦å»åˆ†åˆ«ä¼˜åŒ–ã€‚

## 1ã€source

sourcesæ˜¯flumeæ—¥å¿—é‡‡é›†çš„èµ·ç‚¹ï¼Œç›‘æ§æ—¥å¿—æ–‡ä»¶ç³»ç»Ÿç›®å½•ã€‚å…¶ä¸­æœ€å¸¸ç”¨çš„æ˜¯ Spooling Directory Source ï¼Œ Exec Source å’Œ Avro Source ã€‚

å…³é”®å‚æ•°è®²è§£ï¼š

ï¼ˆ1ï¼‰batchSizeï¼šè¿™ä¸ªå‚æ•°å½“ä½ é‡‡ç”¨çš„æ˜¯ Exec Source æ—¶ï¼Œå«ä¹‰æ˜¯ä¸€æ¬¡è¯»å…¥channelçš„æ•°æ®çš„è¡Œæ•°ï¼Œå½“ä½ é‡‡ç”¨Spooling Directory Sourceå«ä¹‰æ˜¯ Granularityï¼ˆç²’åº¦ï¼‰ at which to batch transfer to the channel ï¼Œæ®æˆ‘åˆ†æåº”è¯¥æ˜¯eventsï¼ˆflumeæœ€å°å¤„ç†æ•°æ®å•å…ƒï¼‰çš„æ•°é‡ã€‚

è¿™ä¸ªå‚æ•°ä¸€èˆ¬ä¼šè®¾ç½®æ¯”è¾ƒå¤§ï¼Œä¸€èˆ¬çš„æ•°å€¼è·Ÿæ¯ç§’è¦å¤„ç†çš„æ•°å€¼ç›¸å½“ã€‚

ï¼ˆ2ï¼‰inputCharset è¿™ä¸ªå¾ˆé‡è¦ï¼Œå°±æ˜¯æ–‡æœ¬æ–‡ä»¶çš„ç¼–ç ï¼Œé»˜è®¤æ˜¯flumeæŒ‰ç…§utf-8å¤„ç†ï¼Œå¦‚æœæ–‡æœ¬æ•°æ®æ˜¯gbkï¼Œåˆ™è¦å¢åŠ æ­¤å‚æ•°,

ï¼ˆ3ï¼‰interceptors flumeè‡ªå¸¦çš„æ‹¦æˆªå™¨ï¼Œå¯ä»¥æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼å»è¿‡æ»¤æ•°æ®ï¼Œä½†æ˜¯æ®æˆ‘å®é™…ç»éªŒæ€»ç»“ï¼Œè¿™ä¸ªé…ç½®å¾ˆå½±å“å…¥åº“æ€§èƒ½ï¼Œå› æ­¤è¿™éƒ¨åˆ†å·¥ä½œæˆ‘åŸºæœ¬éƒ½åœ¨sinkä»£ç é‡Œé¢åšã€‚

## 2ã€channel

channel æ˜¯flumeçš„ä¸­é—´æ•°æ®ç¼“å­˜ç®¡é“ï¼Œæœ‰ç‚¹ç±»ä¼¼kafkaçš„æœºåˆ¶ï¼Œå› æ­¤ä¸ªç»„ä»¶çš„æ€§èƒ½å¾ˆé‡è¦ã€‚

æˆ‘åœ¨é¡¹ç›®ä¸­ä¸»è¦é‡‡ç”¨çš„æ˜¯menmory channel,åŸå› æ˜¯æ•°æ®é‡å¤§ï¼Œè¦æ±‚æå¤§çš„æ•°æ®ååé‡å’Œé€Ÿåº¦ï¼Œä½†æ˜¯æœ‰ä¸€ç‚¹ä¸å¥½çš„æ˜¯

å¦‚æœä¸€æ—¦flumeè¿›ç¨‹downæ‰ï¼Œæ˜¯æ²¡æœ‰â€œç»­ç‚¹ä¼ è¾“â€çš„æœºåˆ¶çš„ï¼Œfilechannel å’Œå®ƒæ­£å¥½ç›¸åã€‚

å…³é”®å‚æ•°è®²è§£ï¼š

(1) capacity : å­˜å‚¨åœ¨channelä¸­çš„eventsçš„æœ€å¤§æ•°é‡

(2) transactionCapacity ï¼š æ¯æ¬¡æ•°æ®ç”±channelåˆ°sinkä¼ è¾“çš„æœ€å¤§eventsçš„æ•°é‡

(3) byteCapacity ï¼šè¯¥channelçš„å†…å­˜å¤§å°ï¼Œå•ä½æ˜¯ byte ã€‚


å…¶ä¸­transactionCapacityå…³é”®ä¸­æœ€å®¹æ˜“å¿½ç•¥çš„ï¼Œå› ä¸ºæ¯ä¸ªsinkçš„ç»ˆç«¯ä¸ä¸€æ ·ï¼Œæ‰¹å¤„ç†çš„æ•°é‡è¦ä¸¥æ ¼é™åˆ¶ã€‚è¿˜æœ‰ä¸€ç‚¹ï¼Œeventsçš„æ•°é‡å€¼å’Œchannelå¤§å°ä¸æ˜¯ä¸€å›äº‹ï¼Œä¸€ä¸ªeventåŒ…æ‹¬å•ä½æ•°æ®çš„å†…å®¹+å¤´æ•°æ®+æ•°æ®ä¼ è¾“çŠ¶æ€ã€‚å¯ä»¥è¯´ ï¼ˆeventsçš„æ•°é‡å€¼*å•ä½æ•°æ®æ‰€å å­—èŠ‚æ•°ï¼‰* 0.9 = æ‰€å ç©ºé—´å†…å­˜æ•°å€¼ï¼ˆå°±æ˜¯æƒ³è¯´æ˜transactionCapacity çš„å¤§å°å’ŒbyteCapacity ä¸èƒ½ç®€ç­”çš„æ•°å€¼æ¯”è¾ƒï¼‰ã€‚

## 3ã€sink
sinkç»„ä»¶çš„æ ¸å¿ƒå·¥ä½œæ˜¯æŠŠchannelä¸­æ•°æ®è¿›è¡Œè¾“å‡ºåˆ°ç‰¹å®šçš„ç»ˆç«¯ï¼Œæ¯”å¦‚hdfs,Hbase,databaseï¼Œavroç­‰ç­‰ã€‚

å› æ­¤è¿™å—çš„æ ¸å¿ƒä¼˜åŒ–å·¥ä½œåœ¨ä¼˜åŒ–å„ä¸ªç»ˆç«¯ï¼ˆhdfs,hbase,databaseï¼Œavroï¼‰çš„æ•°æ®æ’å…¥æ€§èƒ½ã€‚åœ¨è¿™é‡Œé¢æˆ‘åªä¼˜åŒ–è¿‡hbaseçš„æ•°æ®æ’å…¥æ€§èƒ½(å…·ä½“çš„åšæ³•å°±æ˜¯æ‰“å¼€flume hbasesinkæºç ï¼Œä¿®æ”¹ç„¶åæ‰“åŒ…)ï¼Œå½“ç„¶è¿™å—çš„å·¥ä½œä¸åœ¨flumeæœ¬èº«ï¼Œè¿™ä¹Ÿä¸æ˜¯flumeæ‰€èƒ½æ§åˆ¶çš„ã€‚
