---
title: 集群问题2
categories:
- 大数据离线
---
### 踩坑一
- 现象：sqoop导出数据到mysql时，到reduce卡住。
- 原因：在yarn服务器上查询运行日志后发现，mysql数据库中的字段是not null，但是导出时的数据是null，导致报错无法导出，然后卡在了reduce。
- 解决：mysql中字段设置可以为null。

### 踩坑二
- 现象：sqoop导出数据到mysql时，报错`Can't parse input data: '\N'`。
- 原因：导入时直接使用分区字段作为mysql中的对应字段，虽然查询时会在最后一列显示分区字段，但实际上hdfs文件上并没有该字段，导致字段数量对不上。
- 解决：需要在ads表中再手动添加日期字段。


### 踩坑三
- 现象：使用hive执行hql语句正常，但是使用beeline或datagrip执行复杂语句会报错
```
2021-06-17T17:50:15,988 ERROR [HiveServer2-Background-Pool: Thread-4814] operation.Operation: Error running hive query: 
org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
	at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380) ~[hive-service-2.3.6.jar:2.3.6]
	at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:257) ~[hive-service-2.3.6.jar:2.3.6]
	at org.apache.hive.service.cli.operation.SQLOperation.access$800(SQLOperation.java:91) ~[hive-service-2.3.6.jar:2.3.6]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:348) ~[hive-service-2.3.6.jar:2.3.6]
	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144]
	at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_144]
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657) ~[hadoop-common-2.7.2.jar:?]
	at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:362) ~[hive-service-2.3.6.jar:2.3.6]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_144]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_144]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]
	at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]
```
- 原因：
- 解决：


### 踩坑四
- 现象：使用datax导入到hdfs时报错没有权限
```
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=root, access=WRITE, inode="/user/hive/warehouse":hxr:supergroup:drwxr-xr-x
```
- 原因：因为系统用户是root，所以启动python datax.py [配置文件] 时上传的用户是root，而root没有修改权限，导致写入失败。
- 解决：
1. 在hdfs的配置文件中，将dfs.permissions.enabled修改为False
2. 执行这样的操作 hadoop fs -chmod 777 /user/hadoop
3. 修改执行时的登陆用户
4. 上传时指定有权限的用户 HADOOP_USER_NAME=hxr


### 踩坑五
- 现象：HiveServer2报错
```
Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread "TriggerValidator"
Exception in thread "org.apache.hadoop.hive.common.JvmPauseMonitor$Monitor@69feb4d9" java.lang.OutOfMemoryError: Java heap space
AsyncLogger error handling event seq=26839, value='[ERROR calling class org.apache.logging.log4j.core.async.RingBufferLogEvent.toString(): java.lang.NullPointerException]':
java.lang.OutOfMemoryError: Java heap space
Exception in thread "HiveServer2-Handler-Pool: Thread-4418" java.lang.OutOfMemoryError: Java heap space
```
- 原因：heapsize太小了
- 解决：修改hive-env.sh配置文件
```
if [ "$SERVICE" = "hiveserver2" ]; then
    echo $HADOOP_OPTS
    export HADOOP_OPTS="$HADOOP_OPTS -XX:PermSize=512M -XX:MaxPermSize=1024M"
    echo $HADOOP_OPTS
fi

export HADOOP_HEAPSIZE=4096
```

### 踩坑六
- 现象：使用Ranger的hive组件后报错
```
User: root is not allowed to impersonate root
```
- 原因：可能是root用户启动的Ranger Hive Plugin，该插件通过权限判断后最终提交任务时是以root用户提交的，所以hadoop需要允许root用户进行外部访问。
- 解决：core-site.xml中添加root代理
```
    <!-- 配置root允许通过代理访问主机节点 -->
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <!-- 配置root允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
```

### 踩坑七
- 现象：hive on spark 报错
```
Caused by: java.util.concurrent.ExecutionException: java.net.UnknownHostException: cos-bigdata-hadoop-01: Name or service not known
```
- 原因：可能是spark会读取hostname，但是hostname没有配置到DNS，所以无法解析。
- 解决：配置到hosts文件中
```
192.168.101.179 bigdata1 cos-bigdata-hadoop-01
192.168.101.180 bigdata2 cos-bigdata-hadoop-02
192.168.101.181 bigdata3 cos-bigdata-hadoop-03
```


### 踩坑八
- 现象：Ranger添加hive源操作时，一直卡在Please wait..，并报错
```
Exception [EclipseLink-4002] (Eclipse Persistence Services - 2.5.2.v20140319-9ad6abd): org.eclipse.persistence.exceptions.DatabaseException Internal Exception: com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction Error Code: 1205 Call: UPDATE x_service SET tag_service = ?, UPDATE_TIME = ?, version = ? WHERE ((id = ?) AND (version = ?)) bind => [5 parameters bound] Query: UpdateObjectQuery(XXService [id=3])
```
- 原因：事务执行速度慢，锁等待超时。可以查看表information_schema.innodb_trx、information_schema.innodb_locks、information_schema.innodb_lock_waits
查看事务和锁的情况。
- 解决：未解决


### 踩坑九
- 现象：load data inpath 导入数据后，select * 查询不到数据，但是select count(*) 查询到数据；
- 原因：因为文件是text格式的，没有进行压缩，但是表是lzo压缩格式。所以hive表查询不到数据，但是select count(*) 是通过mapred或spark直接对文件进行计算，所以可以得到计算结果。
- 解决：使用insert into ... select ... from 进行导入；或对文件使用正确的压缩格式。

**这里就需要提及如下配置了：**
```
STORED AS
    INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat'
    OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
```
INPUTFORMAT 是读取该表时使用的格式，而OUTPUTFORMAT 是导入该表时使用的格式；需要注意的是，load data inpath 导入时不会使用OUTPUTFORMAT 的配置，只有insert into ... select ... 导入时会使用OUTPUTFORMAT 配置。
所以该表保存的数据是lzo格式的，load data inpath 导入数据的源文件的格式应当是lzo格式，这样导入数据后才能被hive正确的读取。OUTPUTFORMAT 一般不使用，因为这种建表语句创建的表都是通过直接添加文件而不是insert into ... select ... 来实现数据的插入的。
对于flume来说需要设置输出的文件压缩格式为lzo；对于datax来说，不支持lzo格式，需要想其他办法。

### 踩坑十
- 现象：sqoop执行命令后卡死在reduce 0%
- 原因：①可能是mysql中不允许null值，而导入数据中存在null值，导致卡死。②类型不匹配 ③资源不足
- 解决：需要手动杀死任务 yarn application -kill application_1628043648968_0137，然后解决问题后再执行。


### 踩坑十一：
- 现象：hdfs-site.xml中配置硬盘挂载路径，但是启动hdfs时，报错Too many failed volumes。NameNode启动正常，可以查询原数据，但是DataNode启动失败，无法查看文件内容。
```
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file://${hadoop.tmp.dir}/dfs/data,file:///dfs/data1,file:///home/dfs/data2</value>
    </property>
```
- 原因：hxr用户没有权限访问root用户的硬盘。将目录权限设置为777，并将存储目录用户改为启动hadoop的用户。
```
2021-08-31 16:12:35,884 WARN org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker: Exception checking StorageLocation [DISK]file:/dev/mapper/centos_cos-root
EPERM: Operation not permitted
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmodImpl(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.chmod(NativeIO.java:233)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:836)
	at org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:508)
	at org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:489)
	at org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:511)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:234)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:239)
	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:52)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
- 另一个问题：如果挂载新盘时没有挂载原有的路径，那么启动时就会发现，原有数据会丢失；这时再挂载原有路径时，会报错`InconsistentFSStateException: Directory /home/dfs/data2 is in an inconsistent state`
- 解决：启动时会在新盘下创建新的数据文件，需要将这些文件全部清空后再启动即可。

### 踩坑十二
- 现象：在hive执行过程中出现错误如下，这个错误在dt='2021-09-01'时正常，在dt='2021-08-31'时出现
```
2021-09-02 13:27:45,360 ERROR scheduler.AsyncEventQueue: Listener ClientListener threw an exception
java.lang.NullPointerException
	at org.apache.hive.spark.client.RemoteDriver$ClientListener.onJobEnd(RemoteDriver.java:491)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:115)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:99)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
......
Job failed with java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper cannot be cast to org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerDirectAccess
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Spark job failed during runtime. Please check stacktrace for the root cause.
```
- 原因：hive-on-spark模式下提交的任务中，LEFT JOIN语句右表是张空表。
- 理解：Join操作时，hive-on-spark会将小表转化为hashTable存储到内存中，然后匹配大表，当小表为空表时，hashTable为null，读取hashTable时就会报错NullPointerException。
- 解决：将hashtable关闭即可 `set hive.mapjoin.optimized.hashtable=false;`


### 踩坑十三
- 现象：当使用hive-on-spark时，可能出现的现象①在子查询中使用datediff函数时出现空指针异常；②开窗函数中对日期进行排序，指定行不生效，默认为第一行到当前行；
- 原因：日期使用string进行保存，而不是使用date类型进行保存；
- 解决：使用date保存日期函数，或者将string类型转换为date类型；

### 踩坑十四
- 现象：正确的hql语句，使用hive-on-spark启动时却报错
```
[42000][3] Error while processing statement: FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Spark job failed during runtime. Please check stacktrace for the root cause.
```
- 原因：查看yarn日志，发现报错如下
```
Driver stacktrace:
Exception in thread "task-result-getter-146" java.lang.NoClassDefFoundError: org/antlr/runtime/tree/CommonTree
......
Caused by: java.lang.ClassNotFoundException: org.antlr.runtime.tree.CommonTree
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
```
说明缺少jar包。
- 解决：对于缺包的问题，当然就是找到对应的jar包，其实org/antlr/runtime/tree/CommonTree所在的jar在hive的lib库中包含。就是hive的lib下的antlr-runtime-3.5.2.jar。
可以通过命令进行查找该jar包
```
find / -name "antlr-runtime-*"
```
将hive的lib库下的antlr-runtime-3.5.2.jar拷贝到hdfs上的spark的依赖包下
```
hadoop fs -put antlr-runtime-3.5.2.jar /spark/jars-3.0.0
```
杀死已经存在的spark application，然后重新执行任务
```
yarn application -kill application_1628043648968_xxxx
```

### 踩坑十五
- 现象：
```
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Column vector class org.apache.hadoop.hive.ql.exec.vector.VoidColumnVector is not supported!
```
或
```
Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.LongColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector
```
或
```
DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.D
```
- 原因：矢量计算的技术，在计算类似scan, filter, aggregation的时候，vectorization技术以设置批处理的增量大小为 1024 行单次来达到比单条记录单次获得更高的效率。但是vector强转报错，具体为什么会报错还不得而知。
- 解决：不使用向量化执行可以解决vector强转报错的问题，set hive.vectorized.execution.enabled=false;

报错的hql如下
```
         SELECT 2                                                     as type,
                code                                                  as device_code,
                0                                                     as time_type,
                date_format('2021-09-10', 'yyyy')                     as `time`,
                nvl(tmp_sales.sales_num, 0)                           as sales_num,
--        nvl(tmp_sales.sales_money, 0)     as sales_money,
                nvl(cast(tmp_sales.sales_money as decimal(20, 2)), 0) as sales_money,
                nvl(tmp_sales.send_num, 0)                            as send_num,
                null                                                  as rate,
                current_timestamp                                     as create_time,
                current_timestamp                                     as update_time
         FROM (
                  SELECT code
                  FROM finedb_ads.ba_device_type
                  WHERE dt = '2021-09-10'
              ) tmp_device_type
                  LEFT JOIN
              (
                  SELECT device_code,
                         sum(sales_num)   as sales_num,
                         sum(sales_money) as sales_money,
                         sum(send_num)    as send_num
                  FROM finedb_ads.ca_sales
                  WHERE year(sales_date) = year('2021-09-10')
                    and sales_date <= '2021-09-10'
                    and type = 2
                    and dt = '2021-09-10'
                  GROUP BY device_code
              ) tmp_sales
              ON tmp_device_type.code = tmp_sales.device_code;
```




### 踩坑十七
- 现象：hiveserver2拒绝连接，10000端口关闭，报错如下
```
Exception in thread "HiveServer2-Handler-Pool: Thread-308" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:57)
        ......
```
- 原因：hiveserver2的进程内存溢出。
   （1）主要是由于jdbc连接hiveserver2的并发量非常高，而且某些sql任务会严重消耗hiveserver2的内存，导致hiveserver2压力巨大，比如：reducetask获取mapOutputStatus的时候。这种情况是由于hiveserver2自身的复杂压力大，内存损耗严重，严重GC进而导致hiveserver2故障。这种故障对应于上面介绍的“故障现象1”，通过jdbc无法正常连接到hiveserver2。为了解决该故障，可以通过优化内存GC可以缓解hiveserver2的GC卡死问题。
   （2）由于执行某个大型sql任务，把资源池资源消耗殆尽，且长时间不释放，导致所有通过hiveserver2提交的sql任务都无法执行。这种故障对应于上面介绍的“故障现象2”，能够很顺利通过jdbc连接到hiveserver2，但是无法执行任何sql任务。为了解决此类故障，只能让开发者减少复杂任务执行，或者将不同类型的任务，提交到不同的资源队列执行。
- 解决：修改bin/hive-config.sh中的HADOOP_HEAPSIZE参数，

[Hiveserver2 性能优化与GC优化_mnasd的博客-CSDN博客](https://blog.csdn.net/mnasd/article/details/82690414)
```
-Xms49152m 
-Xmx49152m 
-Djava.rmi.server.hostname=xxx.xxx.xxx.xxx 
-Dcom.sun.management.jmxremote.port=8999 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.ssl=false 
-XX:+UseParNewGC 
-XX:ParallelGCThreads=30 
-XX:MaxTenuringThreshold=10 
-XX:TargetSurvivorRatio=70 
-XX:+UseConcMarkSweepGC 
-XX:+CMSConcurrentMTEnabled 
-XX:ParallelCMSThreads=30 
-XX:+UseCMSInitiatingOccupancyOnly 
-XX:+CMSClassUnloadingEnabled 
-XX:+DisableExplicitGC 
-XX:CMSInitiatingOccupancyFraction=70 
-XX:+UseCMSCompactAtFullCollection 
-XX:CMSFullGCsBeforeCompaction=1 
-verbose:gc 
-XX:+PrintGCDetails 
-XX:+PrintGCDateStamps 
-XX:GCLogFileSize=512M 
-Xloggc:/data/log/tbds/spark/gc-sparkthrift.log-${timenow}
```
- -Xms49152m -Xmx49152m 这两个参数需要考虑服务器实际的可用内存资源来设定；
- -XX:ParallelGCThreads=30 和 -XX:ParallelCMSThreads=30 这两个参数需要考虑服务器实际的CPU核数来决定。切记不要超过CPU核数。


### 踩坑十八
- 现象：在azkaban任务调度中，使用useExecutor指定192.168.101.176的节点执行的executor执行任务。直接killing，报错如下
```
Failed to dispatch queued execution basic because reached azkaban.maxDispatchingErrors (tried 4 executors)
azkaban.executor.ExecutorManagerException: azkaban.executor.ExecutorManagerException: cos-bigdata-datax-01 at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1068) at azkaban.executor.ExecutorManager.access$500(ExecutorManager.java:69) at azkaban.executor.ExecutorManager$QueueProcessorThread.selectExecutorAndDispatchFlow(ExecutorManager.java:1238) at azkaban.executor.ExecutorManager$QueueProcessorThread.processQueuedFlows(ExecutorManager.java:1210) at azkaban.executor.ExecutorManager$QueueProcessorThread.run(ExecutorManager.java:1148) Caused by: azkaban.executor.ExecutorManagerException: cos-bigdata-datax-01 at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:80) at azkaban.executor.ExecutorApiGateway.callWithExecutable(ExecutorApiGateway.java:43) at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1062) ... 4 more Caused by: java.net.UnknownHostException: cos-bigdata-datax-01 at java.net.InetAddress.getAllByName0(InetAddress.java:1280) at java.net.InetAddress.getAllByName(InetAddress.java:1192) at java.net.InetAddress.getAllByName(InetAddress.java:1126) 
```
- 原因：executor通过将信息保存到mysql中，供web-server进行调用。保存的节点信息为主机名，如果主机名无法解析，就会报错UnknownHostException。
- 解决：将所有executor的主机名添加到/etc/hosts文件中，这样就可以解析主机名找到对应的ip地址。此外，还需要关闭executor节点的firewalld。
```
192.168.101.179 cos-bigdata-hadoop-01
```

### 踩坑十九
- 现象：创建维度表，使用了lzo压缩并且创建了索引。但是在对维度表进行join时，会发现结果多了一条null数据，并且ods表中丢失了原有的lzo索引，可能导致下次读取时无法切片的问题。
- 原因：hive在join时，会误把lzo索引当成小文件进行合并，并插入一条。
select * from ods_log不执行MR操作，直接采用的是ods_log建表语句中指定的DeprecatedLzoTextInputFormat，能够识别lzo.index为索引文件。
select count(*) from ods_log执行MR操作，会先经过hive.input.format，其默认值为CombineHiveInputFormat，其会先将索引文件当成小文件合并，将其当做普通文件处理。
- 解决：在对ods表进行降维等操作时，直接关闭hive自动合并小文件即可 `set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat`

### 踩坑二十
- 现象：不管是mr引擎还是spark引擎都会报错，但是mr能继续运行，spark会一直处于accepted。
```
No Stats for hxr_u8@ods_inventoryclass, Columns: cinvcname, iinvcgrade, cinvccode
No Stats for hxr_u8@ods_inventory, Columns: cinvdefine1, cinvdefine2, cinvcode, cinvstd, cinvccode
No Stats for hxr_u8@ods_dispatchlist, Columns: ddate, dlid
No Stats for hxr_u8@ods_dispatchlists, Columns: dlid, cinvcode
```
- 原因：
- 解决：set hive.compute.query.using.stats=false;


### 踩坑二十一
- 现象：使用hive-on-spark时，如果有多个hiveserver2连接，那么在队列中会维持多个spark-session。此时再提交任务，任务会超时且该任务一直处于Accepted阶段等待运行。
- 原因：该队列最大可使用的资源容量大小百分比过小。
```
Maximum percent of resources in the cluster which can be used to run application masters - controls number of concurrent active applications. Limits on each queue are directly proportional to their queue capacities and user limits. Specified as a float - ie 0.5 = 50%. Default is 10%. This can be set for all queues with yarn.scheduler.capacity.maximum-am-resource-percent and can also be overridden on a per queue basis by setting yarn.scheduler.capacity.<queue-path>.maximum-am-resource-percent
```
- 解决：yarn.scheduler.capacity..maximum-capacity 的值调大为0.5。目前来看还是稳定的，有待观察。

### 踩坑二十二
- 现象：还是上面的问题，提交任务会处于Accepted导致任务失败。尝试启动spark-shell，报错 [BindException: 地址已在使用]；重启hiveserver2，也报错 [BindException: 地址已在使用]；随后重启yarn，nodemanager进程同样报错 [BindException: 地址已在使用]。
```
2021-11-11 10:12:59,300 WARN util.Utils: Service 'SparkUI' could not bind on a random free port. You may check whether configuring an appropriate binding address.
2021-11-11 10:12:59,314 ERROR ui.SparkUI: Failed to bind SparkUI
java.net.BindException: Failed to bind to /0.0.0.0:0: Service 'SparkUI' failed after 16 retries (on a random free port)! Consider explicitly setting the appropriate binding address for the service 'SparkUI' (for example spark.driver.bindAddress for SparkDriver) to the correct binding address.
```
- 原因：查看监控，发现节点上有一个盘满了
![7262acd00c6d061bf90c9ae33ad5d02.png](集群问题2.assets\45474a8c12ad4b7cab373a071a31d08d.png)
- 解决：使用 hdfs diskbalancer 命令，进性数据平衡。

### 踩坑二十三
- 现象：部署了Atlas框架，使用import_hive.sh脚本将hive的元数据倒入到Atlas是OK的，但是运行Azkaban希望通过hive的hook同步血缘时无法同步。
- 原因：因为Atlas部署的节点和Azkaban指定运行任务的节点不是同一个；Azkaban运行的节点上的hive并没有配置hook和Atlas，导致Atlas无法同步血缘。
- 解决：修改 [Azkaban指定的运行任务的节点] 为 [配置了hive-hook和Atlas的节点] ;


### 踩坑二十四
- 现象：通过Datax向需要Kerberos认证的hdfs中导入数据，会报错认证失败，无法导入。
- 原因：认证失败的原因是，启动azkaban时使用的jdk包中的Security目录下的两个文件US_export_policy.jar和local_policy.jar没有替换。
- 解决：
1、jce下载地址：https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html
2、将解压得到的local_policy.jar和US_export_policy.jar拷贝到$JAVA_HOME/jre/lib/security目录下面
再次启动Datax任务，但是还是会报错
```
Caused by: java.io.IOException: Server asks us to fall back to SIMPLE auth, but this client is configured to only allow secure connections.
```
原因是因为粗心，没有将Datax导入到新搭建的配置了Kerberos的hdfs集群。导致Datax使用了Kerberos而hdfs没有使用Kerberos认证，报了上述的错误。

### 踩坑二十五
- 现象：书接上回，解决了Kerberos认证问题后，导入数据又出现了新的问题
```
There are 3 datanode(s) running and 3 node(s) are excluded in this operation
```
- 原因：猜测被selinux或防火墙阻拦
- 解决：先使用命令`setenforce 0`临时关闭selinux，然后修改配置文件/etc/selinux/config或/etc/sysconfig/selinux，设置SELINUX=disabled。还是未能解决，查看datanode日志：
```
2021-11-17 09:29:20,207 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /192.168.101.177:39434. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c50d9 instead of deadbeef from client.
```
应该是hadoop版本不一致导致的，datax当前版本Hadoop版本为2.7.1，而目标Hadoop版本为3.1.3，因此尝试下载Datax源码，将对应的hadoop版本改为3.1.3，hive版本改为兼容的2.3.6，然后重新编译打包并部署
```
$ git clone https://github.com/alibaba/DataX.git
# 修改hadoop和hive本版后，在根目录下运行maven进行打包
$ mvn -U clean package assembly:assembly -Dmaven.test.skip=true
```
打包成功后的DataX包位于 target 目录下，解压后使用，发现还是有问题
```
There are 3 datanode(s) running and 3 node(s) are excluded in this operation
```
```
2021-11-17 15:58:26,109 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Failed to read expected SASL data transfer protection handshake from client at /192.168.101.177:41208. Perhaps the client is running an older version of Hadoop which does not support SASL data transfer protection
org.apache.hadoop.hdfs.protocol.datatransfer.sasl.InvalidMagicNumberException: Received 1c5086 instead of deadbeef from client.
```
Kerberos验证通过且版本也对应了，那么这个错误的具体原因是啥？？？
- 真正的原因：最终发现是配置问题，在服务器端配置了dfs.data.transfer.protection为authentication，而客户端没有配置，只需要在Datax源码中进行相应配置即可。
- 真正的解决：在任务脚本的hadoopConfig参数中配置`"dfs.data.transfer.protection": "authentication"`
也可以在源码中添加配置`hadoopConf.set("dfs.data.transfer.protection", "authentication");`；或在hdfs-site.xml配置文件中添加配置
```
    <property>
      <name>dfs.data.transfer.protection</name>
      <value>authentication</value>
    </property>
```
然后进行Datax源码重新编译打包部署即可。
>官网解释：A comma-separated list of SASL protection values used for secured connections to the DataNode when reading or writing block data. Possible values are authentication, integrity and privacy. authentication means authentication only and no integrity or privacy; integrity implies authentication and integrity are enabled; and privacy implies all of authentication, integrity and privacy are enabled. If dfs.encrypt.data.transfer is set to true, then it supersedes the setting for dfs.data.transfer.protection and enforces that all connections must use a specialized encrypted SASL handshake. This property is ignored for connections to a DataNode listening on a privileged port. In this case, it is assumed that the use of a privileged port establishes sufficient trust.


### 踩坑二十六
- 现象：解决上述问题后，迎来最后一个问题如下
```
19-11-2021 15:18:08 CST so_sodetails INFO - com.alibaba.datax.common.exception.DataXException: Code:[Framework-03], Description:[DataX引擎配置错误，该问题通常是由于DataX安装错误引起，请联系您的运维解决 .].  - 在有总bps限速条件下，单个channel的bps值不能为空，也不能为非正数
```
- 原因：
- 解决：在Datax脚本中添加配置
```
{"core": {
    "transport": {
        "channel": {
            "speed": {
                "byte": 1048576
            }
        }
    }
 },
 "job":{
        ......
    }
}
```

### 踩坑二十七
- 现象：Kerberos问题解决后，为了使Datax支持Lzop压缩，引入了依赖
```
        <dependency>
            <groupId>org.anarres.lzo</groupId>
            <artifactId>lzo-hadoop</artifactId>
            <version>1.0.5</version>
        </dependency>
```
结果启动后报错
```
No FileSystem for scheme:file
```
- 原因：因为lzo-hadoop包中引入了hadoop-core依赖，该依赖带有hdfs-site.xml配置文件，导致我们的配置都被其覆盖了
- 解决：将lzo-hadoop中的依赖排除掉
```
        <dependency>
            <groupId>org.anarres.lzo</groupId>
            <artifactId>lzo-hadoop</artifactId>
            <version>1.0.5</version>
            <exclusions>
                <exclusion>
                    <groupId>org.apache.hadoop</groupId>
                    <artifactId>hadoop-core</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
```

### 踩坑二十八
- 现象：动态分区插入数据时
```
INSERT OVERWRITE TABLE compass_dwd.dwd_edb_order_detail PARTITION (ds)
SELECT ...... ;
```
出现错误（spark引擎）
```
[08S01][1] Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Exception when loading 30 in table dwd_edb_order_detail with loadPath=hdfs://cos-bigdata-test-hadoop-01:98202021-12-05T03:29:59,626  WARN [load-dynamic-partitions-0] net.NetUtils: Unable to wrap exception of type class java.nio.channels.ClosedByInterruptException: it has no (String) constructor
java.lang.NoSuchMethodException: java.nio.channels.ClosedByInterruptException.<init>(java.lang.String)

2021-12-05T03:29:59,627 ERROR [load-dynamic-partitions-0] metadata.Hive: Exception when loading partition with parameters  partPath=hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-28-58_216_622270070896267164-1/-ext-10000/ds=2021-11-25,  table=dwd_edb_order_detail,  partSpec={ds=2021-11-25},  loadFileType=REPLACE_ALL,  listBucketingLevel=0,  isAcid=false,  hasFollowingStatsTask=true
org.apache.hadoop.hive.ql.metadata.HiveException: Getting globStatus hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-28-58_216_622270070896267164-1/-ext-10000/ds=2021-11-25

Caused by: java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException; Host Details : local host is: "cos-bigdata-test-hadoop-01/192.168.101.184"; destination host is: "cos-bigdata-test-hadoop-01":9820; 
```
即使换成mr引擎，还是报错
```
2021-12-05T03:55:59,044 ERROR [load-dynamic-partitions-2] metadata.Hive: Exception when loading partition with parameters  partPath=hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-55-18_450_3496090080792646897-97/-ext-10000/ds=2021-11-16,  table=dwd_edb_order_detail,  partSpec={ds=2021-11-16},  loadFileType=REPLACE_ALL,  listBucketingLevel=0,  isAcid=false,  hasFollowingStatsTask=true
org.apache.hadoop.hive.ql.metadata.HiveException: Getting globStatus hdfs://cos-bigdata-test-hadoop-01:9820/warehouse/compass/compass_dwd.db/dwd_edb_order_detail/.hive-staging_hive_2021-12-05_03-55-18_450_3496090080792646897-97/-ext-10000/ds=2021-11-16

Caused by: java.io.InterruptedIOException: Interrupted waiting to send RPC request to server

Caused by: java.lang.InterruptedException
```
- 原因：有分区存在但是该分区的文件夹不存在，导致overwrite时找不到需要清空的分区文件夹。多分区插入时报错信息会很奇怪。
- 解决：删除分区或新建分区文件夹

### 踩坑二十九
- 现象: 问题①使用mr执行历史数据初始化时
```
2021-12-09 20:29:56,924 ERROR [IPC Server handler 0 on default port 41316] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1638981807800_0026_m_000000_0 - exited : GC overhead limit exceeded
2021-12-09 20:29:56,924 INFO [IPC Server handler 0 on default port 41316] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Diagnostics report from attempt_1638981807800_0026_m_000000_0: Error: GC overhead limit exceeded
```
②改用hive-on-spark执行初始化任务时报错内存溢出
```
2021-12-08 19:23:56,625 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
2021-12-08 19:23:56,625 INFO storage.BlockManagerMaster: Removal of executor 1 requested
2021-12-08 19:23:56,627 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 1
2021-12-08 19:23:59,607 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead)
2021-12-08 19:23:59,608 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
2021-12-08 19:23:59,612 INFO yarn.YarnAllocator: Completed container container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02 (state: COMPLETE, exit status: 143)
2021-12-08 19:23:59,612 WARN yarn.YarnAllocator: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143
[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. 
[2021-12-08 19:23:57.659]Killed by external signal
.
2021-12-08 19:23:59,613 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143
[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. 
[2021-12-08 19:23:57.659]Killed by external signal
.
2021-12-08 19:23:59,613 ERROR cluster.YarnClusterScheduler: Lost executor 2 on cos-bigdata-test-hadoop-02: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143
[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. 
[2021-12-08 19:23:57.659]Killed by external signal
.
2021-12-08 19:23:59,614 WARN scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, cos-bigdata-test-hadoop-02, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1637563725346_0176_01_000003 on host: cos-bigdata-test-hadoop-02. Exit status: 143. Diagnostics: [2021-12-08 19:23:57.656]Container killed on request. Exit code is 143
[2021-12-08 19:23:57.656]Container exited with a non-zero exit code 143. 
[2021-12-08 19:23:57.659]Killed by external signal
.
2021-12-08 19:23:59,615 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.
2021-12-08 19:23:59,615 INFO storage.BlockManagerMaster: Removal of executor 2 requested
2021-12-08 19:23:59,616 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asked to remove non-existent executor 2
2021-12-08 19:24:02,613 INFO yarn.YarnAllocator: Will request 1 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead)
2021-12-08 19:24:02,614 INFO yarn.YarnAllocator: Submitted 1 unlocalized container requests.
2021-12-08 19:24:02,618 INFO yarn.YarnAllocator: Launching container container_1637563725346_0176_01_000004 on host cos-bigdata-test-hadoop-02 for executor with ID 3
2021-12-08 19:24:02,619 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them.
2021-12-08 19:24:05,628 INFO yarn.YarnAllocator: Launching container container_1637563725346_0176_01_000005 on host cos-bigdata-test-hadoop-02 for executor with ID 4
2021-12-08 19:24:05,629 INFO yarn.YarnAllocator: Received 2 containers from YARN, launching executors on 1 of them.
2021-12-08 19:24:06,129 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.101.185:55294) with ID 3
2021-12-08 19:24:06,272 INFO storage.BlockManagerMasterEndpoint: Registering block manager cos-bigdata-test-hadoop-02:44266 with 366.3 MiB RAM, BlockManagerId(3, cos-bigdata-test-hadoop-02, 44266, None)
2021-12-08 19:24:06,318 INFO scheduler.TaskSetManager: Starting task 1.1 in stage 0.0 (TID 2, cos-bigdata-test-hadoop-02, executor 3, partition 1, RACK_LOCAL, 8748 bytes)
2021-12-08 19:24:08,434 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on cos-bigdata-test-hadoop-02:44266 (size: 94.5 KiB, free: 366.2 MiB)
2021-12-08 19:24:08,636 INFO yarn.YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
2021-12-08 19:24:09,170 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.101.185:55482) with ID 4
2021-12-08 19:24:09,294 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on cos-bigdata-test-hadoop-02:44266 (size: 90.8 KiB, free: 366.1 MiB)
2021-12-08 19:24:09,329 INFO storage.BlockManagerMasterEndpoint: Registering block manager cos-bigdata-test-hadoop-02:32770 with 366.3 MiB RAM, BlockManagerId(4, cos-bigdata-test-hadoop-02, 32770, None)
2021-12-08 19:24:09,382 INFO scheduler.TaskSetManager: Starting task 0.1 in stage 0.0 (TID 3, cos-bigdata-test-hadoop-02, executor 4, partition 0, RACK_LOCAL, 14062 bytes)
2021-12-08 19:24:11,430 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on cos-bigdata-test-hadoop-02:32770 (size: 94.5 KiB, free: 366.2 MiB)
2021-12-08 19:24:12,304 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on cos-bigdata-test-hadoop-02:32770 (size: 90.8 KiB, free: 366.1 MiB)
2021-12-08 19:24:21,157 WARN scheduler.TaskSetManager: Lost task 1.1 in stage 0.0 (TID 2, cos-bigdata-test-hadoop-02, executor 3): java.lang.OutOfMemoryError: Java heap space
	at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.<init>(BytesColumnVector.java:86)
```
- 原因: ①内存溢出，已经设置的配置如下：
**yarn-site.xml**
```
    <!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>10240</value>
    </property>
    <!-- resourcemanager可以分配给容器的核数 -->
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>16</value>
    </property>
    <!-- 虚拟核数和物理核数乘数 -->
    <property>
        <name>yarn.nodemanager.resource.pcores-vcores-multiplier</name>
        <value>1</value>
    </property>

    <!-- 该节点上YARN可使用的物理内存总量 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>12288</value>
    </property>

    <!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
```
**capacity-scheduler**
```
  <property>
    <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
    <value>1</value>
    <description>
      Maximum percent of resources in the cluster which can be used to run
      application masters i.e. controls number of concurrent running
      applications.
    </description>
  </property>
```
但是没有配置map和reduce可使用的内存配置，默认时1024。
- 解决: 在mapred-site.xml中配置map和reduce的内存
```
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>8192</value>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>8192</value>
    </property>

    <property>  
       <name>mapred.child.java.opts</name>  
       <value>-Xmx8192m</value>  
    </property>  
    <property>  
```
然后执行mr任务，发现可以成功执行了，但是spark任务还是失败报错。

`一般情况下，无法开始正常的MR任务，无法显示启动了多少个map reduce，则可能是因为启动资源过大造成的内存溢出，这时候就要看表原始的数据，数据量是不是有问题；如果是map和reduce执行了一段时间显示内存溢出的问题，很有可能是脚本设计不合理或者原始数据倾斜，则需要通过set参数来进行调整，最常见的就是group by，或者map，reduce处理的数据量不均匀导致。`如下例子
```
SELECT *
FROM (
         SELECT line, item, row_number() over (partition by get_json_object(line, '$.tid')) row_num
         FROM (
                  SELECT line
                  FROM ods_edb.ods_edb_order
                  where ds = '$do_date'
                    and get_json_object(line, '$.tid_item') is not null
                    and get_json_object(line, '$.tid') is not null
                    and get_json_object(line, '$.tid') <> ''
              ) tmp_edb_order
                  lateral view compass_dwd.explode_json_array(get_json_object(tmp_edb_order.line, '$.tid_item')) table_tmp as item
              -- 排除套餐、套餐内单品的子订单，只剩单品子订单(必须保证套餐一定会拆分，且销量根据拆分后计算)
         where get_json_object(item, '$.iscombination') = 0
     ) tmp
where row_num = 1;
```
在脚本中多次对全量数据的json串进行解析，导致内存溢出。优化方式就是，在临时表中对全量数据一次性进行解析，然后直接读取需要的字段，避免多次进行解析。


### 踩坑三十
- 现象：使用mr引擎执行hive初始化历史数据任务时出现错误
```
2021-12-10 15:12:04,451 WARN [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Task attempt attempt_1639119531715_0006_r_000000_0 is done from TaskUmbilicalProtocol's point of view. java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)
2021-12-10 15:42:40,895 INFO [eventHandlingThread] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Unable to write out JobSummaryInfo to [hdfs://cos-bigdata-test-hadoop-01:9820/tmp/hadoop-yarn/staging/history/done_intermediate/chenjie/job_1639119531715_0018.summary_tmp]
java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread
	at org.apache.hadoop.hdfs.ExceptionLastSeen.set(ExceptionLastSeen.java:45)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:829)
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)
2021-12-10 15:42:40,897 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.
org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:732)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:383)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread
	at org.apache.hadoop.hdfs.ExceptionLastSeen.set(ExceptionLastSeen.java:45)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:829)
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:717)
	at org.apache.hadoop.hdfs.DataStreamer.initDataStreaming(DataStreamer.java:633)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:717)
```
- 原因：①可能是没有设置系统参数；②可能是动态分区插入数据时导致 内存不足。
- 解决：针对原因1，修改/etc/security/limits.conf 		
```
* soft nofile 65536	
* hard nofile 131072		
* soft nproc 2048	
* hard nproc 65536
```
发现还是报错。
针对原因2，查看SQL脚本是否有误导致分区过多，最后发现是动态分区的ds时间不是yyyy-MM-dd格式的，导致group by和最终插入的分区实在太多了，才使得内存溢出。

### 踩坑三十一
- 现象：执行历史任务时，报错如下
```
14-12-2021 13:58:11 CST dws_total_model_init INFO - Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unexpected exception from MapJoinOperator : Unexpected exception from MapJoinOperator : org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.parquet.hadoop.MemoryManager$1: New Memory allocation 1048216 bytes is smaller than the minimum allocation size of 1048576 bytes.
```
- 原因：未知
- 解决：增加内存


### 踩坑三十二
- 现象：使用mr引擎正常运行，但是使用spark引擎执行任务时报错
```
java.lang.AssertionError:Cannot add expression of different type to set: set type is RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f1, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f2, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f3, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f4, INTEGER $f6, INTEGER $f7, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f8, BIGINT $f9) NOT NULL expression type is RecordType(VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f1, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f2, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f3, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en_US$primary" $f4, INTEGER $f6, VARCHAR(2147483647) CHARACTER SET "UTF-16LE" COLLATE "ISO-8859-1$en ...
```
- 原因：未知。查询资料，有人说这是calcite的bug。
- 解决：关闭CBO优化器 `set hive.cbo.enable=false`

### 踩坑三十三
- 现象：mr引擎执行脚本时报错如下
```
2021-12-28 10:35:12,632 ERROR [IPC Server handler 8 on default port 36440] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1639119531715_1552_m_000004_0 - exited : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:163)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapRunner.run(ExecMapRunner.java:37)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:465)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:349)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row 
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:973)
	at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:154)
	... 9 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: 
DeserializeRead detail: Reading byte[] of length 30912 at start offset 0 for length 14791 to read 10 fields with types [string, string, string, string, string, string, string, string, string, array<string>].  Read field #9 at field start position 0 current read offset 13539
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:928)
	... 10 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 1024
	at org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.setVal(BytesColumnVector.java:187)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storePrimitiveRowColumn(VectorDeserializeRow.java:588)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeComplexFieldRowColumn(VectorDeserializeRow.java:778)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeListRowColumn(VectorDeserializeRow.java:822)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.storeRowColumn(VectorDeserializeRow.java:938)
	at org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.deserialize(VectorDeserializeRow.java:1360)
	at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.process(VectorMapOperator.java:923)
	... 10 more
```
- 原因：未知
- 解决：禁用hive矢量执行
`set hive.vectorized.execution.enabled=false;`
`set hive.vectorized.execution.reduce.enabled=false;`
`set hive.vectorized.execution.reduce.groupby.enabled=false;`


### 踩坑三十四
- 问题：mapred的中间任务启动的mappers数量过大，导致整个任务慢
```
31-12-2021 20:36:20 CST dwd_edb_order_info_init INFO - Stage-5 is selected by condition resolver.
31-12-2021 20:36:23 CST dwd_edb_order_info_init INFO - Starting Job = job_1640834494731_0089, Tracking URL = [http://cos-bigdata-test-hadoop-02:8088/proxy/application_1640834494731_0089/](http://cos-bigdata-test-hadoop-02:8088/proxy/application_1640834494731_0089/)
31-12-2021 20:36:23 CST dwd_edb_order_info_init INFO - Kill Command = /opt/module/hadoop-3.1.3/bin/mapred job  -kill job_1640834494731_0089
31-12-2021 20:36:31 CST dwd_edb_order_info_init INFO - Hadoop job information for Stage-5: number of mappers: 930; number of reducers: 0
31-12-2021 20:36:31 CST dwd_edb_order_info_init INFO - 2021-12-31 20:36:31,852 Stage-5 map = 0%,  reduce = 0%</pre>
```
![image.png](集群问题2.assets\1f2bb214e38745ec818af628921bea03.png)

- 原因：

### 踩坑三十五
- 问题：azkaban重启之后，有时会发生找不到executor的情况
```
Failed to dispatch queued execution hxr_compass_hive because reached azkaban.maxDispatchingErrors (tried 1 executors)
azkaban.executor.ExecutorManagerException: azkaban.executor.ExecutorManagerException: java.nio.file.FileSystemException: executions/1446/flow20.project -> /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1068) at azkaban.executor.ExecutorManager.access$500(ExecutorManager.java:69) at azkaban.executor.ExecutorManager$QueueProcessorThread.selectExecutorAndDispatchFlow(ExecutorManager.java:1238) at azkaban.executor.ExecutorManager$QueueProcessorThread.processQueuedFlows(ExecutorManager.java:1210) at azkaban.executor.ExecutorManager$QueueProcessorThread.run(ExecutorManager.java:1148) Caused by: azkaban.executor.ExecutorManagerException: java.nio.file.FileSystemException: executions/1446/flow20.project -> /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:80) at azkaban.executor.ExecutorApiGateway.callWithExecutable(ExecutorApiGateway.java:43) at azkaban.executor.ExecutorManager.dispatch(ExecutorManager.java:1062) ... 4 more Caused by: java.io.IOException: java.nio.file.FileSystemException: executions/1446/flow20.project -> /opt/module/azkaban-3.84.4/exec/projects/18.8/flow20.project: ?????? at azkaban.executor.ExecutorApiGateway.callForJsonObjectMap(ExecutorApiGateway.java:108) at azkaban.executor.ExecutorApiGateway.callWithExecutionId(ExecutorApiGateway.java:78) ... 6 more
```
- 原因：发现是有一个executor异常导致分配到该executor时出错。根据异常信息查看该project文件夹下的18.8文件夹，怀疑是权限问题，上次接收任务文件时是root用户，现在是hxr用户运行的executor程序，导致无法修改该文件而报错。
- 解决：尝试将所有文件修改为启动executor的用户，效果待观察。。。

### 踩坑三十六
- 问题：多次连接Mysql失败后报错如下
```
java.sql.SQLException: null,  message from server: "Host '192.168.101.177' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'".
```
- 原因：  max_connect_errors是一个MySQL中与安全有关的计数器值，它负责阻止过多尝试失败的客户端以防止暴力破解密码的情况。max_connect_errors的值与性能并无太大关系。当此值设置为10时，意味着如果某一客户端尝试连接此MySQL服务器，但是失败（如密码错误等等）10次，则MySQL会无条件强制阻止此客户端连接。
- 解决：重置此计数器的值，则必须重启MySQL服务器或者执行 mysql> flush hosts; 命令。当这一客户端成功连接一次MySQL服务器后，针对此客户端的max_connect_errors会清零。

### 踩坑三十七
- 问题：初始化脚本运行，在写入了部分分区后报错如下
```
return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask. Exception when loading 1 in table
```
- 原因：分区存在，但是分区文件夹被删除了
- 解决：修复分区表`MSCK REPAIR TABLE table_name`；或直接重建表和删除所有分区文件夹，然后重新运行脚本。


### 踩坑三十八
- 问题：使用Tez引擎，map和reducer一直pending，任务无法运行。日志如下
```
2022-01-18 01:43:08,450 [WARN] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Held container expected to be not null for a non-AM-released container
2022-01-18 01:43:08,450 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Ignoring unknown container: container_1642441041019_0001_01_000087
2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Preempting new container: container_1642441041019_0001_01_000088 with priority: 152 to free resource for request: Capability[<memory:10240, vCores:2>]Priority[122]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null] . Current free resources: <memory:9216, vCores:30>
2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Resending request for task again: attempt_1642441041019_0001_1_10_000000_0
2022-01-18 01:43:10,458 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Deallocating task: attempt_1642441041019_0001_1_10_000000_0 before allocation
2022-01-18 01:43:10,459 [INFO] [AMRM Callback Handler Thread] |rm.YarnTaskSchedulerService|: Allocation request for task: attempt_1642441041019_0001_1_10_000000_0 with request: Capability[<memory:10240, vCores:2>]Priority[152]AllocationRequestId[0]ExecutionTypeRequest[{Execution Type: GUARANTEED, Enforce Execution Type: false}]Resource Profile[null] host: cos-bigdata-test-hadoop-01 rack: null
2022-01-18 01:43:10,459 [INFO] [DelayedContainerManager] |rm.YarnTaskSchedulerService|: AssignAll - Skipping delayed container as container is no longer running, containerId=container_1642441041019_0001_01_000088
```
- 原因：mr的mapreduce.reduce.cpu.vcores和yarn.scheduler.minimum-allocation-vcores配置对Tez不起作用，需要手动设置。
- 解决：set hive.tez.cpu.vcores=1;

### 踩坑三十九
- 问题：Tez执行任务报错 `Too many counters: 121 max=120`
- 原因：
- 解决：修改Hadoop的mapred-site.xml配置文件的mapreduce.job.counters.max参数，默认为120，需要设置成更大的值。
```
<property> 
    <name>mapreduce.job.counters.max</name> 
    <value>200</value> 
</property> 
```

### 踩坑四十
- 问题：Tez执行过程中报错如下
```
OutOfMemoryError: unable to create new native thread
```
- 原因：可能是因为最大创建线程限制
- 解决：
查看当前允许最大线程`sysctl kernel.pid_max`
查看当前系统线程总数`ps -eLf | wc -l`
临时修改`echo 1000000 > /proc/sys/kernel/pid_max`
永久修改`echo "kernel.pid_max=1000000 " >> /etc/sysctl.conf; sysctl -p`


### 踩坑四十一
- 问题：使用Azkaban调度时，偶尔会报错如下，但是资源应该是充足的，且重新跑会成功执行
```
Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
```
- 原因：
- 解决：


### 踩坑四十二
- 问题：使用Tez引擎时，如果使用INSERT ... UNION ALL语句，那么会出现查询不到数据的情况。
- 原因：Tez引擎对于insert union操作会进行优化，通过并行加快速度，为防止有相同文件输出，所以对并行的输出各自生成成了一个子目录，在子目录中存放结果。而MR和Spark引擎不会递归查询分区目录下的文件。
- 解决：尝试使用UNION语句后再SELECT到一张临时表中再进行INSERT，还是会有子目录出现。
   - 方法一：查询时引擎设置为Tez；
   - 方法二：设置mr引擎递归查询 `set mapreduce.input.fileinputformat.input.dir.recursive=true;` 对spark引擎同样生效。但是加上这个语句会造成运行insert union语句卡死；
   - 方法三：使用union替代union all。


### 踩坑四十三
- 问题：使用Hive-on-spark引擎计算开窗函数，那么开窗函数中的row between ... and ... 语句将不会生效，默认是从第一行到当前行。
- 原因：Hive-on-spark引擎的bug。
- 解决：切换回MR引擎则恢复正常。

### 踩坑四十四
- 问题：使用Hive-on-spark引擎，在子查询中使用datediff函数，且函数中的日期是string类型的，那么就会报空指针异常。
- 原因：Hive-on-spark引擎的bug。
- 解决：1. 切换回MR引擎； 2. 将string类型转换为date类型。


### 踩坑四十五
- 问题：使用Hive-on-spark引擎时，报错 Caused by: java.lang.ClassNotFoundException: org.antlr.runtime.tree.CommonTree
- 原因：缺少包 antlr-runtime-3.5.2.jar
- 解决：需要在hive-site.xml中配置的spark.yarn.jars的hdfs路径中，添加缺失的包 antlr-runtime-3.5.2.jar

### 踩坑四十六
- 问题：使用Mapreduce引擎对历史数据进行动态分区数据插入，报错如下
**Error: Java heap space**或**Error: GC overhead limit exceeded**；不动态插入，直接查询就没有问题；
- 原因：Parquet和ORC是列式批处理文件格式。这些格式要求在写入文件之前将批次的行（batches of rows）缓存在内存中。在这种情况下，每个mapper必须为遇到的每个动态分区创建一个新的文件写入器（file writer）。在执行INSERT语句时，动态分区目前的实现是：至少为每个动态分区目录打开一个文件写入器（file writer）。由于这些缓冲区是按分区维护的，因此在运行时所需的内存量随着分区数量的增加而增加。所以经常会导致mappers或reducers的OOM，具体取决于打开的文件写入器（file writer）的数量。默认情况下，Hive为每个打开的Parquet文件缓冲区（file buffer）分配128MB。这个buffer大小由参数parquet.block.size控制。为获得最佳性能，parquet的buffer size需要与HDFS的block size保持对齐（比如相等），从而使每个parquet文件在单个HDFS的块中，以便每个I/O请求都可以读取整个数据文件，而无需通过网络传输访问后续的block。`set parquet.block.size=268435456;(256MB)`
- 解决：
1. 首先确定开启了动态分区和非严格模式.
```
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
```
2. 启用自动排序参数，强制产生reduce任务。
```
set hive.optimize.sort.dynamic.partition=true;
```
3. 如果还报错内存溢出，强制修改map端内存大小.
```
set mapreduce.map.java.opts=-Xmx4096m
```
4. 如果仍然报错内存溢出，调整时间间隔，分多次进行动态分区或增加map或者reduce的数量。

动态分区导致内存溢出，以上优化无效时，可以打开这个参数
`hive.optimize.sort.dynamic.partition=true;`
产生OOM问题的原因是同时打开了太多写分区的record writer同时写入文件，开启该参数的话，分区列会全局排序，使得reduce端每个分区只有一个文件写入，降低reduce的内存压力，但会严重降低reduce处理并写入一个分区的速度。

>官网解释是：When enabled, dynamic partitioning column will be globally sorted. This way we can keep only one record writer open for each partition value in the reducer thereby reducing the memory pressure on reducers.


### 踩坑四十七
- 现象：使用mr引擎时，使用如下sql会出现相同的数据无法匹配的问题
```
set hive.execution.engine=mr;
set mapred.reduce.tasks=5;
select *
from (
         select t1.id, t1.name
         from default.test t1
                  left join
              default.test t2
              on t1.id = t2.id
     ) t3
         full outer join
     default.test t4
     on t3.id = t4.id;
```

奇怪的是，如下的每一种操作都会使结果正确：
①使用spark引擎时结果正确，
②去掉LEFT JOIN 语句结果也是正确的
③将full outer join 换成inner join也是正确的。
④reduce任务数量为10，如果将reduce数量设置为1
- 原因：自己建假数据也能复现这个问题，感觉是个bug，需要后续观察；
- 解决：目前来看最好的办法就是将 full join 前一张表的left join删掉，或者直接挪到最后。





###踩坑五十
- 问题：执行mr任务时，reduce任务进程到100%，但是一直没有完成，直到报错如下
```
	AttemptID:attempt_1648013040469_0016_r_000000_0 Timed out after 600 secs [2022-03-23 14:49:58.214]Sent signal OUTPUT_THREAD_DUMP (SIGQUIT) to pid 11603 as user hive for container container_1648013040469_0016_01_000005, result=success [2022-03-23 14:49:58.219]Container killed by the ApplicationMaster. [2022-03-23 14:49:58.224]Container killed on request. Exit code is 143 [2022-03-23 14:49:58.243]Container exited with a non-zero exit code 143. 
```
- 原因：因为我在inner join/left join/full outer join的连接条件中使用了if 和 case when语句。hql的join on操作只支持相等条件，但是又不会报错，只会卡死在那里。
- 解决：不再连接条件on后面使用if 和 case when 。如果连接条件比较复杂，可以在连接表中对列进行处理，再作为连接条件即可。

### 踩坑五十一
- 问题：执行tez任务时，设置的参数 hive.tez.cpu.vcores 不等于1时，就会出现任务一直pending
- 原因：未知
- 解决：将参数 hive.tez.cpu.vcores 设置为1


### 踩坑五十二
- 问题：执行hive脚本时，发现如下脚本中的hql语句，插入的分区都是**__HIVE_DEFAULT_PARTITION__**，但是直接执行hql是正常的
```
INSERT OVERWRITE TABLE compass_dwd.dwd_edb_send PARTITION (ds)
SELECT tmp_send.dlid,
       tmp_send.csocode,
       regexp_extract(cdefine32, '(\d){4}-(\d){2}-(\d){2}', 0) as ds
FROM (
         SELECT *
         FROM compass_dwd.dwd_so_send
         where ds = '$do_date'
     ) tmp_send
         JOIN
     (
         SELECT *
         FROM compass_dwd.dwd_edb_order_paid
     ) tmp_order_model
     ON tmp_send.cdlcode = tmp_order_model.tid;
```
- 原因：在脚本中，函数 **regexp_extract(cdefine32, '(\d){4}-(\d){2}-(\d){2}', 0)**  中的特殊符号需要再次转义。同datax脚本中的转义 **\${ds} as ds**
- 解决：添加转义符如下
```
INSERT OVERWRITE TABLE compass_dwd.dwd_edb_send PARTITION(ds)
SELECT tmp_send.dlid,
       tmp_send.csocode,
       regexp_extract(cdefine32, '(\\d){4}-(\\d){2}-(\\d){2}', 0) as ds
FROM (
         SELECT *
         FROM compass_dwd.dwd_so_send
         where ds = '2022-04-17'
     ) tmp_send
         JOIN
     (
         SELECT *
         FROM compass_dwd.dwd_edb_order_paid
     ) tmp_order_model
     ON tmp_send.cdlcode = tmp_order_model.tid
WHERE regexp_extract(cdefine32, '(\\d){4}-(\\d){2}-(\\d){2}', 0) is not null;
```
所以在用到特殊字符时，需要注意是否需要转义，如 `select str_to_map("name=cj|age=18","\|","=");`  中的符号需要转义，而在shell脚本中还需要对转义符转义。


### 踩坑五十三
- 问题：dwt_so_paid_order_topic计算同比环比时，发现理应有值的字段结果却是 null ，设置 set hive.vectorized.execution.enabled=false; 后就正常了
- 原因：未知
- 解决：设置  set hive.vectorized.execution.enabled=false; 

### 踩坑五十四
- 问题：还是上面的同一个脚本 dwt_so_paid_order_topic 计算同比环比时，有两个LEFT JOIN 语句如下，发现最后一张LEFT JOIN 表的字段结果都是 null，tmp_mom和tmp_yoy表无论谁放在后面都会变成null。
```
         SELECT *
         FROM tmp_date tmp_ori
                  LEFT JOIN tmp_date tmp_mom
                            ON tmp_ori.mom_date = tmp_mom.date_end and tmp_ori.date_type = tmp_mom.date_type
                                and nvl(tmp_ori.cinvcode, 'null') = nvl(tmp_mom.cinvcode, 'null')
                                and nvl(tmp_ori.cinvccode, 'null') = nvl(tmp_mom.cinvccode, 'null')
                                and nvl(tmp_ori.group_code, 'null') = nvl(tmp_mom.group_code, 'null')
                                and nvl(tmp_ori.division_code, 'null') = nvl(tmp_mom.division_code, 'null')
                                and nvl(tmp_ori.shopid, 'null') = nvl(tmp_mom.shopid, 'null')
                                and nvl(tmp_ori.channel_id, 'null') = nvl(tmp_mom.channel_id, 'null')
                                and nvl(tmp_ori.class_id, 'null') = nvl(tmp_mom.class_id, 'null')
                  LEFT JOIN tmp_date tmp_yoy
                            ON tmp_ori.yoy_date = tmp_yoy.date_end and tmp_ori.date_type = tmp_yoy.date_type
                                and nvl(tmp_ori.cinvcode, 'null') = nvl(tmp_yoy.cinvcode, 'null')
                                and nvl(tmp_ori.cinvccode, 'null') = nvl(tmp_yoy.cinvccode, 'null')
                                and nvl(tmp_ori.group_code, 'null') = nvl(tmp_yoy.group_code, 'null')
                                and nvl(tmp_ori.division_code, 'null') = nvl(tmp_yoy.division_code, 'null')
                                and nvl(tmp_ori.shopid, 'null') = nvl(tmp_yoy.shopid, 'null')
                                and nvl(tmp_ori.channel_id, 'null') = nvl(tmp_yoy.channel_id, 'null')
                                and nvl(tmp_ori.class_id, 'null') = nvl(tmp_yoy.class_id, 'null');
```
配置中配置了mapjoin的相关参数
```
set hive.auto.convert.join=true;
set hive.mapjoin.smalltable.filesize=250000000;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join.noconditionaltask.size=500000000;
```
- 原因：未知，不知道是不是 三张表其实都是同一张表，即 自己join自己join自己 的原因
- 解决：直接关闭mapjoin
```
set hive.auto.convert.join=false;
```


### 踩坑五十五
- 问题：执行任务时报错
```
Exception: Too many counters: 121 max=120
```
- 原因：如果执行引擎时 tez，则说明当前作业的 counters 数量超过 tez 默认的 counters 限制。
- 解决：设置新的限制值
```
set tez.counters.max =20000;
set tez.counters.max.groups=10000;
```

### 踩坑五十六
- 问题：跑任务时报错
```
java.lang.IllegalArgumentException: tez.runtime.io.sort.mb 2048 should be larger than 0 and should be less than the available task memory (MB):1456
```
- 原因：设置的tez.runtime.io.sort.mb比hive.tez.container.size相等或还要大，tez.runtime.io.sort.mb无法获取指定的内存大小导致任务失败
- 解决：增大hive.tez.container.size或减小tez.runtime.io.sort.mb

### 踩坑五十七
- 问题：执行完任务进行多分区插入时报错
```
30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - Loading data to table compass_dwd.dwd_so_stock_out partition (ds=null)
30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - 
30-05-2022 18:20:07 CST dwd_so_stock_out_init INFO - 
30-05-2022 18:26:14 CST dwd_so_stock_out_init INFO - 	 Time taken to load dynamic partitions: 367.517 seconds
30-05-2022 18:26:16 CST dwd_so_stock_out_init INFO - 	 Time taken for adding to write entity : 1.362 seconds
30-05-2022 18:27:48 CST dwd_so_stock_out_init INFO - FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.StatsTask. GC overhead limit exceeded
30-05-2022 18:27:48 CST dwd_so_stock_out_init INFO - Process with id 17249 completed unsuccessfully in 602 seconds.
```
- 原因：
- 解决：


### 踩坑五十八
- 问题：自定义UDTF函数compass_dws.parseErrorCodeAndGetDiff(arg1,arg2)，在执行`select compass_dws.parseErrorCodeAndGetDiff("12", "0")`是有两个结果，但是执行如下sql时没有数据
```
SELECT *
FROM (
         SELECT 1 as id
         UNION ALL
         SELECT 2 as id
     ) tmp_event LATERAL VIEW compass_dws.parseErrorCodeAndGetDiff("12", "0") tmp as fault_code;
```
- 原因：因为在写自定义函数的java程序中，通过forward返回的结果需要是一个数组，如果不是数组，那么就会出现上述这种情况。
- 解决：将forward返回的结果用数组包裹起来。



### 踩坑六十一
- 问题：执行sql报错如下
```
Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 5, vertexId=vertex_1649781999334_26772_1_00, diagnostics=[Task failed, taskId=task_1649781999334_26772_1_00_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : java.lang.RuntimeException: java.lang.AssertionError: Output column number expected to be 0 when isRepeating
```
- 原因：
- 解决：禁用vectorized  `set hive.vectorized.execution.enabled=false;`

### 踩坑六十二
- 问题：使用Tez引擎进行计算时报错如下(使用DataGrip用hiveserver2连接)
```
Dag submit failed due to unexpected checked exception stack trace: [org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1167), org.apache.hadoop.ipc.Client.call(Client.java:1441), org.apache.hadoop.ipc.Client.call(Client.java:1388), org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233), org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118), com.sun.proxy.$Proxy60.submitDAG(Unknown Source), org.apache.tez.client.FrameworkClient.submitDag(FrameworkClient.java:133), org.apache.tez.client.TezClient.submitDAGSession(TezClient.java:702), org.apache.tez.client.TezClient.submitDAG(TezClient.java:611), org.apache.hadoop.hive.ql.exec.tez.TezTask.submit(TezTask.java:538), org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:216), org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205), org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97), org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2664), org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2335), org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2011), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1709), org.apache.hadoop.hive.ql.Driver.run(Driver.java:1703), org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157), org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224), org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:316), java.security.AccessController.doPrivileged(Native Method), javax.security.auth.Subject.doAs(Subject.java:422), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729), org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:329), java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511), java.util.concurrent.FutureTask.run(FutureTask.java:266), java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149), java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624), java.lang.Thread.run(Thread.java:748)] retrying...
Session re-established.
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask
```
- 原因：直接用hive本地客户端跑是可以成功的，怀疑是hiveserver2中的配置问题导致任务提交失败。
- 解决：[HDFS问题集（一），使用命令报错：com.google.protobuf.ServiceException:java.lang.OutOfMemoryError:java heap space - osc_z7d2bxvl的个人空间 - OSCHINA - 中文开源技术交流社区](https://my.oschina.net/u/4413909/blog/3659191)


### 踩坑六十三
- 问题：使用如下计算时，会发现year_month_day不为null时，显示的结果不是year_month_day的值。如year_month_day为2022-06-18时，最终date_end的结果为2022-06-30，但是修改为when year_month_day is not null then date_add(year_month_day,1)时，结果时2022-06-19是正确的。
```
         SELECT productkey,
                iotid,
                devicename,
                event_name,
                count(*)       as device_uv,
                case
                    when year_month_day is not null then year_month_day
                    when year_week is not null and date_sub(next_day(max(ds), 'MO'), 1) <= '2022-06-18'
                        then date_sub(next_day(max(ds), 'MO'), 1)
                    when year_month is not null and last_day(max(ds)) <= '2022-06-18' then last_day(max(ds))
                    when year is not null and concat(year, '-12-31') <= '2022-06-18' then concat(year, '-12-31')
                    else '2022-06-18'
                    end        as date_end,
                case
                    when year_month_day is not null then 3
                    when year_week is not null then 2
                    when year_month is not null then 1
                    when year is not null then 0
                    else 4 end as date_type
         FROM (
                  SELECT productkey,
                         iotid,
                         devicename,
                         event_name,
                         happen_count,
                         ds                                                                  as year_month_day,
                         concat(year(date_add(next_day(ds, 'MO'), -4)), '-', weekofyear(ds)) as year_week,
                         date_format(ds, 'yyyy-MM')                                          as year_month,
                         year(ds)                                                            as year,
                         ds
                  FROM compass_dws.dws_feiyan_device_event_count_day
                  WHERE ds <= '2022-06-18'
                    and happen_count > 0
                    and iotid is not null
                    and opt_id is null
              ) tmp_event
         GROUP BY productkey, iotid, devicename, event_name, year_month_day, year_week, year_month, year
             GROUPING SETS (
                  -- 计算总设备日周月年总活跃数和每个产品下的日周月年总活跃度
             ( productkey, event_name, year_month_day),
             ( productkey, event_name, year_week),
             ( productkey, event_name, year_month),
             ( productkey, event_name, year),
             ( productkey, event_name),
             ( event_name, year_month_day),
             ( event_name, year_week),
             ( event_name, year_month),
             ( event_name, year),
             ( event_name)
             )
```
- 原因：奇怪的bug，原因未知。不管是hive本地客户端还是datagrip连接hiveserver2，显示的结果是错误的，但是如果对该字段进行计算或筛选，会发现最终的计算结果时正确的；如果去掉GROUPING SETS中的聚合条件只剩下一个，最终结果也是正确的。但是在某些条件下还是会影响结果正确性。
- 解决：修改为 `when year_month_day is not null then date_add(year_month_day,0)`


### 踩坑六十四
- 问题：执行sql如下
```
SELECT tmp_event_count.productkey,
       tmp_event_count.iotid,
       tmp_event_count.devicename,
       tmp_event_count.event_name,
       tmp_event_count.opt_id,
       tmp_event_count.opt_name,
       tmp_event_count.event_num,
       tmp_event_count.year_month_day,
       tmp_event_count.year_week,
       tmp_event_count.year_month,
       tmp_event_count.year,
       tmp_event_count.date_end,
       tmp_device_uv.device_uv
FROM tmp_event_count
         LEFT JOIN
     tmp_device_uv
     ON tmp_event_count.event_name = tmp_device_uv.event_name
          and NVL(tmp_event_count.productkey, 'NULL') = NVL(tmp_device_uv.productkey, 'NULL')
         and tmp_event_count.date_type = tmp_device_uv.date_type
         and tmp_event_count.date_end = tmp_device_uv.date_end
         and tmp_event_count.iotid is null
         and tmp_event_count.opt_id is null
```
会发现结果如下
```
NULL,,,ErrorCode,,,135,2022-06-18,,,,2022-06-18,88
NULL,,,StOvMode,,,115,2022-06-18,,,,2022-06-18,70
,,,MultiStageName,,,15,2022-06-18,,,,2022-06-18,14
a17JZbZVctc,,,MultiStageName,,,15,2022-06-18,,,,2022-06-18,14
,,,StOvMode,,,115,2022-06-18,,,,2022-06-18,70
,,,ErrorCode,,,135,2022-06-18,,,,2022-06-18,88
```
可以发现有部分productkey被替换为了连接条件中的"NULL"，
- 原因：应该时bug，同if和case when一样，这些条件不能用在连接条件ON里面
- 解决：先对表中的列进行处理(如将null值替换为空串)，然后用处理后的列进行连接。


### 踩坑六十五
- 问题：sql中使用full outer join时，莫名其妙多了一条数据，且该数据时脏数据
```
WITH tmp_order AS (
    SELECT date_range_type,
           date_range,
           smart_product_code,
           sum(sale_num)       as order_num,
           sum(send_stock_out) as stock_out_num
    FROM (
             SELECT smart_product_code, smart_product_name, product_code
             FROM compass_dim.dim_feiyan_smart_product
             WHERE ds = '2022-06-19'
         ) tmp_smart_product
             INNER JOIN
         (
             SELECT data['date_range_type']                        as date_range_type,
                    data['date_range']                             as date_range,
                    cast(nvl(data['sale_num'], 0) as bigint)       as sale_num,
                    cast(nvl(data['send_stock_out'], 0) as bigint) as send_stock_out,
                    cinvccode
             FROM (
                      SELECT cinvccode,
                             ds,
                             sale_num_day,
                             send_stock_out_day,
                             sale_num_week,
                             send_stock_out_week,
                             sale_num_month,
                             send_stock_out_month,
                             sale_num_year,
                             send_stock_out_year
                      FROM compass_dwt.dwt_so_paid_order_topic
                      WHERE ds = '2022-06-19'
                        and cinvcode is null
                        and cinvccode is not null
                        and class_id is null
                  ) tmp_order LATERAL VIEW explode(
                     array(map('date_range_type', '3', 'date_range', ds, 'sale_num', sale_num_day,
                               'send_stock_out', send_stock_out_day),
                           map('date_range_type', '2',
                               'date_range', concat(year(date_add(next_day(ds, 'MO'), -4)), '-', weekofyear(ds)),
                               'sale_num', sale_num_week, 'send_stock_out', send_stock_out_week),
                           map('date_range_type', '1', 'date_range', date_format(ds, 'yyyy-MM')
                               , 'sale_num', sale_num_month, 'send_stock_out', send_stock_out_month),
                           map('date_range_type', '0', 'date_range', year(ds), 'sale_num', sale_num_year,
                               'send_stock_out',
                               send_stock_out_year))
                 ) tmp as data
             UNION ALL
             SELECT '4'                         as date_range_type,
                    'total'                     as date_range,
                    nvl(sale_num_year, 0)       as sale_num,
                    nvl(send_stock_out_year, 0) as send_stock_out_year,
                    cinvccode
             FROM compass_dwt.dwt_so_paid_order_topic
             WHERE (ds = concat(year(ds), '-12-31') or ds = '2022-06-19')
               and cinvcode is null
               and cinvccode is not null
               and class_id is null
         ) tmp_order
         ON tmp_smart_product.product_code = tmp_order.cinvccode
    GROUP BY smart_product_code, date_range_type, date_range
)
   , tmp_install AS (
    SELECT 4             as date_range_type,
           'total'       as date_range,
           'a17JZbZVctc' as smart_product_code,
           0             as install_num
)
   , tmp_app_bind AS (
    SELECT 4             as date_range_type,
           'total'       as date_range,
           'a17JZbZVctc' as smart_product_code,
           0             as binding_num
)
INSERT OVERWRITE TABLE compass_ads.ads_cb_hifun_device_info
SELECT coalesce(tmp_order.date_range_type, tmp_app_bind.date_range_type, tmp_install.date_range_type)
           as date_ranger_type,
       coalesce(tmp_order.date_range, tmp_app_bind.date_range, tmp_install.date_range)
           as date_ranger_type,
       coalesce(tmp_order.smart_product_code, tmp_app_bind.smart_product_code, tmp_install.smart_product_code)
           as date_ranger_type,
       nvl(order_num, 0) as order_num,
       nvl(stock_out_num, 0) as stock_out_num,
       nvl(install_num, 0) as install_num,
       nvl(binding_num, 0) as binding_num
FROM tmp_order
         FULL OUTER JOIN tmp_app_bind
                         ON tmp_order.date_range = tmp_app_bind.date_range
                             and tmp_order.date_range_type = tmp_app_bind.date_range_type
                             and tmp_order.smart_product_code = tmp_app_bind.smart_product_code
         FULL OUTER JOIN tmp_install
                         ON tmp_order.date_range = tmp_install.date_range
                             and tmp_order.date_range_type = tmp_install.date_range_type
                             and tmp_order.smart_product_code = tmp_install.smart_product_code;
```
结果为![image.png](集群问题2.assets\463d180020ce4128b7877396b9be08a4.png)
而实际结果为![image.png](集群问题2.assets\3b6479abedb642b99f33b908cbfdf1c3.png)
- 原因：当关闭mapjoin后，结果正常。推断时mapjoin的bug。
- 解决：关闭mapjoin `set hive.merge.mapredfiles=false;`


### 踩坑六十七
- 问题：namenode高可用配置后，运行了6个小时，active namenode转为standby，但是没有选举出新的active namenode节点
```
2023-04-21 09:15:34,484 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode
2023-04-21 09:15:34,493 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn2, ipcAddress=cos-bigdata-test-flink-02/192.168.101.194:8020, httpAddress=http://cos-bigdata-test-flink-02:9870], try next.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error
2023-04-21 09:15:34,497 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nn3, ipcAddress=cos-bigdata-test-flink-03/192.168.101.195:8020, httpAddress=http://cos-bigdata-test-flink-03:9870], try next.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error
2023-04-21 09:15:34,504 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN
java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:206)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:416)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:475)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$300(EditLogTailer.java:441)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:458)
	at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:482)
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:454)
Caused by: java.io.IOException: Cannot find any valid remote NN to service request!
	at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:573)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
```
- 原因：未找到原因
- 解决：关闭hdfs集群`stop-dfs.sh`，格式化zookeeper `hdfs zkfc -formatZK`，重启hdfs集群 `start-dfs.sh`后正常。此时再kill掉namenode后可以正常进行故障转移。


### 踩坑六十八
- 现象：在运行几周后，101.194节点上的namenode掉线，报错如下
```
2023-04-30 02:48:42,880 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Took 2112ms to send a batch of 1 edits (17 bytes) to remote journal 192.168.101.193:8485
2023-04-30 02:48:45,182 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem write lock held for 7419 ms via java.lang.Thread.getStackTrace(Thread.java:1559)
org.apache.hadoop.util.StringUtils.getStackTrace(StringUtils.java:1032)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:263)
org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeUnlock(FSNamesystemLock.java:225)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeUnlock(FSNamesystem.java:1624)
org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4692)
org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1311)
org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
java.security.AccessController.doPrivileged(Native Method)
javax.security.auth.Subject.doAs(Subject.java:422)
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)
	Number of suppressed write-lock reports: 0
	Longest write-lock held interval: 7419.0 
	Total suppressed write-lock held time: 0.0
2023-04-30 02:56:52,890 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 6001 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.
2023-04-30 02:56:53,892 INFO org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 7002 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.
2023-04-30 02:57:05,904 WARN org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager: Waited 19015 ms (timeout=20000 ms) for a response for startLogSegment(578736). No responses yet.
2023-04-30 02:57:06,891 FATAL org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: starting log segment 578736 failed for required journal (JournalAndStream(mgr=QJM to [192.168.101.193:8485, 192.168.101.194:8485, 192.168.101.195:8485], stream=null))
java.io.IOException: Timed out waiting 20000ms for a quorum of nodes to respond.
	at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138)
	at org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.startLogSegment(QuorumJournalManager.java:436)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalAndStream.startLogSegment(JournalSet.java:95)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$1.apply(JournalSet.java:210)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:385)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.startLogSegment(JournalSet.java:207)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegment(FSEditLog.java:1385)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.startLogSegmentAndWriteHeaderTxn(FSEditLog.java:1397)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog(FSEditLog.java:1321)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog(FSImage.java:1367)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4690)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1311)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:146)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12974)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:527)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1036)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1000)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:928)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2916)
2023-04-30 02:57:06,898 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: Error: starting log segment 578736 failed for required journal (JournalAndStream(mgr=QJM to [192.168.101.193:8485, 192.168.101.194:8485, 192.168.101.195:8485], stream=null))
2023-04-30 02:57:06,906 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: FSImageSaver clean checkpoint: txid = 99507 when meet shutdown.
2023-04-30 02:57:06,922 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: FSImageSaver clean checkpoint: txid = 143829 when meet shutdown.
```
其中193的journa日志如下
```
2023-04-30 02:57:08,532 WARN org.apache.hadoop.ipc.Server: IPC Server handler 0 on default port 8485, call Call#176302 Retry#0 org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocol.startLogSegment from 192.168.101.194:57076: output error
2023-04-30 02:57:12,834 WARN org.apache.hadoop.hdfs.qjournal.server.Journal: Latest log EditLogFile(file=/opt/module/hadoop/journal/hdfscluster/current/edits_inprogress_0000000000000578736,first=0000000000000578736,last=-000000000000012345,inProgress=true,hasCorruptHeader=false) has no transactions. moving it aside and looking for previous log ; journal id: hdfscluster
```
- 原因：
- 解决：
