---
title: Spark-Tezè°ƒä¼˜
categories:
- å¤§æ•°æ®å»ºæ¨¡
---
# ä¸€ã€Spark
## 1.1 åŸç†è¯´æ˜

![åŸç†å›¾](Spark-Tezè°ƒä¼˜.assets2c85806a6494f5db82bcf806925b58a.png)

è¯¦ç»†åŸç†è§ä¸Šå›¾ã€‚
æˆ‘ä»¬ä½¿ç”¨spark-submitæäº¤ä¸€ä¸ªSparkä½œä¸šä¹‹åï¼Œè¿™ä¸ªä½œä¸šå°±ä¼šå¯åŠ¨ä¸€ä¸ªå¯¹åº”çš„Driverè¿›ç¨‹ã€‚æ ¹æ®ä½ ä½¿ç”¨çš„éƒ¨ç½²æ¨¡å¼ï¼ˆdeploy-modeï¼‰ä¸åŒï¼ŒDriverè¿›ç¨‹å¯èƒ½åœ¨æœ¬åœ°å¯åŠ¨ï¼Œä¹Ÿå¯èƒ½åœ¨é›†ç¾¤ä¸­æŸä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šå¯åŠ¨ã€‚Driverè¿›ç¨‹æœ¬èº«ä¼šæ ¹æ®æˆ‘ä»¬è®¾ç½®çš„å‚æ•°ï¼Œå æœ‰ä¸€å®šæ•°é‡çš„å†…å­˜å’ŒCPU coreã€‚è€ŒDriverè¿›ç¨‹è¦åšçš„ç¬¬ä¸€ä»¶äº‹æƒ…ï¼Œå°±æ˜¯å‘é›†ç¾¤ç®¡ç†å™¨ï¼ˆå¯ä»¥æ˜¯Spark Standaloneé›†ç¾¤ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¶ä»–çš„èµ„æºç®¡ç†é›†ç¾¤ï¼Œç¾å›¢â€¢å¤§ä¼—ç‚¹è¯„ä½¿ç”¨çš„æ˜¯YARNä½œä¸ºèµ„æºç®¡ç†é›†ç¾¤ï¼‰ç”³è¯·è¿è¡ŒSparkä½œä¸šéœ€è¦ä½¿ç”¨çš„èµ„æºï¼Œè¿™é‡Œçš„èµ„æºæŒ‡çš„å°±æ˜¯Executorè¿›ç¨‹ã€‚YARNé›†ç¾¤ç®¡ç†å™¨ä¼šæ ¹æ®æˆ‘ä»¬ä¸ºSparkä½œä¸šè®¾ç½®çš„èµ„æºå‚æ•°ï¼Œåœ¨å„ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œå¯åŠ¨ä¸€å®šæ•°é‡çš„Executorè¿›ç¨‹ï¼Œæ¯ä¸ªExecutorè¿›ç¨‹éƒ½å æœ‰ä¸€å®šæ•°é‡çš„å†…å­˜å’ŒCPU coreã€‚

Sparkæ˜¯æ ¹æ®shuffleç±»ç®—å­æ¥è¿›è¡Œstageçš„åˆ’åˆ†ã€‚å¦‚æœæˆ‘ä»¬çš„ä»£ç ä¸­æ‰§è¡Œäº†æŸä¸ªshuffleç±»ç®—å­ï¼ˆæ¯”å¦‚reduceByKeyã€joinç­‰ï¼‰ï¼Œé‚£ä¹ˆå°±ä¼šåœ¨è¯¥ç®—å­å¤„ï¼Œåˆ’åˆ†å‡ºä¸€ä¸ªstageç•Œé™æ¥ã€‚å¯ä»¥å¤§è‡´ç†è§£ä¸ºï¼Œshuffleç®—å­æ‰§è¡Œä¹‹å‰çš„ä»£ç ä¼šè¢«åˆ’åˆ†ä¸ºä¸€ä¸ªstageï¼Œshuffleç®—å­æ‰§è¡Œä»¥åŠä¹‹åçš„ä»£ç ä¼šè¢«åˆ’åˆ†ä¸ºä¸‹ä¸€ä¸ªstageã€‚å› æ­¤ä¸€ä¸ªstageåˆšå¼€å§‹æ‰§è¡Œçš„æ—¶å€™ï¼Œå®ƒçš„æ¯ä¸ªtaskå¯èƒ½éƒ½ä¼šä»ä¸Šä¸€ä¸ªstageçš„taskæ‰€åœ¨çš„èŠ‚ç‚¹ï¼Œå»é€šè¿‡ç½‘ç»œä¼ è¾“æ‹‰å–éœ€è¦è‡ªå·±å¤„ç†çš„æ‰€æœ‰keyï¼Œç„¶åå¯¹æ‹‰å–åˆ°çš„æ‰€æœ‰ç›¸åŒçš„keyä½¿ç”¨æˆ‘ä»¬è‡ªå·±ç¼–å†™çš„ç®—å­å‡½æ•°æ‰§è¡Œèšåˆæ“ä½œï¼ˆæ¯”å¦‚reduceByKey()ç®—å­æ¥æ”¶çš„å‡½æ•°ï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹å°±æ˜¯shuffleã€‚

taskçš„æ‰§è¡Œé€Ÿåº¦æ˜¯è·Ÿæ¯ä¸ªExecutorè¿›ç¨‹çš„CPU coreæ•°é‡æœ‰ç›´æ¥å…³ç³»çš„ã€‚ä¸€ä¸ªCPU coreåŒä¸€æ—¶é—´åªèƒ½æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹ã€‚è€Œæ¯ä¸ªExecutorè¿›ç¨‹ä¸Šåˆ†é…åˆ°çš„å¤šä¸ªtaskï¼Œéƒ½æ˜¯ä»¥æ¯ä¸ªtaskä¸€æ¡çº¿ç¨‹çš„æ–¹å¼ï¼Œå¤šçº¿ç¨‹å¹¶å‘è¿è¡Œçš„ã€‚å¦‚æœCPU coreæ•°é‡æ¯”è¾ƒå……è¶³ï¼Œè€Œä¸”åˆ†é…åˆ°çš„taskæ•°é‡æ¯”è¾ƒåˆç†ï¼Œé‚£ä¹ˆé€šå¸¸æ¥è¯´ï¼Œå¯ä»¥æ¯”è¾ƒå¿«é€Ÿå’Œé«˜æ•ˆåœ°æ‰§è¡Œå®Œè¿™äº›taskçº¿ç¨‹ã€‚

ä»¥ä¸Šå°±æ˜¯Sparkä½œä¸šçš„åŸºæœ¬è¿è¡ŒåŸç†çš„è¯´æ˜ï¼Œå¤§å®¶å¯ä»¥ç»“åˆä¸Šå›¾æ¥ç†è§£ã€‚ç†è§£ä½œä¸šåŸºæœ¬åŸç†ï¼Œæ˜¯æˆ‘ä»¬è¿›è¡Œèµ„æºå‚æ•°è°ƒä¼˜çš„åŸºæœ¬å‰æã€‚

## 1.2 è°ƒä¼˜å‚æ•°
[å®˜ç½‘é…ç½®](https://spark.apache.org/docs/latest/configuration.html)

| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ | æ¨èå€¼ |
| --- | --- | --- | --- |
| num-executors/spark.executor.instances | è¯¥å‚æ•°ç”¨äºè®¾ç½®Sparkä½œä¸šæ€»å…±è¦ç”¨å¤šå°‘ä¸ªExecutorè¿›ç¨‹æ¥æ‰§è¡Œã€‚Driveråœ¨å‘YARNé›†ç¾¤ç®¡ç†å™¨ç”³è¯·èµ„æºæ—¶ï¼ŒYARNé›†ç¾¤ç®¡ç†å™¨ä¼šå°½å¯èƒ½æŒ‰ç…§ä½ çš„è®¾ç½®æ¥åœ¨é›†ç¾¤çš„å„ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šï¼Œå¯åŠ¨ç›¸åº”æ•°é‡çš„Executorè¿›ç¨‹ã€‚è¿™ä¸ªå‚æ•°éå¸¸ä¹‹é‡è¦ï¼Œå¦‚æœä¸è®¾ç½®çš„è¯ï¼Œé»˜è®¤åªä¼šç»™ä½ å¯åŠ¨å°‘é‡çš„Executorè¿›ç¨‹ï¼Œæ­¤æ—¶ä½ çš„Sparkä½œä¸šçš„è¿è¡Œé€Ÿåº¦æ˜¯éå¸¸æ…¢çš„ |  | æ¯ä¸ªSparkä½œä¸šçš„è¿è¡Œä¸€èˆ¬è®¾ç½®50~100ä¸ªå·¦å³çš„Executorè¿›ç¨‹æ¯”è¾ƒåˆé€‚ï¼Œè®¾ç½®å¤ªå°‘æˆ–å¤ªå¤šçš„Executorè¿›ç¨‹éƒ½ä¸å¥½ã€‚è®¾ç½®çš„å¤ªå°‘ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨é›†ç¾¤èµ„æºï¼›è®¾ç½®çš„å¤ªå¤šçš„è¯ï¼Œå¤§éƒ¨åˆ†é˜Ÿåˆ—å¯èƒ½æ— æ³•ç»™äºˆå……åˆ†çš„èµ„æºã€‚ |
| executor-cores/spark.executor.cores | è®¾ç½®æ¯ä¸ªExecutorè¿›ç¨‹çš„CPU coreæ•°é‡ã€‚è¿™ä¸ªå‚æ•°å†³å®šäº†æ¯ä¸ªExecutorè¿›ç¨‹å¹¶è¡Œæ‰§è¡Œtaskçº¿ç¨‹çš„èƒ½åŠ›ã€‚å› ä¸ºæ¯ä¸ªCPU coreåŒä¸€æ—¶é—´åªèƒ½æ‰§è¡Œä¸€ä¸ªtaskçº¿ç¨‹ï¼Œå› æ­¤æ¯ä¸ªExecutorè¿›ç¨‹çš„CPU coreæ•°é‡è¶Šå¤šï¼Œè¶Šèƒ½å¤Ÿå¿«é€Ÿåœ°æ‰§è¡Œå®Œåˆ†é…ç»™è‡ªå·±çš„æ‰€æœ‰taskçº¿ç¨‹ã€‚ | Yarnæ¨¡å¼ä¸‹é»˜è®¤ä¸º1 | 2~4ä¸ª,num-executors * executor-coresä¸è¦è¶…è¿‡é˜Ÿåˆ—æ€»CPU coreçš„1/3~1/2 |
| executor-memory/spark.executor.memory | è¯¥å‚æ•°ç”¨äºè®¾ç½®æ¯ä¸ªExecutorè¿›ç¨‹çš„å†…å­˜ã€‚Executorå†…å­˜çš„å¤§å°ï¼Œå¾ˆå¤šæ—¶å€™ç›´æ¥å†³å®šäº†Sparkä½œä¸šçš„æ€§èƒ½ï¼Œè€Œä¸”è·Ÿå¸¸è§çš„JVM OOMå¼‚å¸¸ï¼Œä¹Ÿæœ‰ç›´æ¥çš„å…³è”ï¼›ä¸èƒ½å¤§äºcontainerçš„å†…å­˜å€¼yarn.scheduler.maximum-allocation-mb |  | 4G~8G,num-executors * executor-memoryä¸è¦è¶…è¿‡é˜Ÿåˆ—æ€»å†…å­˜çš„1/3~1/2 |
| spark.executor.memoryOverhead | executorå †å¤–å†…å­˜ |é»˜è®¤å€¼æ˜¯max(384M, 0.07 Ã— spark.executor.memory) | 1.5G |
| driver-memory/spark.driver.memory | ç”¨äºé©±åŠ¨ç¨‹åºè¿›ç¨‹çš„å†…å­˜é‡ï¼Œå³åˆå§‹åŒ–SparkContextå†…å­˜ã€‚å¦‚æœéœ€è¦ä½¿ç”¨collectç®—å­å°†RDDçš„æ•°æ®å…¨éƒ¨æ‹‰å–åˆ°Driverä¸Šè¿›è¡Œå¤„ç†ï¼Œé‚£ä¹ˆå¿…é¡»ç¡®ä¿Driverçš„å†…å­˜è¶³å¤Ÿå¤§ï¼Œå¦åˆ™ä¼šå‡ºç°OOMå†…å­˜æº¢å‡ºçš„é—®é¢˜ã€‚ | 1G | 2G |
| spark.driver.memoryOverhead | driverå †å¤–å†…å­˜ | driverMemory * 0.10 |  |
| spark.driver.cores | åœ¨é›†ç¾¤æ¨¡å¼ä¸‹ç®¡ç†èµ„æºæ—¶ï¼Œç”¨äºdriverç¨‹åºçš„CPUå†…æ ¸æ•°é‡ã€‚ | 1 | ç”Ÿäº§ç¯å¢ƒä¸­è°ƒå¤§ |
| spark.default.parallelism | è¯¥å‚æ•°ç”¨äºè®¾ç½®æ¯ä¸ªstageçš„é»˜è®¤taskæ•°é‡ã€‚è¿™ä¸ªå‚æ•°æä¸ºé‡è¦ï¼Œå¦‚æœä¸è®¾ç½®å¯èƒ½ä¼šç›´æ¥å½±å“ä½ çš„Sparkä½œä¸šæ€§èƒ½ | HDFS blockå¯¹åº”ä¸€ä¸ªtaskï¼Œå³blockæ•°é‡ | 500~1000ï¼Œnum-executors * executor-coresçš„2~3å€ |
| spark.storage.memoryFraction | è¯¥å‚æ•°ç”¨äºè®¾ç½®RDDæŒä¹…åŒ–æ•°æ®åœ¨Executorå†…å­˜ä¸­èƒ½å çš„æ¯”ä¾‹ï¼Œé»˜è®¤æ˜¯0.6ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œé»˜è®¤Executor 60%çš„å†…å­˜ï¼Œå¯ä»¥ç”¨æ¥ä¿å­˜æŒä¹…åŒ–çš„RDDæ•°æ®ã€‚æ ¹æ®ä½ é€‰æ‹©çš„ä¸åŒçš„æŒä¹…åŒ–ç­–ç•¥ï¼Œå¦‚æœå†…å­˜ä¸å¤Ÿæ—¶ï¼Œå¯èƒ½æ•°æ®å°±ä¸ä¼šæŒä¹…åŒ–ï¼Œæˆ–è€…æ•°æ®ä¼šå†™å…¥ç£ç›˜ã€‚ | é»˜è®¤æ˜¯0.6 | å¦‚æœSparkä½œä¸šä¸­ï¼Œæœ‰è¾ƒå¤šçš„RDDæŒä¹…åŒ–æ“ä½œï¼Œè¯¥å‚æ•°çš„å€¼å¯ä»¥é€‚å½“æé«˜ä¸€äº›ï¼Œä¿è¯æŒä¹…åŒ–çš„æ•°æ®èƒ½å¤Ÿå®¹çº³åœ¨å†…å­˜ä¸­ã€‚é¿å…å†…å­˜ä¸å¤Ÿç¼“å­˜æ‰€æœ‰çš„æ•°æ®ï¼Œå¯¼è‡´æ•°æ®åªèƒ½å†™å…¥ç£ç›˜ä¸­ï¼Œé™ä½äº†æ€§èƒ½ã€‚ä½†æ˜¯å¦‚æœSparkä½œä¸šä¸­çš„shuffleç±»æ“ä½œæ¯”è¾ƒå¤šï¼Œè€ŒæŒä¹…åŒ–æ“ä½œæ¯”è¾ƒå°‘ï¼Œé‚£ä¹ˆè¿™ä¸ªå‚æ•°çš„å€¼é€‚å½“é™ä½ä¸€äº›æ¯”è¾ƒåˆé€‚ã€‚æ­¤å¤–ï¼Œå¦‚æœå‘ç°ä½œä¸šç”±äºé¢‘ç¹çš„gcå¯¼è‡´è¿è¡Œç¼“æ…¢ï¼ˆé€šè¿‡spark web uiå¯ä»¥è§‚å¯Ÿåˆ°ä½œä¸šçš„gcè€—æ—¶ï¼‰ï¼Œæ„å‘³ç€taskæ‰§è¡Œç”¨æˆ·ä»£ç çš„å†…å­˜ä¸å¤Ÿç”¨ï¼Œé‚£ä¹ˆåŒæ ·å»ºè®®è°ƒä½è¿™ä¸ªå‚æ•°çš„å€¼ã€‚ |
| spark.shuffle.memoryFraction | è¯¥å‚æ•°ç”¨äºè®¾ç½®shuffleè¿‡ç¨‹ä¸­ä¸€ä¸ªtaskæ‹‰å–åˆ°ä¸Šä¸ªstageçš„taskçš„è¾“å‡ºåï¼Œè¿›è¡Œèšåˆæ“ä½œæ—¶èƒ½å¤Ÿä½¿ç”¨çš„Executorå†…å­˜çš„æ¯”ä¾‹ï¼Œé»˜è®¤æ˜¯0.2ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒExecutoré»˜è®¤åªæœ‰20%çš„å†…å­˜ç”¨æ¥è¿›è¡Œè¯¥æ“ä½œã€‚shuffleæ“ä½œåœ¨è¿›è¡Œèšåˆæ—¶ï¼Œå¦‚æœå‘ç°ä½¿ç”¨çš„å†…å­˜è¶…å‡ºäº†è¿™ä¸ª20%çš„é™åˆ¶ï¼Œé‚£ä¹ˆå¤šä½™çš„æ•°æ®å°±ä¼šæº¢å†™åˆ°ç£ç›˜æ–‡ä»¶ä¸­å»ï¼Œæ­¤æ—¶å°±ä¼šæå¤§åœ°é™ä½æ€§èƒ½ã€‚ | é»˜è®¤æ˜¯0.2 | å¦‚æœSparkä½œä¸šä¸­çš„RDDæŒä¹…åŒ–æ“ä½œè¾ƒå°‘ï¼Œshuffleæ“ä½œè¾ƒå¤šæ—¶ï¼Œå»ºè®®é™ä½æŒä¹…åŒ–æ“ä½œçš„å†…å­˜å æ¯”ï¼Œæé«˜shuffleæ“ä½œçš„å†…å­˜å æ¯”æ¯”ä¾‹ï¼Œé¿å…shuffleè¿‡ç¨‹ä¸­æ•°æ®è¿‡å¤šæ—¶å†…å­˜ä¸å¤Ÿç”¨ï¼Œå¿…é¡»æº¢å†™åˆ°ç£ç›˜ä¸Šï¼Œé™ä½äº†æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¦‚æœå‘ç°ä½œä¸šç”±äºé¢‘ç¹çš„gcå¯¼è‡´è¿è¡Œç¼“æ…¢ï¼Œæ„å‘³ç€taskæ‰§è¡Œç”¨æˆ·ä»£ç çš„å†…å­˜ä¸å¤Ÿç”¨ï¼Œé‚£ä¹ˆåŒæ ·å»ºè®®è°ƒä½è¿™ä¸ªå‚æ•°çš„å€¼ã€‚ |
| spark.sql.adaptive.enabled | å¼€å¯ spark çš„è‡ªé€‚åº”æ‰§è¡Œåï¼Œè¯¥å‚æ•°æ§åˆ¶shuffle é˜¶æ®µçš„å¹³å‡è¾“å…¥æ•°æ®å¤§å°ï¼Œé˜²æ­¢äº§ç”Ÿè¿‡å¤šçš„task | false | true |
| spark.sql.adaptive.shuffle.targetPostShuffleInputSize | åŠ¨æ€è°ƒæ•´ reduce ä¸ªæ•°çš„ partition å¤§å°ä¾æ®ã€‚å¦‚è®¾ç½® 64MBï¼Œåˆ™ reduce é˜¶æ®µæ¯ä¸ª task æœ€å°‘å¤„ç† 64MB çš„æ•°æ®ã€‚ | 64MB |128000000 |
| spark.sql.shuffle.partitions | shuffleå¹¶å‘åº¦ | 200 | 800 |
| spark.yarn.executor.memoryOverhead | executoræ‰§è¡Œçš„æ—¶å€™ï¼Œç”¨çš„å†…å­˜å¯èƒ½ä¼šè¶…è¿‡executor-memoyï¼Œæ‰€ä»¥ä¼šä¸ºexecutoré¢å¤–é¢„ç•™ä¸€éƒ¨åˆ†å†…å­˜ | max(MEMORY_OVERHEAD_FACTOR*executorMemory, MEMORY_OVERHEAD_MIN),   é»˜è®¤MEMORY_OVERHEAD_FACTORä¸º0.1, MEMORY_OVERHEAD_MINä¸º384MB | |
| spark.dynamicAllocation.enabled | å¼€å¯åŠ¨æ€èµ„æºé…ç½® | false | true |
| spark.dynamicAllocation.minExecutors | åŠ¨æ€åˆ†é…æœ€å°executorä¸ªæ•°ï¼Œåœ¨å¯åŠ¨æ—¶å°±ç”³è¯·å¥½çš„ | 0 | è§†èµ„æºè€Œå®š |
| spark.dynamicAllocation.maxExecutors | å¼€å¯åŠ¨æ€èµ„æºé…ç½® | infinity | è§†èµ„æºè€Œå®š |
| spark.dynamicAllocation.initialExecutors | åŠ¨æ€åˆ†é…åˆå§‹executorä¸ªæ•°é»˜è®¤å€¼| spark.dynamicAllocation.minExecutors |  |
| spark.dynamicAllocation.executorIdleTimeout | å½“æŸä¸ªexecutorç©ºé—²è¶…è¿‡è¿™ä¸ªè®¾å®šå€¼ï¼Œå°±ä¼šè¢«kill | 60s |  |
| spark.dynamicAllocation.cachedExecutorIdleTimeout | å½“æŸä¸ªç¼“å­˜æ•°æ®çš„executorç©ºé—²æ—¶é—´è¶…è¿‡è¿™ä¸ªè®¾å®šå€¼ï¼Œå°±ä¼šè¢«kill | infinity |  |
| spark.dynamicAllocation.schedulerBacklogTimeout | ä»»åŠ¡é˜Ÿåˆ—éç©ºï¼Œèµ„æºä¸å¤Ÿï¼Œç”³è¯·executorçš„æ—¶é—´é—´éš” | 1s |  |
| spark.dynamicAllocation.sustainedSchedulerBacklogTimeout | åŒschedulerBacklogTimeoutï¼Œæ˜¯ç”³è¯·äº†æ–°executorä¹‹åç»§ç»­ç”³è¯·çš„é—´éš” | é»˜è®¤åŒschedulerBacklogTimeout |  |
| spark.executor.memory | executorå †å†…å­˜ | 1G | 9G |
| spark.executor.cores | executoræ‹¥æœ‰çš„coreæ•° | 1 | 3 |
| spark.locality.wait.process | è¿›ç¨‹å†…ç­‰å¾…æ—¶é—´ | 3 | 3 |
| spark.locality.wait.node | èŠ‚ç‚¹å†…ç­‰å¾…æ—¶é—´ | 3 | 8 |
| spark.locality.wait.rack | æœºæ¶å†…ç­‰å¾…æ—¶é—´ | 3 | 5 |
| spark.hadoop.mapreduce.input.fileinputformat.split.minsize | æ§åˆ¶è¾“å…¥æ–‡ä»¶å—çš„å¤§å°ï¼Œå½±å“å¹¶è¡Œåº¦ | 3 | 5 |
| spark.hadoop.mapreduce.input.fileinputformat.split.maxsize | æ§åˆ¶è¾“å…¥æ–‡ä»¶å—çš„å¤§å°ï¼Œå½±å“å¹¶è¡Œåº¦| 3 | 5 |
| spark.rpc.askTimeout | rpcè¶…æ—¶æ—¶é—´ | 10 | 1000 |
| spark.sql.autoBroadcastJoinThreshold | å°è¡¨éœ€è¦broadcastçš„å¤§å°é˜ˆå€¼ | 10485760 | 33554432 |
| spark.sql.hive.convertCTAS | åˆ›å»ºè¡¨æ˜¯å¦ä½¿ç”¨é»˜è®¤æ ¼å¼ | false | true |
| spark.sql.sources.default | é»˜è®¤æ•°æ®æºæ ¼å¼ | parquet | orc |
| spark.sql.files.openCostIlnBytes | å°æ–‡ä»¶åˆå¹¶é˜ˆå€¼ | 4194304 | 6291456 |
| spark.sql.orc.filterPushdown | orcæ ¼å¼è¡¨æ˜¯å¦è°“è¯ä¸‹æ¨ | false | true |
| spark.shuffle.sort.bypassMergeThreshold | shuffle read taské˜ˆå€¼ï¼Œå°äºè¯¥å€¼åˆ™shuffle writeè¿‡ç¨‹ä¸è¿›è¡Œæ’åº | 200 | 600 |
| spark.shuffle.io.retryWait | æ¯æ¬¡é‡è¯•æ‹‰å–æ•°æ®çš„ç­‰å¾…é—´éš” | 5 | 30 |
| spark.shuffle.io.maxRetries | æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•° | 3 | 10 |
| spark.shuffle.service.enabled | NodeManagerä¸­ä¸€ä¸ªé•¿æœŸè¿è¡Œçš„è¾…åŠ©æœåŠ¡ï¼Œç”¨äºæå‡Shuffleè®¡ç®—æ€§èƒ½ | false | true |
| spark.speculation | å¼€å¯æ¨æµ‹æ‰§è¡Œ | false | true |
| spark.speculation.quantile | ä»»åŠ¡å»¶è¿Ÿçš„æ¯”ä¾‹ï¼Œæ¯”å¦‚å½“70%çš„taskéƒ½å®Œæˆï¼Œé‚£ä¹ˆå–ä»–ä»¬è¿è¡Œæ—¶é—´çš„ä¸­ä½æ•°è·Ÿè¿˜æœªæ‰§è¡Œå®Œçš„ä»»åŠ¡ä½œå¯¹æ¯”ã€‚å¦‚æœè¶…è¿‡1.2å€ï¼Œåˆ™å¼€å¯æ¨æµ‹æ‰§è¡Œã€‚ |  | 0.7 |
| spark.speculation.multiplier | æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•° |  | 1.2 |
| spark.shuffle.io.maxRetries | æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•° | 3 | 10 |
| spark.shuffle.io.maxRetries | æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•° | 3 | 10 |
| spark.shuffle.io.maxRetries | æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•° | 3 | 10 |
| hive.merge.sparkfiles | è¾“å‡ºæ—¶æ˜¯å¦åˆå¹¶å°æ–‡ä»¶ | false | true |
| spark.speculation | æ˜¯å¦å¼€å¯æ¨æµ‹æ‰§è¡Œ | false |  |
| spark.app.name | è®¾ç½®sparkä»»åŠ¡å | Hive on Spark | |
>å¦‚æœéœ€è¦æŒ‡å®šsparkä½¿ç”¨çš„é˜Ÿåˆ—ï¼Œå¯ä»¥é€šè¿‡`set mapreduce.job.queuename=default`è¿›è¡Œè®¾ç½®ã€‚

**Sparkå†…å­˜ä½¿ç”¨è¯´æ˜ï¼š**
1. é€šè¿‡`spark.executor.memory`æŒ‡å®šçš„executorå€¼ä¸èƒ½å¤§äº`yarn.scheduler.maximum-allocation-mb`çš„å€¼ï¼Œå› ä¸ºexecutorè·‘åœ¨containerä¸­ï¼Œå¦‚æœè¶…è¿‡containerå†…å­˜ä¼šæŠ¥é”™å†…å­˜è¶Šç•Œã€‚å…·ä½“çš„è®¾ç½®è¿˜æ˜¯å¾—æ ¹æ®ä¸åŒéƒ¨é—¨çš„èµ„æºé˜Ÿåˆ—æ¥å®šï¼Œå¯ä»¥çœ‹çœ‹è‡ªå·±å›¢é˜Ÿçš„èµ„æºé˜Ÿåˆ—çš„æœ€å¤§å†…å­˜é™åˆ¶æ˜¯å¤šå°‘ï¼Œnum-executorsä¹˜ä»¥executor-memoryï¼Œæ˜¯ä¸èƒ½è¶…è¿‡é˜Ÿåˆ—çš„æœ€å¤§å†…å­˜é‡çš„1/3ã€‚
2. `spark.yarn.executor.memoryOverhead`ï¼šexecutoræ‰§è¡Œçš„æ—¶å€™ï¼Œç”¨çš„å†…å­˜å¯èƒ½ä¼šè¶…è¿‡executor-memoyï¼Œæ‰€ä»¥ä¼šä¸ºexecutoré¢å¤–é¢„ç•™ä¸€éƒ¨åˆ†å†…å­˜ï¼Œ`spark.yarn.executor.memoryOverhead`ä»£è¡¨äº†è¿™éƒ¨åˆ†å†…å­˜ï¼Œæ‰€ä»¥æŒ‡å®šçš„`spark.executor.memory+spark.yarn.executor.memoryOverhead`ä¸èƒ½å¤§äº`yarn.scheduler.maximum-allocation-mb`ã€‚
3. æ‰€ä»¥å½“æˆ‘ä»¬è®¾ç½®spark.executor.memoryä¸º6Gæ—¶ï¼Œç®—ä¸Šé¢„ç•™å†…å­˜ä¸€å…±ä½¿ç”¨å†…å­˜ä¸º6G*1.1ã€‚è€Œå®é™…ä½¿ç”¨äº†7168 MBï¼Œæ˜¯å› ä¸º**è§„æ•´åŒ–å› å­**ï¼Œå³æœ€ç»ˆä½¿ç”¨çš„å†…å­˜ä¸ºå®¹å™¨æœ€å°å†…å­˜`yarn.scheduler.minimum-allocation-mb`çš„æœ€å°æ•´æ•°å€ã€‚


**è¾“å…¥è¾“å‡ºæ–‡ä»¶è¯´æ˜ï¼š**
1. å¼€å¯org.apache.hadoop.hive.ql.io.CombineHiveInputFormatè€Œä¸é…ç½®åˆ†ç‰‡å¤§å°çš„å‚æ•°ï¼Œæ‰€æœ‰è¾“å…¥ä¼šåˆå¹¶ä¸ºä¸€ä¸ªæ–‡ä»¶ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ç®¡ä½ æ•°æ®å¤šå¤§ï¼Œåªæœ‰ä¸€ä¸ªMapã€‚å¯ä»¥é€šè¿‡æœ€æ–°çš„å‚æ•°
- set mapreduce.input.fileinputformat.split.maxsize=256000000;
- set mapreduce.input.fileinputformat.split.minsize=100000000;
æ¥è®¾ç½®æœ€ç»ˆåˆå¹¶åæ–‡ä»¶çš„å¤§å°èŒƒå›´ã€‚


**Sparkè¯»Hiveã€HDFSæ—¶çš„Taskæ•°é‡:**
sparkè¯»å–hdfsæ•°æ®æ—¶ï¼Œæœ€ç»ˆä¼šç”ŸæˆHadoopRDDï¼Œæ‰€ä»¥HadoopRDDçš„åˆ†åŒºæ•°é‡å°±æ˜¯taskæ•°é‡ã€‚
ä»¥sparkContext.textFileä¸ºä¾‹ï¼Œæ¥åˆ†æä¸‹HadoopRDDçš„åˆ†åŒºæ•°ã€‚
```
// æœ‰ä¸¤ä¸ªé‡è½½æ–¹æ³•ã€‚ä¸€ä¸ªè¦æ±‚æŒ‡å®šminPartitionsï¼Œå¦ä¸€ä¸ªä¸åšè¦æ±‚ã€‚
def textFile(path: String): JavaRDD[String] = sc.textFile(path)

// minPartitionsæ˜¯æŒ‡å¸Œæœ›è¿”å›çš„æœ€å°åˆ†åŒºæ•°ã€‚ä¹Ÿå°±è¯´è¿”å›çš„RDDåˆ†åŒºæ•°å¤§äºæˆ–è€…ç­‰äºminPartitionsã€‚
def textFile(path: String, minPartitions: Int): JavaRDD[String] =
    sc.textFile(path, minPartitions)
```
ä¸€è·¯è·Ÿè¿›ï¼Œæœ€ç»ˆä¼šè°ƒç”¨åˆ°org.apache.hadoop.mapred.FileInputFormatçš„getSplitsæ–¹æ³•ï¼Œè¿”å›çš„splitæ•°å°±æ˜¯rddåˆ†åŒºæ•°ã€‚

ä»¥ä¸‹æ˜¯getSplitsæ–¹æ³•çš„å®ç°é€»è¾‘ã€‚
```
    // numSplitsæ˜¯æœŸæœ›çš„åˆ†ç‰‡æ•°é‡
    public InputSplit[] getSplits(JobConf job, int numSplits) throws IOException {
        Stopwatch sw = new Stopwatch().start();
        // åˆ—å‡ºè¦è¯»å–çš„æ‰€æœ‰æ–‡ä»¶
        FileStatus[] files = listStatus(job);
        // è®¾ç½®è¾“å…¥æ–‡ä»¶çš„ä¸ªæ•°
        job.setLong(NUM_INPUT_FILES, files.length);
        // è®°å½•æ‰€æœ‰æ–‡ä»¶çš„æ€»å¤§å°ã€‚å­—èŠ‚
        long totalSize = 0; // compute total size

        // éå†æ‰€æœ‰æ–‡ä»¶
        for (FileStatus file : files) {// check we have valid files
            if (file.isDirectory()) {
                throw new IOException("Not a file: " + file.getPath());
            }
            // è®¡ç®—æ‰€æœ‰æ–‡ä»¶æ€»å¤§å°
            totalSize += file.getLen();
        }
        // ç®—å‡ºä¸€ä¸ªåˆ†ç‰‡çš„æœŸæœ›å¤§å°ã€‚å¦‚æœæŒ‡å®šçš„åˆ†ç‰‡ä¸º0ï¼Œåˆ™ä½¿ç”¨æ€»å¤§å°é™¤ä»¥1ï¼›å¦åˆ™ç”¨æ€»å¤§å°é™¤ä»¥æœŸæœ›çš„åˆ†ç‰‡ä¸ªæ•°
        long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
        // æœ€å°çš„ç‰‡å¤§å°ã€‚minSplitSize=1ï¼ŒSPLIT_MINSIZEé»˜è®¤ä¹Ÿä¸º1ï¼Œå–ä¸¤è€…ä¸­çš„æœ€å¤§å€¼ï¼›
        long minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.FileInputFormat.SPLIT_MINSIZE, 1), minSplitSize);
        // åˆ›å»ºæœ€ç»ˆçš„åˆ†ç‰‡åˆ—è¡¨
        ArrayList<FileSplit> splits = new ArrayList<FileSplit>(numSplits);
        NetworkTopology clusterMap = new NetworkTopology();
        // éå†æ‰€æœ‰æ–‡ä»¶ for (FileStatus file: files) {
        Path path = file.getPath();
        // è·å–è¿™ä¸ªæ–‡ä»¶å¤§å°
        long length = file.getLen();
        if (length != 0) {
            FileSystem fs = path.getFileSystem(job);
            // è·å–åˆ†ç‰‡ä½ç½®ä¿¡æ¯
            BlockLocation[] blkLocations;
            if (file instanceof LocatedFileStatus) {
                blkLocations = ((LocatedFileStatus) file).getBlockLocations();
            } else {
                blkLocations = fs.getFileBlockLocations(file, 0, length);
            }
            // å¦‚æœæ–‡ä»¶æ˜¯å¯åˆ‡åˆ†æ ¼å¼çš„
            if (isSplitable(fs, path)) {
                // è·å–æ–‡ä»¶å¤§å°
                long blockSize = file.getBlockSize();
                // è®¡ç®—å¾—å‡ºæœ€ç»ˆçš„åˆ†ç‰‡å¤§å°ã€‚
                // åœ¨goalSizeå’ŒblockSizeä¸­å–è¾ƒå°çš„å€¼ä½œä¸ºç»“æœï¼Œç„¶åå»å’ŒminSizeæ¯”å¤§å°ï¼Œå–è¾ƒå¤§çš„å€¼
                long splitSize = computeSplitSize(goalSize, minSize, blockSize);
                // å‰©ä½™çš„å­—èŠ‚
                long bytesRemaining = length;
                // SPLIT_SLOP =1.1ã€‚å¦‚æœå‰©ä½™çš„å­—èŠ‚æ•°é™¤ä»¥åˆ†ç‰‡å¤§å°å¤§äº1.1ï¼Œåˆ™ç»§ç»­ç”Ÿæˆåˆ†ç‰‡
                // å¾ªç¯ç”Ÿæˆåˆ†ç‰‡
                while (((double) bytesRemaining) / splitSize > SPLIT_SLOP) {
                    // è·å–åˆ†ç‰‡ä½ç½®
                    String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, splitSize, clusterMap);
                    // ç”Ÿæˆæ–°çš„åˆ†ç‰‡
                    splits.add(makeSplit(path, length - bytesRemaining, splitSize, splitHosts[0], splitHosts[1]));
                    // æ›´æ–°å‰©ä½™å­—èŠ‚æ•°
                    bytesRemaining -= splitSize;
                }
                // å¦‚æœè¿˜æœ‰å‰©ä¸‹çš„å­—èŠ‚ï¼Œç”Ÿæˆç‹¬ç«‹çš„åˆ†ç‰‡
                if (bytesRemaining != 0) {
                    String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length - bytesRemaining, bytesRemaining, clusterMap);
                    splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining, splitHosts[0], splitHosts[1]));
                }
            } else {
                // å¦‚æœæ–‡ä»¶ä¸å¯åˆ‡åˆ†ï¼Œåˆ™ç›´æ¥ç”Ÿæˆåˆ†ç‰‡ã€‚
                String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, 0, length, clusterMap);
                splits.add(makeSplit(path, 0, length, splitHosts[0], splitHosts[1]));
            }
        } else {
            // å¦‚æœæ–‡ä»¶ä¸ºç©ºï¼Œç›´æ¥ç”Ÿæˆç©ºçš„split
            // Create empty hosts array for zero length files
            splits.add(makeSplit(path, 0, length, new String[0]));
        }
    }
    sw.stop();
    if(LOG.isDebugEnabled()) {
        LOG.debug("Total # of splits generated by getSplits: " + splits.size() + ", TimeTaken: " + sw.elapsedMillis());
    }
    // è¿”å›ç”Ÿæˆçš„splitåˆ—è¡¨ 
     return splits.toArray(new FileSplit[splits.size()]);
}
```
**ç»“è®ºï¼š**
å…³é”®ç‚¹ï¼šæ˜¯å¦æŒ‡å®šäº†minPartitionsã€è¯»å–çš„æ–‡ä»¶æ˜¯å¦å¯åˆ‡åˆ†
- æ˜¯å¦å¯åˆ‡åˆ†ï¼šå¦‚æœæ–‡ä»¶æ²¡æœ‰å‹ç¼©æˆ–è€…ä½¿ç”¨çš„æ˜¯BZip2å‹ç¼©ï¼Œåˆ™æ–‡ä»¶å¯åˆ‡åˆ†ï¼›å¦‚æœæ˜¯å…¶å®ƒå‹ç¼©æ–¹å¼ï¼Œå¦‚gzipï¼Œæ–‡ä»¶å°±ä¸å¯åˆ‡åˆ†ã€‚
-  å¦‚æœæŒ‡å®šäº†minPartitionsï¼Œä¸”è¯»å–çš„æ–‡ä»¶å¯åˆ‡åˆ†ï¼Œåˆ™è¿”å›çš„åˆ†åŒºæ•°ä¼šå¤§äºç­‰äºminPartitionsã€‚ä¼šæ ¹æ®æ–‡ä»¶çš„æ€»å¤§å°å’ŒminPartitionsè®¡ç®—å‡ºæ¥æ¯ä¸ªåˆ†åŒºçš„æœŸæœ›æ–‡ä»¶å¤§å°ï¼Œæ¥ç”Ÿæˆåˆ†åŒºã€‚
- å¦‚æœæŒ‡å®šäº†minPartitionsï¼Œä¸”è¯»å–çš„æ–‡ä»¶ä¸å¯åˆ‡åˆ†ï¼Œè¿”å›çš„åˆ†åŒºæ•°å°±æ˜¯æ–‡ä»¶çš„ä¸ªæ•°
- å¦‚æœæ²¡æœ‰æŒ‡å®šminPartitionsï¼Œè¿”å›çš„åˆ†åŒºæ•°å°±æ˜¯æ–‡ä»¶çš„ä¸ªæ•°ã€‚

hiveä¹Ÿæ˜¯åŒç†ã€‚
å‚è€ƒ[https://blog.csdn.net/ifenggege/article/details/104581273](https://blog.csdn.net/ifenggege/article/details/104581273)


<br>
## 1.3 Hive-on-sparkè°ƒä¼˜æ¡ˆä¾‹
### 1.3.1 æ¡ˆä¾‹ä¸€
**æ•°æ®é‡ï¼š10g**
![image.png](Spark-Tezè°ƒä¼˜.assets\6c0bb69381e64a37bc7913e81328c0fa.png)

å¯ä»¥çœ‹å‡ºï¼š
- éšç€æ¯ä¸ªexecutorå ç”¨çš„CPU coreæ•°å¢åŠ ï¼Œq04æŸ¥è¯¢çš„æ—¶é—´æ˜¾è‘—ä¸‹é™ï¼Œq03ä¹Ÿä¸‹é™ï¼Œä½†å¹…åº¦æ²¡é‚£ä¹ˆå¤§ã€‚

æœ¬æ¬¡è°ƒä¼˜åªè®¾ç½®äº†spark.executor.memoryå’Œspark.executor.coresä¸¤ä¸ªå‚æ•°ï¼Œæ²¡æœ‰æ¶‰åŠåˆ°spark.executor.instanceså‚æ•°ï¼Œè€Œé»˜è®¤çš„spark.executor.instancesä¸º2ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªä½œä¸šåªç”¨åˆ°2ä¸ªexecutorï¼Œå› æ­¤è¿˜æ²¡å°†æ€§èƒ½å‘æŒ¥åˆ°æœ€ä½³ã€‚

æ¥ä¸‹æ¥é‡‡ç”¨100gçš„æ•°æ®é‡ï¼Œå¹¶ä¸”å¢åŠ spark.executor.instanceså‚æ•°çš„è®¾ç½®ã€‚

**æ•°æ®é‡ï¼š100g**
![image.png](Spark-Tezè°ƒä¼˜.assetsabaa483f2a5453a84531128bd4cb86f.png)

å¯ä»¥çœ‹å‡ºï¼š
- è°ƒä¼˜å‰åæŸ¥è¯¢æ—¶é—´æœ‰äº†å¾ˆå¤§çš„é£è·ƒï¼›
- å¢åŠ spark.executor.instancesè®¾ç½®é¡¹æŒ‡å®šæ¯ä¸ªä½œä¸šå ç”¨çš„executorä¸ªæ•°åæ€§èƒ½åˆæœ‰å¾ˆå¤§æå‡ï¼ˆé€šè¿‡ç›‘æ§æˆ‘ä»¬å‘ç°æ­¤æ—¶CPUåˆ©ç”¨ç‡å¹³å‡æœ‰å¥½å‡ åï¼Œç”šè‡³å¯ä»¥é«˜åˆ°ç™¾åˆ†ä¹‹ä¹åå‡ ï¼‰ï¼›

è‡³æ­¤ï¼Œæˆ‘ä»¬ç»ˆäºå°†æ•´ä¸ªé›†ç¾¤æ€§èƒ½å……åˆ†å‘æŒ¥å‡ºæ¥ï¼Œè¾¾åˆ°ç›®çš„ã€‚

æœ€åä¸€åˆ—é…ç½®é¡¹æ˜¯æ ¹æ®ç¾å›¢æŠ€æœ¯å›¢é˜Ÿåšå®¢çš„å»ºè®®è®¾ç½®çš„ï¼Œå¯ä»¥çœ‹å‡ºæ€§èƒ½ç›¸æ¯”æˆ‘ä»¬ä¹‹å‰è‡ªå·±çš„è®¾ç½®è¿˜æ˜¯æœ‰ä¸€å®šæå‡çš„ï¼Œè‡³å°‘è¯¥åšå®¢é‡Œå»ºè®®çš„è®¾ç½®æ˜¯æ¯”è¾ƒé€šç”¨çš„ï¼Œå› æ­¤ä¹‹åæˆ‘ä»¬éƒ½é‡‡å–æœ€åä¸€åˆ—çš„è®¾ç½®æ¥è·‘TPCx-BBæµ‹è¯•ã€‚

æœ€åæ¥å¼ å¤§å›¾å±•ç¤ºè°ƒä¼˜å‰å’Œè°ƒä¼˜åè·‘100gæ•°æ®çš„å¯¹æ¯”ï¼š
![image.png](Spark-Tezè°ƒä¼˜.assets\5ea9981d9c00467ca63bd55d0bdfe5e4.png)

å¯ä»¥çœ‹å‡ºï¼š

ç»å¤§å¤šæ•°æŸ¥è¯¢è°ƒä¼˜å‰åæŸ¥è¯¢æ—¶é—´æœ‰äº†æå¤§çš„é£è·ƒï¼›

ä½†æ˜¯åƒq01/q04/q14â€¦è¿™å‡ ä¸ªæŸ¥è¯¢ï¼Œå¯èƒ½å› ä¸ºæŸ¥è¯¢æ¶‰åŠåˆ°çš„è¡¨æ¯”è¾ƒå°ï¼Œè°ƒä¼˜å‰æ—¶é—´å°±å¾ˆçŸ­ï¼Œå› æ­¤è°ƒä¼˜åä¹Ÿçœ‹ä¸å‡ºå¾ˆå¤šå·®åˆ«ï¼Œå¦‚æœæƒ³çœ‹åˆ°å¤§çš„å·®åˆ«ï¼Œå¯èƒ½éœ€è¦æé«˜æ•°æ®é‡ï¼Œæ¯”å¦‚1Tï¼Œ3Tï¼›

q10å’Œq18è°ƒä¼˜å‰åæ—¶é—´éƒ½è¾ƒé•¿ï¼Œè€Œä¸”è°ƒä¼˜åæ€§èƒ½æ²¡æœ‰æå‡ï¼Œéœ€è¦å†æ·±å…¥æ¢ç´¢ä¸‹æ˜¯ä»€ä¹ˆåŸå› ã€‚

æœ€åï¼Œç”¨è°ƒä¼˜åçš„é›†ç¾¤ï¼Œåˆ†åˆ«è·‘10gã€30gã€100gçš„æ•°æ®ï¼Œç»“æœå¦‚ä¸‹ï¼š
![image.png](Spark-Tezè°ƒä¼˜.assets\4dfd60fa1b024adc87024c193ab12c98.png)

å¯ä»¥çœ‹å‡ºï¼š
- éšç€æ•°æ®é‡å¢å¤§ï¼Œå¾ˆå¤šæŸ¥è¯¢æ—¶é—´å¹¶æ²¡æœ‰æ˜æ˜¾å¢åŠ ï¼Œå¯èƒ½æ˜¯å› ä¸ºé›†ç¾¤æ€§èƒ½å¤ªå¼ºï¼Œè€Œä¸”æ•°æ®é‡è¿˜ä¸å¤Ÿå¤§ï¼Œå¯ä»¥å¢å¤§æ•°æ®é‡ç»§ç»­è§‚å¯Ÿ
- å¯¹äºq10ã€q18å’Œq30ï¼Œéšç€æ•°æ®é‡å¢å¤§ï¼Œæ—¶é—´æ˜æ˜¾å¢å¤§ï¼Œéœ€å†æ·±å…¥åˆ†æ


### 1.3.2 æ¡ˆä¾‹äºŒï¼šGroupByå¯¼è‡´çš„æ•°æ®å€¾æ–œä¼˜åŒ–
#### æ•°æ®å€¾æ–œé—®é¢˜
é€šå¸¸æ˜¯æŒ‡å‚ä¸è®¡ç®—çš„æ•°æ®åˆ†å¸ƒä¸å‡ï¼Œå³æŸä¸ªkeyæˆ–è€…æŸäº›keyçš„æ•°æ®é‡è¿œè¶…å…¶ä»–keyï¼Œå¯¼è‡´åœ¨shuffleé˜¶æ®µï¼Œå¤§é‡ç›¸åŒkeyçš„æ•°æ®è¢«å‘å¾€ä¸€ä¸ªReduceï¼Œè¿›è€Œå¯¼è‡´è¯¥Reduceæ‰€éœ€çš„æ—¶é—´è¿œè¶…å…¶ä»–Reduceï¼Œæˆä¸ºæ•´ä¸ªä»»åŠ¡çš„ç“¶é¢ˆã€‚
Hiveä¸­çš„æ•°æ®å€¾æ–œå¸¸å‡ºç°åœ¨åˆ†ç»„èšåˆå’Œjoinæ“ä½œçš„åœºæ™¯ä¸­ï¼Œä¸‹é¢åˆ†åˆ«ä»‹ç»åœ¨ä¸Šè¿°ä¸¤ç§åœºæ™¯ä¸‹çš„ä¼˜åŒ–æ€è·¯ã€‚

**ç¤ºä¾‹SQLè¯­å¥å¦‚ä¸‹**
```
select
    province_id,
    count(*)
from dwd_trade_order_detail_inc
where dt='2020-06-16'
group by province_id;
```
**ä¼˜åŒ–å‰çš„æ‰§è¡Œè®¡åˆ’**
![image.png](Spark-Tezè°ƒä¼˜.assets\32617fd55b04430c821ac738e4eaf758.png)

<br>
#### ä¼˜åŒ–æ€è·¯
ç”±åˆ†ç»„èšåˆå¯¼è‡´çš„æ•°æ®å€¾æ–œé—®é¢˜ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç§ä¼˜åŒ–æ€è·¯ï¼š
**1ï¼‰å¯ç”¨map-sideèšåˆ**
- 1. å¯ç”¨map-sideèšåˆ `set hive.map.aggr=true;`
- 2. hash mapå ç”¨mapç«¯å†…å­˜çš„æœ€å¤§æ¯”ä¾‹ `set hive.map.aggr.hash.percentmemory=0.5;`

å¯ç”¨map-sideèšåˆåçš„æ‰§è¡Œè®¡åˆ’å¦‚ä¸‹å›¾æ‰€ç¤º
![image.png](Spark-Tezè°ƒä¼˜.assets\73261bed766b48538f2573d11c746ee7.png)

**2ï¼‰å¯ç”¨skew groupbyä¼˜åŒ–**
å…¶åŸç†æ˜¯å¯åŠ¨ä¸¤ä¸ªMRä»»åŠ¡ï¼Œç¬¬ä¸€ä¸ªMRæŒ‰ç…§éšæœºæ•°åˆ†åŒºï¼Œå°†æ•°æ®åˆ†æ•£å‘é€åˆ°Reduceï¼Œå®Œæˆéƒ¨åˆ†èšåˆï¼Œç¬¬äºŒä¸ªMRæŒ‰ç…§åˆ†ç»„å­—æ®µåˆ†åŒºï¼Œå®Œæˆæœ€ç»ˆèšåˆã€‚
ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼š
- å¯ç”¨åˆ†ç»„èšåˆæ•°æ®å€¾æ–œä¼˜åŒ–
`set hive.groupby.skewindata=true;`

å¯ç”¨skew groupbyä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’å¦‚ä¸‹å›¾æ‰€ç¤º
![image.png](Spark-Tezè°ƒä¼˜.assets\41e29e53a65f4cde9a80c813525ba68f.png)

### 1.3.3 æ¡ˆä¾‹ä¸‰ï¼šJOINå¯¼è‡´çš„æ•°æ®å€¾æ–œä¼˜åŒ–
**ç¤ºä¾‹SQLå¦‚ä¸‹**
```
select
    *
from
(
    select
        *
    from dwd_trade_order_detail_inc
    where dt='2020-06-16'
)fact
join
(
    select
        *
    from dim_province_full
    where dt='2020-06-16'
)dim
on fact.province_id=dim.id;
```
**ä¼˜åŒ–å‰çš„æ‰§è¡Œè®¡åˆ’**
![image.png](Spark-Tezè°ƒä¼˜.assets5727bfca68e4938a8406b93e960ae0b.png)

#### ä¼˜åŒ–æ€è·¯
ç”±joinå¯¼è‡´çš„æ•°æ®å€¾æ–œé—®é¢˜ä¸»è¦æœ‰ä»¥ä¸‹ä¸¤ç§ä¼˜åŒ–æ€è·¯ï¼š
**1ï¼‰ä½¿ç”¨map joinä¼˜åŒ–**
ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼š
- å¯ç”¨map joinè‡ªåŠ¨è½¬æ¢
`set hive.auto.convert.join=true;`
- common joinè½¬map joinå°è¡¨é˜ˆå€¼
`set hive.auto.convert.join.noconditionaltask.size`

ä½¿ç”¨map joinä¼˜åŒ–åæ‰§è¡Œè®¡åˆ’å¦‚ä¸‹å›¾
![image.png](Spark-Tezè°ƒä¼˜.assetsc80b1dce354297ba115d056918a504.png)

**2ï¼‰å¯ç”¨skew joinä¼˜åŒ–**
å…¶åŸç†å¦‚ä¸‹å›¾
![image.png](Spark-Tezè°ƒä¼˜.assetsfaf784b2c3f4028ba2d5d2fa006edfe.png)

ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼š
- å¯ç”¨skew joinä¼˜åŒ–
`set hive.optimize.skewjoin=true;`
- è§¦å‘skew joinçš„é˜ˆå€¼ï¼Œè‹¥æŸä¸ªkeyçš„è¡Œæ•°è¶…è¿‡è¯¥å‚æ•°å€¼ï¼Œåˆ™è§¦å‘
`set hive.skewjoin.key=100000;`
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ**skew joinåªæ”¯æŒInner Join**ã€‚

å¯åŠ¨skew joinä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](Spark-Tezè°ƒä¼˜.assets441d3fcd24e4456ad16395e1d5847ba.png)


<br>
## 1.4 å¸¸ç”¨é…ç½®
### 1.4.1 ä»»åŠ¡å¹¶è¡Œåº¦ä¼˜åŒ–
**ä¼˜åŒ–è¯´æ˜**
å¯¹äºä¸€ä¸ªåˆ†å¸ƒå¼çš„è®¡ç®—ä»»åŠ¡è€Œè¨€ï¼Œè®¾ç½®ä¸€ä¸ªåˆé€‚çš„å¹¶è¡Œåº¦ååˆ†é‡è¦ã€‚åœ¨Hiveä¸­ï¼Œæ— è®ºå…¶è®¡ç®—å¼•æ“æ˜¯ä»€ä¹ˆï¼Œæ‰€æœ‰çš„è®¡ç®—ä»»åŠ¡éƒ½å¯åˆ†ä¸ºMapé˜¶æ®µå’ŒReduceé˜¶æ®µã€‚æ‰€ä»¥å¹¶è¡Œåº¦çš„è°ƒæ•´ï¼Œä¹Ÿå¯ä»ä¸Šè¿°ä¸¤ä¸ªæ–¹é¢è¿›è¡Œè°ƒæ•´ã€‚

#### 1.4.1.1 Mapé˜¶æ®µå¹¶è¡Œåº¦
Mapç«¯çš„å¹¶è¡Œåº¦ï¼Œä¹Ÿå°±æ˜¯Mapçš„ä¸ªæ•°ã€‚æ˜¯ç”±è¾“å…¥æ–‡ä»¶çš„åˆ‡ç‰‡æ•°å†³å®šçš„ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒMapç«¯çš„å¹¶è¡Œåº¦æ— éœ€æ‰‹åŠ¨è°ƒæ•´ã€‚Mapç«¯çš„å¹¶è¡Œåº¦ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼š
- å¯å°†å¤šä¸ªå°æ–‡ä»¶åˆ‡ç‰‡ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªåˆ‡ç‰‡ï¼Œè¿›è€Œç”±ä¸€ä¸ªmapä»»åŠ¡å¤„ç†
`set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;`

- ä¸€ä¸ªåˆ‡ç‰‡çš„æœ€å¤§å€¼
`set mapreduce.input.fileinputformat.split.maxsize=256000000;`

#### 1.4.1.2 Reduceé˜¶æ®µå¹¶è¡Œåº¦

Reduceç«¯çš„å¹¶è¡Œåº¦ï¼Œç›¸å¯¹æ¥è¯´ï¼Œæ›´éœ€è¦å…³æ³¨ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒHiveä¼šæ ¹æ®Reduceç«¯è¾“å…¥æ•°æ®çš„å¤§å°ï¼Œä¼°ç®—ä¸€ä¸ªReduceå¹¶è¡Œåº¦ã€‚ä½†æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå…¶ä¼°è®¡å€¼ä¸ä¸€å®šæ˜¯æœ€åˆé€‚çš„ï¼Œæ•…éœ€è¦äººä¸ºè°ƒæ•´å…¶å¹¶è¡Œåº¦ã€‚

Reduceå¹¶è¡Œåº¦ç›¸å…³å‚æ•°å¦‚ä¸‹ï¼š
- æŒ‡å®šReduceç«¯å¹¶è¡Œåº¦ï¼Œé»˜è®¤å€¼ä¸º-1ï¼Œè¡¨ç¤ºç”¨æˆ·æœªæŒ‡å®š
`set mapreduce.job.reduces;`
- Reduceç«¯å¹¶è¡Œåº¦æœ€å¤§å€¼
`set hive.exec.reducers.max;`
- å•ä¸ªReduceÂ Taskè®¡ç®—çš„æ•°æ®é‡ï¼Œç”¨äºä¼°ç®—Reduceå¹¶è¡Œåº¦
`set hive.exec.reducers.bytes.per.reducer;`

Reduceç«¯å¹¶è¡Œåº¦çš„ç¡®å®šé€»è¾‘ä¸ºï¼Œè‹¥æŒ‡å®šå‚æ•°mapreduce.job.reducesçš„å€¼ä¸ºä¸€ä¸ªéè´Ÿæ•´æ•°ï¼Œåˆ™Reduceå¹¶è¡Œåº¦ä¸ºæŒ‡å®šå€¼ã€‚å¦åˆ™ï¼ŒHiveä¼šè‡ªè¡Œä¼°ç®—Reduceå¹¶è¡Œåº¦ï¼Œä¼°ç®—é€»è¾‘å¦‚ä¸‹ï¼š
å‡è®¾Reduceç«¯è¾“å…¥çš„æ•°æ®é‡å¤§å°ä¸ºtotalInputBytes
å‚æ•°hive.exec.reducers.bytes.per.reducerçš„å€¼ä¸ºbytesPerReducer
å‚æ•°hive.exec.reducers.maxçš„å€¼ä¸ºmaxReducers
åˆ™Reduceç«¯çš„å¹¶è¡Œåº¦ä¸ºï¼š
![image.png](Spark-Tezè°ƒä¼˜.assets\d30fa7f09dca412994ba22aa62a3bb82.png)

å…¶ä¸­ï¼ŒReduceç«¯è¾“å…¥çš„æ•°æ®é‡å¤§å°ï¼Œæ˜¯ä»Reduceä¸Šæ¸¸çš„Operatorçš„Statisticsï¼ˆç»Ÿè®¡ä¿¡æ¯ï¼‰ä¸­è·å–çš„ã€‚ä¸ºä¿è¯Hiveèƒ½è·å¾—å‡†ç¡®çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œéœ€é…ç½®å¦‚ä¸‹å‚æ•°ï¼š
- æ‰§è¡ŒDMLè¯­å¥æ—¶ï¼Œæ”¶é›†è¡¨çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
`setÂ hive.stats.autogather=true;`
- æ‰§è¡ŒDMLè¯­å¥æ—¶ï¼Œæ”¶é›†å­—æ®µçº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
`set hive.stats.column.autogather=true;`
- è®¡ç®—Reduceå¹¶è¡Œåº¦æ—¶ï¼Œä»ä¸Šæ¸¸Operatorç»Ÿè®¡ä¿¡æ¯è·å¾—è¾“å…¥æ•°æ®é‡
`set hive.spark.use.op.stats=true;`
- è®¡ç®—Reduceå¹¶è¡Œåº¦æ—¶ï¼Œä½¿ç”¨åˆ—çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ä¼°ç®—è¾“å…¥æ•°æ®é‡
`set hive.stats.fetch.column.stats=true;`

### 1.4.2 å°æ–‡ä»¶åˆå¹¶ä¼˜åŒ–
#### 1.4.2.1 ä¼˜åŒ–è¯´æ˜
å°æ–‡ä»¶åˆå¹¶ä¼˜åŒ–ï¼Œåˆ†ä¸ºä¸¤ä¸ªæ–¹é¢ï¼Œåˆ†åˆ«æ˜¯Mapç«¯è¾“å…¥çš„å°æ–‡ä»¶åˆå¹¶ï¼Œå’ŒReduceç«¯è¾“å‡ºçš„å°æ–‡ä»¶åˆå¹¶ã€‚
#### 1.4.2.2 Mapç«¯è¾“å…¥æ–‡ä»¶åˆå¹¶
åˆå¹¶Mapç«¯è¾“å…¥çš„å°æ–‡ä»¶ï¼Œæ˜¯æŒ‡å°†å¤šä¸ªå°æ–‡ä»¶åˆ’åˆ†åˆ°ä¸€ä¸ªåˆ‡ç‰‡ä¸­ï¼Œè¿›è€Œç”±ä¸€ä¸ªMap Taskå»å¤„ç†ã€‚ç›®çš„æ˜¯é˜²æ­¢ä¸ºå•ä¸ªå°æ–‡ä»¶å¯åŠ¨ä¸€ä¸ªMap Taskï¼Œæµªè´¹è®¡ç®—èµ„æºã€‚
ç›¸å…³å‚æ•°ä¸ºï¼š
- å¯å°†å¤šä¸ªå°æ–‡ä»¶åˆ‡ç‰‡ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªåˆ‡ç‰‡ï¼Œè¿›è€Œç”±ä¸€ä¸ªmapä»»åŠ¡å¤„ç†
`set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; `

#### 1.4.2.3 Reduceè¾“å‡ºæ–‡ä»¶åˆå¹¶
åˆå¹¶Reduceç«¯è¾“å‡ºçš„å°æ–‡ä»¶ï¼Œæ˜¯æŒ‡å°†å¤šä¸ªå°æ–‡ä»¶åˆå¹¶æˆå¤§æ–‡ä»¶ã€‚ç›®çš„æ˜¯å‡å°‘HDFSå°æ–‡ä»¶æ•°é‡ã€‚
ç›¸å…³å‚æ•°ä¸ºï¼š
- å¼€å¯åˆå¹¶Hive on Sparkä»»åŠ¡è¾“å‡ºçš„å°æ–‡ä»¶
`set hive.merge.sparkfiles=true;`


#### 1.4.2.4 ä»HDFSè¯»å–æ•°æ®çš„ä»»åŠ¡åˆ‡åˆ†
Sparkä»HDFSä¸­è¯»å–æ•°æ®ï¼Œä¼šå…ˆåˆå¹¶å°æ–‡ä»¶ï¼Œç„¶åè¿›è¡Œå¯¼å…¥ï¼Œé»˜è®¤æŒ‰Blockå¤§å°åˆ‡åˆ†ä¸ºå¤šä¸ªHadoopRDDï¼Œæ¯ä¸ªRDDå¯¹åº”ä¸€ä¸ªtaskã€‚å¦‚æœå¸Œæœ›èƒ½åˆ‡åˆ†æ›´å¤štaskï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹è®¾ç½®é…ç½®æ¯ä¸ªåˆ†å—å¤§å°ï¼Œå•ä½ä¸ºByteã€‚
```
set mapreduce.input.fileinputformat.split.maxsize=10000000;
```

## 1.4.3 Sparkæ‰§è¡Œèµ„æºé…ç½®
hive on sparkå‚æ•°é…ç½®ï¼ˆé™æ€åˆ†é…instanceï¼‰
```sql
set spark.app.name='${filename}';
set mapreduce.job.queuename=job;

set hive.execution.engine=spark;
set spark.executor.instances=4;
set spark.executor.cores=3;
set spark.executor.memory=6G;
set spark.driver.memory=819m;
set spark.driver.memoryOverhead=205m;
set spark.default.parallelism=30;
set spark.sql.shuffle.partitions=30;
set mapreduce.input.fileinputformat.split.maxsize=10000000;
set spark.serializer=org.apache.spark.serializer.KryoSerializer;
set hive.merge.sparkfiles=true;
```

hive on sparkå‚æ•°é…ç½®ï¼ˆåŠ¨æ€åˆ†é…instanceï¼‰
```sql
-- å¯åŠ¨åŠ¨æ€åˆ†é…
set spark.dynamicAllocation.enabled=true;
-- å¯ç”¨Spark shuffleæœåŠ¡
set spark.shuffle.service.enabled=true;
-- Executorä¸ªæ•°åˆå§‹å€¼
set spark.dynamicAllocation.initialExecutors=1;
-- Executorä¸ªæ•°æœ€å°å€¼
set spark.dynamicAllocation.minExecutors=1;
-- Executorä¸ªæ•°æœ€å¤§å€¼
set spark.dynamicAllocation.maxExecutors=12;
-- Executorç©ºé—²æ—¶é•¿ï¼Œè‹¥æŸExecutorç©ºé—²æ—¶é—´è¶…è¿‡æ­¤å€¼ï¼Œåˆ™ä¼šè¢«å…³é—­
set spark.dynamicAllocation.executorIdleTimeout=60s;
-- ç§¯å‹ä»»åŠ¡ç­‰å¾…æ—¶é•¿ï¼Œè‹¥æœ‰Taskç­‰å¾…æ—¶é—´è¶…è¿‡æ­¤å€¼ï¼Œåˆ™ç”³è¯·å¯åŠ¨æ–°çš„Executor
set spark.dynamicAllocation.schedulerBacklogTimeout=1s;
set spark.shuffle.useOldFetchProtocol=true;
```
>è¯´æ˜ï¼šSpark shuffleæœåŠ¡çš„ä½œç”¨æ˜¯ç®¡ç†Executorä¸­çš„å„Taskçš„è¾“å‡ºæ–‡ä»¶ï¼Œä¸»è¦æ˜¯shuffleè¿‡ç¨‹mapç«¯çš„è¾“å‡ºæ–‡ä»¶ã€‚ç”±äºå¯ç”¨èµ„æºåŠ¨æ€åˆ†é…åï¼ŒSparkä¼šåœ¨ä¸€ä¸ªåº”ç”¨æœªç»“æŸå‰ï¼Œå°†å·²ç»å®Œæˆä»»åŠ¡ï¼Œå¤„äºç©ºé—²çŠ¶æ€çš„Executorå…³é—­ã€‚Executorå…³é—­åï¼Œå…¶è¾“å‡ºçš„æ–‡ä»¶ï¼Œä¹Ÿå°±æ— æ³•ä¾›å…¶ä»–Executorä½¿ç”¨äº†ã€‚éœ€è¦å¯ç”¨Spark shuffleæœåŠ¡ï¼Œæ¥ç®¡ç†å„Executorè¾“å‡ºçš„æ–‡ä»¶ï¼Œè¿™æ ·å°±èƒ½å…³é—­ç©ºé—²çš„Executorï¼Œè€Œä¸å½±å“åç»­çš„è®¡ç®—ä»»åŠ¡äº†ã€‚

<br>
### 1.4.4 hive on sparkçš„è¶…æ—¶æ—¶é—´é…ç½®
```
    <!-- Spark driverè¿æ¥Hive clientçš„è¶…æ—¶æ—¶é—´,é»˜è®¤å•ä½æ¯«ç§’ -->
    <property>
        <name>hive.spark.client.connect.timeout</name>
        <value>600000ms</value>
    </property>
    <!-- Hive clientå’Œè¿œç¨‹Spark clientæ¡æ‰‹çš„è¶…æ—¶æ—¶é—´,é»˜è®¤å•ä½æ¯«ç§’ -->
    <property>
        <name>hive.spark.client.server.connect.timeout</name>
        <value>600000ms</value>
    </property>
    <!-- Hive clientè¯·æ±‚Spark driverçš„è¶…æ—¶æ—¶é—´,é»˜è®¤å•ä½ç§’ -->
    <property>
        <name>hive.spark.client.future.timeout</name>
        <value>600s</value>
    </property>
    <!-- Jobç›‘æ§è·å–Sparkä½œä¸šçŠ¶æ€çš„è¶…æ—¶æ—¶é—´,é»˜è®¤å•ä½ç§’ -->
    <property>
        <name>hive.spark.client.future.timeout</name>
        <value>600s</value>
    </property>
```

### 1.4.5 å…¶ä»–ä¼˜åŒ–
å‚è€ƒèµ„æ–™ï¼š
1.[https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning](https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hos_tuning.html#hos_tuning)
2.https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started)


<br>
# äºŒã€Tezè°ƒä¼˜
è¯¦ç»†é…ç½®å¯æŸ¥çœ‹å®˜ç½‘çš„[Tezé…ç½®](https://tez.apache.org/releases/0.8.4/tez-api-javadocs/configs/TezConfiguration.html)å’Œ[TezRuntimeé…ç½®](https://tez.apache.org/releases/0.10.1/tez-api-javadocs/configs/TezConfiguration.html)

- **tez.runtime.io.sort.mb**æ˜¯å½“éœ€è¦å¯¹è¾“å‡ºè¿›è¡Œæ’åºçš„å†…å­˜ã€‚
- **tez.runtime.unordered.output.buffer.size-mb**æ˜¯è¾“å‡ºä¸éœ€è¦æ’åºçš„å†…å­˜ã€‚
- **hive.auto.convert.join.noconditionaltask.size**æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ï¼Œç”¨äºè®¾ç½®æ‰§è¡ŒMap joinæ—¶çš„å†…å­˜å¤§å°ã€‚
- **tez.am.resource.memory.mb**è®¾ç½®ä¸ºä¸**yarn.scheduler.minimum-allocate-mb** YARNæœ€å°å®¹å™¨å¤§å°ç›¸åŒã€‚ 
- **hive.tez.container.size**è®¾ç½®ä¸ºä¸**yarn.scheduler.minimum-allocation-mb**å¤§å°ç›¸åŒæˆ–å°å€æ•°(1æˆ–2å€)ï¼Œä½†ä¸èƒ½è¶…è¿‡**yarn.scheduler.maximum-allocation-mb**ã€‚
- **tez.runtime.io.sort.mb**ä¸º**hive.tez.container.size**çš„40%ï¼Œä¸åº”è¯¥è¶…è¿‡2gbã€‚
- **hive.auto.convert.join.noconditionaltask.size**ä¸º**hive.tez.container.size**çš„1/3
- **tez.runtime.unordered.output.buffer.size-mb**ä¸º**hive.tez.container.size**çš„10%

**1. AMã€Containerå¤§å°è®¾ç½®**
- tez.am.resource.memory.mbã€€ã€€#è®¾ç½® tez AMå®¹å™¨å†…å­˜
ã€€ã€€é»˜è®¤å€¼ï¼š1024ã€€ã€€
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-site.xml
ã€€ã€€å»ºè®®ï¼šç­‰äºyarn.scheduler.minimum-allocation-mbå€¼ã€‚
ã€€ã€€
>**Noteï¼š**é‡åˆ°è¿‡tezæ‰§è¡Œæ•°æ®é‡ç‰¹åˆ«å¤§çš„ä»»åŠ¡æ—¶ï¼Œä¸€ç›´å¡åœ¨Scheduledé˜¶æ®µçš„æƒ…å†µã€‚éœ€è¦è°ƒæ•´amçš„å¤§å°
set tez.am.resource.memory.mb=5120;
set tez.am.launch.cmd-opts=-Xms5120m -Xmx5120m;

- hive.tez.container.sizeã€€ã€€#è®¾ç½® tez containerå†…å­˜
ã€€ã€€é»˜è®¤å€¼ï¼š-1
ã€€ã€€é»˜è®¤æƒ…å†µä¸‹ï¼ŒTezå°†ç”Ÿæˆä¸€ä¸ªmapperå¤§å°çš„å®¹å™¨ã€‚è¿™å¯ä»¥ç”¨æ¥è¦†ç›–é»˜è®¤å€¼ã€‚
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml
ã€€ã€€å»ºè®®ï¼šç­‰äºæˆ–å‡ å€äºyarn.scheduler.minimum-allocation-mbå€¼ï¼Œä½†æ˜¯ä¸èƒ½å¤§äºyarn.scheduler.maximum-allocation-mbã€‚

<br>
**2. AMã€Container JVMå‚æ•°è®¾ç½®**
- tez.am.launch.cmd-optsã€€ã€€#è®¾ç½® AM jvmï¼Œå¯åŠ¨TEZä»»åŠ¡è¿›ç¨‹æœŸé—´æä¾›çš„å‘½ä»¤è¡Œé€‰é¡¹ã€‚
ã€€ã€€é»˜è®¤å€¼ï¼š-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC(ç”¨äºGC)ï¼Œé»˜è®¤çš„å¤§å°ï¼š80%*tez.am.resource.memory.mb
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-site.xml
ã€€ã€€å»ºè®®ï¼šä¸è¦åœ¨è¿™äº›å¯åŠ¨é€‰é¡¹ä¸­è®¾ç½®ä»»ä½•xmxæˆ–xmsï¼Œä»¥ä¾¿tezå¯ä»¥è‡ªåŠ¨ç¡®å®šå®ƒä»¬ã€‚
ã€€ã€€
- tez.task.launch.cmd-opts
   é»˜è®¤å€¼ï¼šå¯åŠ¨çš„JVMå‚æ•° ï¼Œé»˜è®¤-XX:+PrintGCDetails -verbose:gc -XX:+PrintGCTimeStamps -XX:+UseNUMA -XX:+UseParallelGC

- tez.container.max.java.heap.fraction
   è¯´æ˜ï¼šåŸºäºyarnæä¾›çš„å†…å­˜ï¼Œåˆ†é…ç»™javaè¿›ç¨‹çš„ç™¾åˆ†æ¯”ï¼Œé»˜è®¤æ˜¯0.8ï¼Œå…·ä½“å¤§å°å–å†³äºmapreduce.reduce.memory.mbå’Œmapreduce.map.memory.mbã€‚ä¸€èˆ¬ä¸ç”¨å˜å³å¯


- hive.tez.java.opsã€€ã€€#è®¾ç½® container jvm
ã€€ã€€é»˜è®¤å€¼ï¼šHortonworkså»ºè®®â€œâ€“server â€“Djava.net.preferIPv4Stack=trueâ€“XX:NewRatio=8 â€“XX:+UseNUMA â€“XX:UseG1Gâ€ï¼Œé»˜è®¤å¤§å°ï¼š80%*hive.tez.container.size
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml
ã€€ã€€å»ºè®®ï¼šHortonworkså»ºè®®â€œâ€“server â€“Djava.net.preferIPv4Stack=trueâ€“XX:NewRatio=8 â€“XX:+UseNUMA â€“XX:UseG1Gâ€

- tez.container.max.java.heap.fractionã€€ã€€#è®¾ç½®task/AMå ç”¨jvmå†…å­˜å¤§å°çš„æ¯”ä¾‹ã€‚
ã€€ã€€é»˜è®¤å€¼ï¼š0.8
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-site.xml
ã€€ã€€è¯´æ˜ï¼štask\AMå ç”¨JVM Xmxçš„æ¯”ä¾‹ï¼Œè¿™ä¸ªå€¼æŒ‰å…·ä½“éœ€è¦è°ƒæ•´ï¼Œå½“å†…å­˜ä¸è¶³æ—¶ï¼Œä¸€èˆ¬éƒ½è¦è°ƒå°ã€‚

<br>
**3. Hiveå†…å­˜Map Joinå‚æ•°è®¾ç½®**
- tez.runtime.io.sort.mbã€€ã€€#è®¾ç½®è¾“å‡ºæ’åºå†…å­˜å¤§å°
ã€€ã€€é»˜è®¤å€¼ï¼š100
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-site.xml
ã€€ã€€å»ºè®®ï¼š40%*hive.tez.container.sizeï¼Œä¸€èˆ¬ä¸è¶…è¿‡2G


- hive.auto.convert.join  #æ˜¯å¦è‡ªåŠ¨è½¬æ¢ä¸ºmapjoin
ã€€ã€€é»˜è®¤å€¼ï¼štrue
ã€€ã€€å»ºè®®ä½¿ç”¨é»˜è®¤å€¼ã€‚
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml

- hive.auto.convert.join.noconditionaltaskã€€ã€€#æ˜¯å¦å°†å¤šä¸ªmapjoinåˆå¹¶ä¸ºä¸€ä¸ª
ã€€ã€€é»˜è®¤å€¼ï¼štrue
ã€€ã€€å»ºè®®ä½¿ç”¨é»˜è®¤å€¼ã€‚
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml

- hive.auto.convert.join.noconditionaltask.size  
ã€€ã€€é»˜è®¤å€¼ï¼š10000000ã€€ã€€(10M)
ã€€ã€€è¯´æ˜ï¼šè¿™ä¸ªå‚æ•°ä½¿ç”¨çš„å‰ææ˜¯hive.auto.convert.join.noconditionaltaskå€¼ä¸ºtrueï¼Œå¤šä¸ªmapjoinè½¬æ¢ä¸º1ä¸ªæ—¶ï¼Œæ‰€æœ‰å°è¡¨çš„æ–‡ä»¶å¤§å°æ€»å’Œå°äºè¿™ä¸ªå€¼ï¼Œè¿™ä¸ªå€¼åªæ˜¯é™åˆ¶è¾“å…¥çš„è¡¨æ–‡ä»¶çš„å¤§å°ï¼Œå¹¶ä¸ä»£è¡¨å®é™…mapjoinæ—¶hashtableçš„å¤§å°ã€‚ å»ºè®®å€¼ï¼š1/3* hive.tez.container.size
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml

- tez.runtime.unordered.output.buffer.size-mbã€€ã€€#å¦‚æœä¸ç›´æ¥å†™å…¥ç£ç›˜ï¼Œä½¿ç”¨çš„ç¼“å†²åŒºå¤§å°. Size of the buffer to use if not writing directly to disk.
ã€€ã€€é»˜è®¤å€¼ï¼š100M
 ã€€å»ºè®®ï¼š10%* hive.tez.container.size
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-site.xml

- tez.am.container.reuse.enabledã€€ã€€#å®¹å™¨é‡ç”¨
ã€€ã€€é»˜è®¤å€¼ï¼štrue
ã€€ã€€é…ç½®æ–‡ä»¶ï¼štez-ste.xml

<br>
**4. Hiveä½¿ç”¨CPUè®¾ç½®**
- hive.tez.cpu.vcores
ã€€ã€€é»˜è®¤å€¼ï¼š-1
 ã€€  è¯´æ˜ï¼šæ¯ä¸ªå®¹å™¨åˆ†é…çš„CPUä¸ªæ•°
ã€€ã€€é…ç½®æ–‡ä»¶ï¼šhive-site.xml

- tez.am.resource.cpu.vcores
   è¯´æ˜ï¼šamåˆ†é…çš„cpuä¸ªæ•°ï¼Œé»˜è®¤1

<br>
**5.Mapper/Reducerè®¾ç½®**

â‘ Mapperæ•°è®¾ç½®
åœ¨Tezåˆ†é…ä»»åŠ¡æ—¶ï¼Œä¸ä¼šåƒmré‚£æ ·ä¸ºæ¯ä¸ªsplitç”Ÿæˆä¸€ä¸ªmapä»»åŠ¡ï¼Œè€Œæ˜¯ä¼šå°†å¤šä¸ªsplitè¿›è¡Œgroupingï¼Œè®©mapä»»åŠ¡æ›´é«˜æ•ˆåœ°çš„å®Œæˆã€‚é¦–å…ˆTezä¼šæ ¹æ®è®¡ç®—å¾—åˆ°çš„ estimated number of tasks = 5ï¼Œå°†splitsèšåˆä¸º5ä¸ªSplit Groupï¼Œç”Ÿæˆ5ä¸ªmapperæ‰§è¡Œä»»åŠ¡ã€‚
Tezä¼šæ£€æŸ¥lengthPerGroupæ˜¯å¦åœ¨ tez.grouping.min-size ï¼ˆé»˜è®¤ä¸º50MBï¼‰ä»¥åŠ tez.grouping.max-sizeï¼ˆé»˜è®¤ä¸º1GBï¼‰ å®šä¹‰èŒƒå›´å†…ã€‚å¦‚æœè¶…è¿‡äº†max-sizeï¼Œåˆ™æŒ‡å®šlengthPerGroupä¸ºmax-sizeï¼Œå¦‚æœå°äºmin-sizeï¼Œåˆ™æŒ‡å®šlengthPerGroupä¸ºmin-sizeã€‚

- tez.grouping.min-size
   é»˜è®¤å€¼ï¼š5010241024 (50M)
   å‚æ•°è¯´æ˜ï¼šLower bound on the size (in bytes) of a grouped split, to avoid generating too many small splits.

- tez.grouping.max-size
   é»˜è®¤å€¼ï¼š102410241024
   å‚æ•°è¯´æ˜ï¼šUpper bound on thesize (in bytes) of a grouped split, to avoid generating excessively largesplits.

- tez.grouping.split-count
   é»˜è®¤å€¼ï¼šæ— 
   å‚æ•°è¯´æ˜ï¼šgroupçš„åˆ†å‰²ç»„æ•°ï¼Œå¹¶ä¸æ˜¯ç²¾ç¡®æŒ‡å®šåˆ‡åˆ†æ•°é‡ã€‚å½“è®¾ç½®çš„å€¼å¤§äºåŸå§‹çš„895æ—¶ï¼Œtezä¼šç›´æ¥ä½¿ç”¨895ã€‚


â‘¡Reduceræ•°è®¾ç½®
- hive.tez.auto.reducer.parallelism
é»˜è®¤å€¼ï¼šfalse
å‚æ•°è¯´æ˜ï¼šTezä¼šä¼°è®¡æ•°æ®é‡å¤§å°ï¼Œè®¾ç½®å¹¶è¡Œåº¦ã€‚åœ¨è¿è¡Œæ—¶ä¼šæ ¹æ®éœ€è¦è°ƒæ•´ä¼°è®¡å€¼ã€‚Turn on Tez' autoreducer parallelism feature. When enabled, Hive will still estimate data sizesand set parallelism estimates. Tez will sample source vertices' output sizesand adjust the estimates at runtime as necessary.
å»ºè®®è®¾ç½®ä¸ºtrue.

- hive.tex.min.partition.factor
é»˜è®¤å€¼ï¼š0.25
å‚æ•°è¯´æ˜ï¼šWhen auto reducerparallelism is enabled this factor will be used to put a lower limit to thenumber of reducers that Tez specifies.

- hive.tez.max.partition.factor
é»˜è®¤å€¼ï¼š2.0
å‚æ•°è¯´æ˜ï¼šWhen auto reducerparallelism is enabled this factor will be used to over-partition data inshuffle edges.

- hive.exec.reducers.bytes.per.reducer
é»˜è®¤å€¼ï¼š256,000,000 (256M)
å‚æ•°è¯´æ˜ï¼šæ¯ä¸ªreduceçš„æ•°æ®å¤„ç†é‡ã€‚Sizeper reducer. The default in Hive 0.14.0 and earlier is 1 GB, that is, if theinput size is 10 GB then 10 reducers will be used. In Hive 0.14.0 and later thedefault is 256 MB, that is, if the input size is 1 GB then 4 reducers willbe used.

- mapred.reduce.tasks
é»˜è®¤å€¼ï¼š-1
å‚æ•°è¯´æ˜ï¼šåŒmapreduceå¼•æ“é…ç½®ï¼ŒæŒ‡å®šreduceçš„ä¸ªæ•°ã€‚

- hive.exec.reducers.max
é»˜è®¤å€¼ï¼š1009
å‚æ•°è¯´æ˜ï¼šæœ€å¤§reduceä¸ªæ•°

   ä»¥ä¸‹å…¬å¼ç¡®è®¤Reducerä¸ªæ•°ï¼š
   `Max(1, Min(hive.exec.reducers.max [1009], ReducerStage estimate/hive.exec.reducers.bytes.per.reducer)) x hive.tez.max.partition.factor`

â‘¢Shuffleå‚æ•°è®¾ç½®
- tez.shuffle-vertex-manager.min-src-fraction
é»˜è®¤å€¼ï¼š0.25
å‚æ•°è¯´æ˜ï¼šthefraction of source tasks which should complete before tasks for the currentvertex are scheduled.

- tez.shuffle-vertex-manager.max-src-fraction
é»˜è®¤å€¼ï¼š0.75
å‚æ•°è¯´æ˜ï¼šoncethis fraction of source tasks have completed, all tasks on the current vertexcan be scheduled. Number of tasks ready for scheduling on the current vertexscales linearly between min-fraction and max-fraction.

>**ä¾‹å­ï¼š**
>hive.exec.reducers.bytes.per.reducer=1073741824; // 1GB
>tez.shuffle-vertex-manager.min-src-fraction=0.25ï¼›
>tez.shuffle-vertex-manager.max-src-fraction=0.75ï¼›
>
>**è¯´æ˜ï¼š**This indicates thatthe decision will be made between 25% of mappers finishing and 75% of mappersfinishing, provided there's at least 1Gb of data being output (i.e if 25% ofmappers don't send 1Gb of data, we will wait till at least 1Gb is sent out).


<br>
**5. å…¶ä»–è®¾ç½®**
- tez.job.name
   è¯´æ˜ï¼šè®¾ç½®tezä»»åŠ¡å

- tez.am.speculation.enabled
   è¯´æ˜ï¼šæ˜¯å¦å¼€å¯æ¨æµ‹æ‰§è¡Œï¼Œé»˜è®¤æ˜¯falseï¼Œåœ¨å‡ºç°æœ€åä¸€ä¸ªä»»åŠ¡å¾ˆæ…¢çš„æƒ…å†µä¸‹ï¼Œå»ºè®®æŠŠè¿™ä¸ªå‚æ•°è®¾ç½®ä¸ºtrue

- tez.task.log.level
   è¯´æ˜ï¼šæ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤info

- tez.queue.name
   è¯´æ˜ï¼šåœ¨yarnä¸­çš„é»˜è®¤æ‰§è¡Œé˜Ÿåˆ—


- tez.am.task.max.failed.attempts
   è¯´æ˜ï¼šä»»åŠ¡ä¸­attemptså¤±è´¥çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤è·Ÿyarnä¸€æ ·æ˜¯4æ¬¡ ï¼Œåœ¨ä¸ç¨³å®šé›†ç¾¤å¯ä»¥è®¾ç½®å¤§ä¸€ç‚¹


- tez.am.max.app.attempts
   è¯´æ˜ï¼šamè‡ªå·±å¤±è´¥çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤æ˜¯2æ¬¡ã€‚è¿™é‡Œå¹¶ä¸æ˜¯è¯´amè‡ªå·±æŒ‚äº†ï¼Œåªæ˜¯å› ä¸ºä¸€äº›ç³»ç»ŸåŸå› å¯¼è‡´å¤±è”äº†

- set mapreduce.input.fileinputformat.input.dir.recursive=true
   è¯´æ˜ï¼šå› ä¸ºTezä½¿ç”¨Insert ... UNION ALL è¯­å¥ï¼Œä¼šå°†æ¯ä¸ªUNION ALLç»“æœå•ç‹¬å­˜æ–‡ä»¶å¤¹ã€‚éœ€è¦è®¾ç½®mrå¼•æ“é€’å½’æŸ¥è¯¢ï¼Œå¯¹sparkå¼•æ“åŒæ ·ç”Ÿæ•ˆã€‚


<br>
**6. å®ä¾‹**
åœ¨Hive ä¸­æ‰§è¡Œä¸€ä¸ªqueryæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°Hive çš„æ‰§è¡Œå¼•æ“åœ¨ä½¿ç”¨ Tez ä¸ MRæ—¶ï¼Œä¸¤è€…ç”Ÿæˆmapperæ•°é‡å·®å¼‚è¾ƒå¤§ã€‚ä¸»è¦åŸå› åœ¨äº Tez ä¸­å¯¹ inputSplit åšäº† grouping æ“ä½œï¼Œå°†å¤šä¸ª inputSplit ç»„åˆæˆæ›´å°‘çš„ groupsï¼Œç„¶åä¸ºæ¯ä¸ª group ç”Ÿæˆä¸€ä¸ª mapper ä»»åŠ¡ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªinputSplit ç”Ÿæˆä¸€ä¸ªmapper ä»»åŠ¡ã€‚ä¸‹é¢æˆ‘ä»¬é€šè¿‡æ—¥å¿—åˆ†æä¸€ä¸‹è¿™ä¸­é—´çš„æ•´ä¸ªè¿‡ç¨‹ã€‚

è°ƒæ•´inputFormtä¸ºCombineInputFormatåˆå¹¶å°æ–‡ä»¶å¯ä»¥æœ‰æ•ˆå‡å°‘ä½œä¸šä¸­Mapperçš„æ•°é‡ã€‚


1ï¼‰MapReduceæ¨¡å¼
åœ¨ mr æ¨¡å¼ä¸‹ï¼Œç”Ÿæˆçš„containeræ•°ä¸º116ä¸ªï¼š

å¯¹åº”æ—¥å¿—æ¡ç›®ä¸ºï¼š
Input size for job job_1566964005095_0003 = 31733311148. Number of splits = 116

åœ¨MRä¸­ï¼Œä½¿ç”¨çš„æ˜¯Hadoopä¸­çš„FileInputFormatï¼Œæ‰€ä»¥è‹¥æ˜¯ä¸€ä¸ªæ–‡ä»¶å¤§äºä¸€ä¸ªblockçš„å¤§å°ï¼Œåˆ™ä¼šåˆ‡åˆ†ä¸ºå¤šä¸ªInputSplitï¼›è‹¥æ˜¯ä¸€ä¸ªæ–‡ä»¶å°äºä¸€ä¸ªblockå¤§å°ï¼Œåˆ™ä¸ºä¸€ä¸ªInputSplitã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ€»æ–‡ä»¶ä¸ªæ•°ä¸º14ä¸ªï¼Œæ¯ä¸ªå‡ä¸º2.1GBï¼Œä¸€å…±29.4GBå¤§å°ã€‚ç”Ÿæˆçš„InputSplitæ•°ä¸º116ä¸ªï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªblockï¼ˆè¿™ä¸ªåœºæ™¯ä¸‹InputSplit å¤§å°ä¸ºä¸€ä¸ªblockå¤§å°ï¼‰çš„å¤§å°å¤§çº¦ä¸º256MBã€‚

2ï¼‰Tezæ¨¡å¼

è€Œåœ¨Tezæ¨¡å¼ä¸‹ï¼Œç”Ÿæˆçš„mapä»»åŠ¡ä¸º32ä¸ªï¼š

ç”Ÿæˆsplit groupsçš„ç›¸å…³æ—¥å¿—å¦‚ä¸‹ï¼š
```
mapred.FileInputFormat|: Total input files to process : 14
io.HiveInputFormat|: number of splits 476
tez.HiveSplitGenerator|: Number of input splits: 476. 3 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat

tez.SplitGrouper|: # Src groups for split generation: 2
tez.SplitGrouper|: Estimated number of tasks: 5 for bucket 1
grouper.TezSplitGrouper|: Grouping splits in Tez
|grouper.TezSplitGrouper|: Desired splits: 5 too small. Desired splitLength: 6346662229 Max splitLength: 1073741824 New desired splits: 30 Total length: 31733311148 Original splits: 476
|grouper.TezSplitGrouper|: Desired numSplits: 30 lengthPerGroup: 1057777038 numLocations: 1 numSplitsPerLocation: 476 numSplitsInGroup: 15 totalLength: 31733311148 numOriginalSplits: 476 . Grouping by length: true count: false nodeLocalOnly: false
|grouper.TezSplitGrouper|: Doing rack local after iteration: 32 splitsProcessed: 466 numFullGroupsInRound: 0 totalGroups: 31 lengthPerGroup: 793332736 numSplitsInGroup: 11
|grouper.TezSplitGrouper|: Number of splits desired: 30 created: 32 splitsProcessed: 476
|tez.SplitGrouper|: Original split count is 476 grouped split count is 32, for bucket: 1
|tez.HiveSplitGenerator|: Number of split groups: 32
```

<br>
**Avaiable Slots**

é¦–å…ˆå¯ä»¥çœ‹åˆ°ï¼Œéœ€è¦å¤„ç†çš„æ–‡ä»¶æ•°ä¸º14ï¼Œåˆå§‹splitsæ•°ç›®ä¸º476ï¼ˆå³æ„å‘³ç€åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œä¸€ä¸ªblockçš„å¤§å°çº¦ä¸º64MBï¼‰ã€‚å¯¹åº”æ—¥å¿—æ¡ç›®å¦‚ä¸‹ï¼š

|mapred.FileInputFormat|: Total input files to process : 14

|io.HiveInputFormat|: number of splits 476

è·å–åˆ°splitsçš„ä¸ªæ•°ä¸º476ä¸ªåï¼ŒDriverå¼€å§‹è®¡ç®—å¯ç”¨çš„slotsï¼ˆcontainerï¼‰æ•°ï¼Œè¿™é‡Œè®¡ç®—å¾—åˆ°3ä¸ªslotsï¼Œå¹¶æ‰“å°äº†é»˜è®¤çš„waveså€¼ä¸º1.7ã€‚
åœ¨æ­¤åœºæ™¯ä¸­ï¼Œé›†ç¾¤ä¸€å…±èµ„æºä¸º 8 vcoreï¼Œ12G å†…å­˜ï¼Œcapacity-schedulerä¸­æŒ‡å®šçš„user limit factor ä¸º0.5ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼šå½“å‰ç”¨æˆ·èƒ½ä½¿ç”¨çš„èµ„æºæœ€å¤šä¸º 6G å†…å­˜ã€‚åœ¨Tez Driverä¸­ï¼Œç”³è¯·çš„container èµ„æºçš„å•ä½ä¸ºï¼š Default Resources=<memory:1536, vCores:1>

æ‰€ä»¥ç†è®ºä¸Šå¯ä»¥ç”³è¯·åˆ°çš„container æ•°ç›®ä¸º4ï¼ˆ6G/1536MB = 4ï¼‰ä¸ªï¼Œè€Œç”±äº Application Master å ç”¨äº†ä¸€ä¸ªcontainerï¼Œæ‰€ä»¥æœ€ç»ˆavailable slotsä¸º3ä¸ªã€‚

åœ¨è®¡ç®—å‡ºäº†å¯ç”¨çš„slotsä¸º3ä¸ªåï¼ŒTez ä½¿ç”¨split-waves ä¹˜æ•°ï¼ˆç”±tez.grouping.split-wavesæŒ‡å®šï¼Œé»˜è®¤ä¸º1.7ï¼‰æŒ‡å®šâ€œé¢„ä¼°â€çš„Map ä»»åŠ¡æ•°ç›®ä¸ºï¼š3 Ã— 1.7 = 5 ä¸ªtasksã€‚å¯¹åº”æ—¥å¿—æ¡ç›®å¦‚ä¸‹ï¼š

|tez.HiveSplitGenerator|: Number of input splits: 476. 3 available slots, 1.7 waves. Input format is: org.apache.hadoop.hive.ql.io.HiveInputFormat

|tez.SplitGrouper|: Estimated number of tasks: 5 for bucket 1

<br>
**Grouping Input Splits**

åœ¨Tezåˆ†é…ä»»åŠ¡æ—¶ï¼Œä¸ä¼šåƒmré‚£æ ·ä¸ºæ¯ä¸ªsplitç”Ÿæˆä¸€ä¸ªmapä»»åŠ¡ï¼Œè€Œæ˜¯ä¼šå°†å¤šä¸ªsplitè¿›è¡Œgroupingï¼Œè®©mapä»»åŠ¡æ›´é«˜æ•ˆåœ°çš„å®Œæˆã€‚é¦–å…ˆTezä¼šæ ¹æ®è®¡ç®—å¾—åˆ°çš„ estimated number of tasks = 5ï¼Œå°†splitsèšåˆä¸º5ä¸ªSplit Groupï¼Œç”Ÿæˆ5ä¸ªmapperæ‰§è¡Œä»»åŠ¡ã€‚

**ä½†æ˜¯è¿™é‡Œè¿˜éœ€è¦è€ƒè™‘å¦ä¸€ä¸ªå€¼**ï¼šlengthPerGroupã€‚Tezä¼šæ£€æŸ¥lengthPerGroupæ˜¯å¦åœ¨ tez.grouping.min-size ï¼ˆé»˜è®¤ä¸º50MBï¼‰ä»¥åŠ tez.grouping.max-sizeï¼ˆé»˜è®¤ä¸º1GBï¼‰ å®šä¹‰èŒƒå›´å†…ã€‚å¦‚æœè¶…è¿‡äº†max-sizeï¼Œåˆ™æŒ‡å®šlengthPerGroupä¸ºmax-sizeï¼Œå¦‚æœå°äºmin-sizeï¼Œåˆ™æŒ‡å®šlengthPerGroupä¸ºmin-sizeã€‚

åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œæ•°æ®æ€»å¤§å°ä¸º 31733311148 bytesï¼ˆ29.5GBå·¦å³ï¼Œä¹Ÿæ˜¯åŸæ•°æ®å¤§å°ï¼‰ï¼Œé¢„ä¼°ä¸º5ä¸ªGroupï¼Œ åˆ™æ¯ä¸ªGroupçš„ splitLengthä¸º6346662229 bytesï¼ˆ5.9GB å·¦å³ï¼‰ï¼Œè¶…è¿‡äº† Max splitLength = 1073741824 bytesï¼ˆ 1GBï¼‰ï¼Œæ‰€ä»¥é‡æ–°æŒ‰ splitLength = 1GB æ¥ç®—ï¼Œè®¡ç®—å‡ºæ‰€éœ€çš„numSplits æ•°ä¸º 30 ä¸ªï¼Œæ¯ä¸ªSplit Groupçš„å¤§å°ä¸º1GBã€‚

åœ¨è®¡ç®—å‡ºäº†æ¯ä¸ªSplit Groupçš„å¤§å°ä¸º1GBåï¼Œç”±äºåŸSplitæ€»æ•°ç›®ä¸º476ï¼Œæ‰€ä»¥éœ€è¦å°†è¿™476ä¸ªinputSplitè¿›è¡Œgroupingï¼Œä½¿å¾—æ¯ä¸ªGroupçš„å¤§å°å¤§çº¦ä¸º1GBå·¦å³ã€‚æŒ‰æ­¤æ–¹æ³•è®¡ç®—ï¼Œé¢„æœŸçš„splitsæ•°ç›®åº”ä¸º30ä¸ªï¼ˆä½†æ˜¯ä»…æ˜¯é€šè¿‡æ€»æ•°æ®å¤§å°/lengthPerGroupå¾—å‡ºï¼Œå°šæœªè€ƒè™‘inputSplitså¦‚ä½•åˆå¹¶çš„é—®é¢˜ï¼Œä¸ä¸€å®šä¸ºæœ€ç»ˆç”Ÿæˆçš„map tasksæ•°ç›®ï¼‰ã€‚ä¸”æœ€ç»ˆå¯è®¡ç®—å¾—å‡ºæ¯ä¸ªgroupä¸­å¯ä»¥åŒ…å«15ä¸ªåŸsplitï¼Œä¹Ÿå°±æ˜¯numSplitsInGroup = 15ã€‚ç›¸å…³æ—¥å¿—æ¡ç›®å¦‚ä¸‹ï¼š

|grouper.TezSplitGrouper|: Grouping splits in Tez

|grouper.TezSplitGrouper|: Desired splits: 5 too small. Desired splitLength: 6346662229 Max splitLength: 1073741824 New desired splits: 30 Total length: 31733311148 Original splits: 476

|grouper.TezSplitGrouper|: Desired numSplits: 30 lengthPerGroup: 1057777038 numLocations: 1 numSplitsPerLocation: 476 numSplitsInGroup: 15 totalLength: 31733311148 numOriginalSplits: 476 . Grouping by length: true count: false nodeLocalOnly: false

åŸsplitsæ€»æ•°ç›®ä¸º 476ï¼Œåœ¨å¯¹splitsè¿›è¡Œgroupingæ—¶ï¼Œæ¯ä¸ªgroupä¸­å°†ä¼šåŒ…å«15ä¸ªinputSplitsï¼Œæ‰€ä»¥æœ€ç»ˆå¯ä»¥è®¡ç®—å‡ºçš„groupæ•°ç›®ä¸º 476/15 = 32 ä¸ªï¼Œä¹Ÿå°±æ˜¯æœ€ç»ˆç”Ÿæˆçš„mapperæ•°é‡ã€‚

|tez.SplitGrouper|: Original split count is 476 grouped split count is 32, for bucket: 1

|tez.HiveSplitGenerator|: Number of split groups: 32

æ‰€ä»¥åœ¨Tezä¸­ï¼ŒinputSplit æ•°ç›®è™½ç„¶æ˜¯476ä¸ªï¼Œä½†æ˜¯æœ€ç»ˆä»…ç”Ÿæˆäº†32ä¸ªmapä»»åŠ¡ç”¨äºå¤„ç†æ‰€æœ‰çš„ 475ä¸ªinputSplitsï¼Œå‡å°‘äº†è¿‡å¤šmapperä»»åŠ¡ä¼šå¸¦æ¥çš„é¢å¤–å¼€é”€ã€‚

<br>
**Split Waves**
è¿™é‡Œä¸ºä»€ä¹ˆè¦å®šä¹‰ä¸€ä¸ªsplit waveså€¼å‘¢ï¼Ÿä½¿ç”¨æ­¤å€¼ä¹‹åä¼šè®©Driverç”³è¯·æ›´å¤šçš„containerï¼Œæ¯”å¦‚æ­¤åœºæ™¯ä¸­æœ¬æ¥ä»…æœ‰3ä¸ªslotså¯ç”¨ï¼Œä½†æ˜¯ä¼šæ ¹æ®è¿™ä¸ªä¹˜æ•°å†å¤šç”³è¯·2ä¸ªcontainerèµ„æºã€‚ä½†æ˜¯è¿™æ ·åšçš„åŸå› æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
1. é¦–å…ˆå®ƒå¯ä»¥è®©åˆ†é…èµ„æºæ›´çµæ´»ï¼šæ¯”å¦‚é›†ç¾¤ä¹‹åæ·»åŠ äº†è®¡ç®—èŠ‚ç‚¹ã€å…¶ä»–ä»»åŠ¡å®Œæˆåé‡Šæ”¾äº†èµ„æºç­‰ã€‚æ‰€ä»¥å³ä½¿åˆšå¼€å§‹ä¼šæœ‰éƒ¨åˆ†mapä»»åŠ¡åœ¨ç­‰å¾…èµ„æºï¼Œå®ƒä»¬åœ¨åç»­ä¹Ÿä¼šå¾ˆå¿«è¢«åˆ†é…åˆ°èµ„æºæ‰§è¡Œä»»åŠ¡
2. å°†æ•°æ®åˆ†é…ç»™æ›´å¤šçš„mapä»»åŠ¡å¯ä»¥æé«˜å¹¶è¡Œåº¦ï¼Œå‡å°‘æ¯ä¸ªmapä»»åŠ¡ä¸­å¤„ç†çš„æ•°æ®é‡ï¼Œå¹¶ç¼“è§£ç”±äºå°‘éƒ¨åˆ†mapä»»åŠ¡æ‰§è¡Œè¾ƒæ…¢ï¼Œè€Œå¯¼è‡´çš„æ•´ä½“ä»»åŠ¡å˜æ…¢çš„æƒ…å†µ
