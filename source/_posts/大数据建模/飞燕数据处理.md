---
title: 飞燕数据处理
categories:
- 大数据建模
---
创建Kafka的topic
```
[root@cos-bigdata-hadoop-03 kafka-2.11]# bin/kafka-topics.sh --zookeeper bigdata1:2181 --create --topic items-model --partitions 3 --replication-factor 2
Created topic "items-model".
[root@cos-bigdata-hadoop-03 kafka-2.11]# bin/kafka-topics.sh --zookeeper bigdata1:2181 --create --topic status-model --partitions 3 --replication-factor 2
Created topic "status-model".
[root@cos-bigdata-hadoop-03 kafka-2.11]# bin/kafka-topics.sh --zookeeper bigdata1:2181 --create --topic version-model --partitions 3 --replication-factor 2
```


1. 客户端接收数据并落盘
```
public class ClientApplication {

    private static final String path = "/tmp/compass_logs/feiyan";

    public static void main(String[] args) throws UnsupportedEncodingException {
        String endPoint = "https://ilop.iot-as-http2.cn-shanghai.aliyuncs.com:443";
        String appKey = "27698993";
        String appSecret = "2d13de8dfdb4284f6e1e5e1ecc21a6d9";

        Profile profile = Profile.getAppKeyProfile(endPoint, appKey, appSecret);

        MessageCallback messageCallback = new MessageCallback() {
            public Action consume(MessageToken messageToken) {
                byte[] payload = messageToken.getMessage().getPayload();
//                String data = new String(payload);
//                System.out.println("receive : " + data);

                // 获取当前时间
                ZoneId Shanghai = ZoneId.of("Asia/Shanghai");
                ZonedDateTime now = ZonedDateTime.now(Shanghai);
                DateTimeFormatter df = DateTimeFormatter.ofPattern("yyyy-MM-dd");
                String strDate = now.format(df);
                // 获取路径
//                String filename = "C:/Users/CJ/Desktop/log" + File.separator + "q6_" + strDate + ".log";
                String filename = path + File.separator + "model_" + strDate + ".log";
                if (args.length > 0) {
                    filename = args[0];
                }

                // 输出到文件中
                File file = new File(filename);
                if (!file.getParentFile().exists()) {
                    file.getParentFile().mkdirs();
                }


                FileOutputStream fos;

                try {
                    fos = new FileOutputStream(filename, true);
                    BufferedOutputStream bos = new BufferedOutputStream(fos);
                    bos.write(payload);
                    bos.write("
".getBytes());
                    bos.flush();
                } catch (Exception e) {
                    e.printStackTrace();
                }

                return Action.CommitSuccess;
            }
        };

        MessageClient messageClient = MessageClientFactory.messageClient(profile);
        messageClient.setMessageListener(messageCallback);
        messageClient.connect(messageCallback);

        try {
            System.in.read();
        } catch (Exception e) {
        }
    }
}
```
启动 `nohup java -jar  hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.jar &`

>为了转化无序的状态日志为操作日志，目前有三种处理方式
> 1. 将数据导入hdfs中，然后通过开窗进行排序比较，得到操作记录
> 2. 在flume中，通过缓存同一设备的历史记录与最新记录来比较(因为数据无序所以不可行)
> 3. 在Flink中使用CEP模块，结合水位线，进行比较后得到操作记录

2. 通过Flume监控文件并导入Kafka(需要区分日志是物模型还是设备绑定状态并插入到不同的topic中)
```
#define
a1.sources= r1
a1.channels= c1 c2 c3

#source
a1.sources.r1.type = TAILDIR
a1.sources.r1.positionFile = /opt/module/flume-1.7.0/taildir_position.json
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /tmp/compass_logs/feiyan/.*log
a1.sources.r1.fileHeader = false
a1.sources.r1.maxBatchCount = 1000

#interceptors
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.hxr.flume.LogTypeInterceptor$Builder

#selector
a1.sources.r1.selector.type = multiplexing
a1.sources.r1.selector.header = topic
a1.sources.r1.selector.mapping.items_model = c1
a1.sources.r1.selector.mapping.status_model = c2
a1.sources.r1.selector.mapping.version_model = c3

#channel
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a1.channels.c1.kafka.topic = items-model
a1.channels.c1.parseAsFlumeEvent = false

a1.channels.c2.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c2.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a1.channels.c2.kafka.topic = status-model
a1.channels.c2.parseAsFlumeEvent = false

a1.channels.c3.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c3.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a1.channels.c3.kafka.topic = version-model
a1.channels.c3.parseAsFlumeEvent = false

#combine
a1.sources.r1.channels = c1 c2 c3
```
启动 `sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_file2kafka.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=feiyan_model_file2kafka.log  -Xms512m -Xmx512m  1>/dev/null 2>&1 &'`

3. 通过Flume将Kafka中的数据导入hdfs中
需要设置时间拦截器指定event的时间
```
public class TimestampInterceptor implements Interceptor {

    private ArrayList<Event> events = new ArrayList<>();

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        Map<String, String> headers = event.getHeaders();
        String log = new String(event.getBody(), StandardCharsets.UTF_8);

        JSONObject jsonObject = JSONObject.parseObject(log);

        Long gmtCreate = jsonObject.getLong("gmtCreate");

        // 获取上海时区当日的日期
        ZonedDateTime now = ZonedDateTime.now(ZoneId.of("Asia/Shanghai"));

        // 获取上海时区当日指定时间对应的时间戳
        long lineTimestamp = Timestamp.from(ZonedDateTime.of(now.getYear(), now.getMonth().getValue(), now.getDayOfMonth(), 0, 29, 0, 0, ZoneId.of("Asia/Shanghai")).toInstant()).getTime();
        // 获取上海时区当日零点的时间戳
        long zeroTimestamp = Timestamp.from(ZonedDateTime.of(now.getYear(), now.getMonth().getValue(), now.getDayOfMonth(), 0, 0, 0, 1000000, ZoneId.of("Asia/Shanghai")).toInstant()).getTime();

        // 如果当前时间已经超过指定的时间(保证flume可以关闭tmp文件)，但是到达的event的事件时间戳比当日0点的时间戳要小，那么用当日0点的时间戳代替
        if (lineTimestamp <= Timestamp.from(now.toInstant()).getTime() && gmtCreate <= zeroTimestamp) {
            gmtCreate = 0L;
        }


        headers.put("timestamp", String.valueOf(gmtCreate));

        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        events.clear();
        for (Event event : list) {
            events.add(intercept(event));
        }

        return events;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder {
        @Override
        public Interceptor build() {
            return new TimestampInterceptor();
        }

        @Override
        public void configure(Context context) {
        }
    }
}
```
任务配置如下
```
#define
a2.sources= r1 r2 r3
a2.channels= c1 c2 c3
a2.sinks = k1 k2 k3

#source
a2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r1.batchSize = 10000
a2.sources.r1.batchDurationMillis = 2000
a2.sources.r1.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a2.sources.r1.kafka.topics = items-model
a2.sources.r1.kafka.consumer.group.id = custom_items_model

a2.sources.r2.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r2.batchSize = 10000
a2.sources.r2.batchDurationMillis = 2000
a2.sources.r2.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
a2.sources.r2.kafka.topics = status-model
a2.sources.r2.kafka.consumer.group.id = custom_status_model

#a2.sources.r3.type = org.apache.flume.source.kafka.KafkaSource
#a2.sources.r3.batchSize = 5000
#a2.sources.r3.batchDurationMillis = 2000
#a2.sources.r3.kafka.bootstrap.servers = BigData1:9092,BigData2:9092,BigData3:9092
#a2.sources.r3.kafka.topics = version-model
#a2.sources.r3.kafka.consumer.group.id = custom_version_model

#interceptors
a2.sources.r1.interceptors = i1
a2.sources.r1.interceptors.i1.type = com.hxr.flume.TimestampInterceptor$Builder
a2.sources.r2.interceptors = i1
a2.sources.r2.interceptors.i1.type = com.hxr.flume.TimestampInterceptor$Builder
#a2.sources.r3.interceptors = i1
#a2.sources.r3.interceptors.i1.type = com.hxr.flume.TimestampInterceptor$Builder

#channels
a2.channels.c1.type = memory
a2.channels.c1.capacity = 50000
a2.channels.c1.transactionCapacity = 10000
a2.channels.c1.byteCapacityBufferPercentage = 20
a2.channels.c1.byteCapacity = 100000000

a2.channels.c2.type = memory
a2.channels.c2.capacity = 50000
a2.channels.c2.transactionCapacity = 10000
a2.channels.c2.byteCapacityBufferPercentage = 20
a2.channels.c2.byteCapacity = 10000000

#a2.channels.c3.type = memory
#a2.channels.c3.capacity = 10000
#a2.channels.c3.transactionCapacity = 10000
#a2.channels.c3.byteCapacityBufferPercentage = 20
#a2.channels.c3.byteCapacity = 800000

#sink
a2.sinks.k1.type = hdfs
a2.sinks.k1.hdfs.path = /origin_data/compass/feiyan/ods_items_model/%Y-%m-%d
a2.sinks.k1.hdfs.filePrefix = ods_items_model
a2.sinks.k1.hdfs.rollInterval = 3600
a2.sinks.k1.hdfs.rollSize = 134217728
a2.sinks.k1.hdfs.rollCount = 0
a2.sinks.k1.hdfs.useLocalTimeStamp = true
a2.sinks.k1.hdfs.minBlockReplicas = 1

a2.sinks.k2.type = hdfs
a2.sinks.k2.hdfs.path = /origin_data/compass/feiyan/ods_status_model/%Y-%m-%d
a2.sinks.k2.hdfs.filePrefix = ods_status_model
a2.sinks.k2.hdfs.rollInterval = 3600
a2.sinks.k2.hdfs.rollSize = 134217728
a2.sinks.k2.hdfs.rollCount = 0
a2.sinks.k2.hdfs.useLocalTimeStamp = true
a2.sinks.k2.hdfs.minBlockReplicas = 1

#a2.sinks.k3.type = hdfs
#a2.sinks.k3.hdfs.path = /origin_data/compass/feiyan/ods_version_model/%Y-%m-%d
#a2.sinks.k3.hdfs.filePrefix = ods_version_model
#a2.sinks.k3.hdfs.rollInterval = 3600
#a2.sinks.k3.hdfs.rollSize = 134217728
#a2.sinks.k3.hdfs.rollCount = 0
#a2.sinks.k3.hdfs.useLocalTimeStamp = true
#a2.sinks.k3.hdfs.minBlockReplicas = 1

#compress
a2.sinks.k1.hdfs.codeC = lzop
a2.sinks.k1.hdfs.fileType = CompressedStream
a2.sinks.k2.hdfs.codeC = lzop
a2.sinks.k2.hdfs.fileType = CompressedStream
#a2.sinks.k3.hdfs.codeC = lzop
#a2.sinks.k3.hdfs.fileType = CompressedStream

#kerberos
a2.sinks.k1.hdfs.kerberosPrincipal = hive@IOTMARS.COM
a2.sinks.k1.hdfs.kerberosKeytab = /etc/security/keytab/hive.keytab
a2.sinks.k2.hdfs.kerberosPrincipal = hive@IOTMARS.COM
a2.sinks.k2.hdfs.kerberosKeytab = /etc/security/keytab/hive.keytab
#a2.sinks.k3.hdfs.kerberosPrincipal = hive@IOTMARS.COM
#a2.sinks.k3.hdfs.kerberosKeytab = /etc/security/keytab/hive.keytab

#combine
a2.sources.r1.channels = c1
a2.sources.r2.channels = c2
#a2.sources.r3.channels = c3
a2.sinks.k1.channel = c1
a2.sinks.k2.channel = c2
#a2.sinks.k3.channel = c3
```
启动 `sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=feiyan_model_kafka2hdfs.log  -Xms512m -Xmx512m -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`



接收kafka中的umeng数据
```
# agent 
a2.sources = r1
a2.channels = c1
a2.sinks = k1

# sources
a2.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a2.sources.r1.batchSize = 5000
a2.sources.r1.batchDurationMillis = 2000
a2.sources.r1.kafka.bootstrap.servers = bigdata1:9092,bigdata2:9092,bigdata3:9092
a2.sources.r1.kafka.topics = compass-umeng-test
a2.sources.r1.kafka.consumer.group.id = youmeng3

# interceptor
#a1.sources.r1.interceptors =  i1 i2
#a1.sources.r1.interceptors.i1.type = com.iotmars.interceptor.ETLInterceptor$Builder
#a1.sources.r1.interceptors.i2.type = com.iotmars.interceptor.TimeStampInterceptor$Builder

# channels
a2.channels.c1.type = memory
a2.channels.c1.capacity = 10000
a2.channels.c1.transactionCapacity = 10000
a2.channels.c1.byteCapacityBufferPercentage = 20
a2.channels.c1.byteCapacity = 800000
#a1.channels.c1.type = file 
#a1.channels.c1.checkpointDir = /opt/module/flume-1.7.0/checkpoint/edb_order_file_channel2
#a1.channels.c1.dataDirs = /opt/module/flume-1.7.0/data/edb_order_file_channel2

# sinks
a2.sinks.k1.type = hdfs
a2.sinks.k1.hdfs.path = /origin_data/compass/youmeng/youmeng_json/%Y-%m-%d
a2.sinks.k1.hdfs.filePrefix = youmeng
a2.sinks.k1.hdfs.batchSize = 1000
a2.sinks.k1.hdfs.rollInterval = 300
a2.sinks.k1.hdfs.rollSize = 134217728
a2.sinks.k1.hdfs.rollCount = 0
a2.sinks.k1.hdfs.useLocalTimeStamp = true

# compress
a2.sinks.k1.hdfs.codeC = lzop
a2.sinks.k1.hdfs.fileType = CompressedStream

#kerberos
a2.sinks.k1.hdfs.kerberosPrincipal = hive@IOTMARS.COM
a2.sinks.k1.hdfs.kerberosKeytab = /etc/security/keytab/hive.keytab

# combine
a2.sources.r1.channels = c1
a2.sinks.k1.channel = c1
```
启动 `sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/youmeng_order_kafka2hdfs.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=youmeng_order_kafka2hdfs.log 1>/dev/null 2>&1 &'`

```
sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/youmeng_order_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=youmeng_order_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'
sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a3 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/youmeng_active_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=youmeng_active_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36003 1>/dev/null 2>&1 &'
```

4. 创建自定义函数比较两条数据的不同属性(因为不同属性间会有联动关系，如启动智慧菜谱会同时启动工作模式，而统计工作模式要排除因为智慧菜谱启动)
```
public class CompareAndGetDiff extends GenericUDTF {

    @Override
    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
        // 1. 检查参数合法性
        if (argOIs.length != 2) {
            throw new UDFArgumentException("需要两个参数");
        }

        // 2. 参数必须都为string
        if (argOIs[0].getCategory() != ObjectInspector.Category.PRIMITIVE) {
            throw new UDFArgumentException("只接受基础类型参数");
        }

        PrimitiveObjectInspector argument0 = (PrimitiveObjectInspector) argOIs[0];
        PrimitiveObjectInspector argument1 = (PrimitiveObjectInspector) argOIs[1];

        if (argument0.getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.STRING) {
            throw new UDFArgumentException("只接受string类型");
        }

        // 2. 定义返回值名称和类型
        ArrayList<String> fieldNames = new ArrayList<>();
        fieldNames.add("event_name");
        fieldNames.add("event_value");
        fieldNames.add("event_ori_value");

        ArrayList<ObjectInspector> fieldType = new ArrayList<>();
        fieldType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        fieldType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        fieldType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldType);
    }

    @Override
    public void process(Object[] objects) throws HiveException {
        // 检查参数
        if (Objects.isNull(objects[0]) || Objects.isNull(objects[1])) {
            return;
        }
        String arg0 = PrimitiveObjectInspectorUtils.getString(objects[0], PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        String arg1 = PrimitiveObjectInspectorUtils.getString(objects[1], PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        if (StringUtils.isEmpty(arg0) || StringUtils.isEmpty(arg1)) {
            return;
        }

        // 比较并获取改变的属性
        try {
            JSONObject newData = new JSONObject(arg0);
            JSONObject oldData = new JSONObject(arg1);

            Set<String> keys = newData.keySet();

            for (String key : keys) {
                String newValue = newData.getJSONObject(key).getString("value");
                String oldValue = oldData.getJSONObject(key).getString("value");

                Logger.getLogger("CompareAndGetDiff").warning("值: " + newValue + "  " + oldValue);
                if (!Objects.isNull(newValue) && !newValue.equalsIgnoreCase(oldValue)) {
                    String[] result = {key,newValue,oldValue};
                    forward(result);
                }
            }

        } catch (Exception e) {
            Logger.getLogger("CompareAndGetDiff").warning("FastJson解析失败: " + arg0 + "  " + arg1);
        }

    }

    @Override
    public void close() throws HiveException {

    }
}
```
设置全局函数 `create function compass_dwd.getDiff as 'com.iotmars.hive.CompareAndGetDiff' using jar 'hdfs://192.168.101.184:9820/user/hive/jars/Hxr_Compass_udf-2.0-SNAPSHOT.jar';`

为了解析飞燕的错误代码为显示的异常代码，需要自定义函数进行转
```
public class ParseErrorCodeAndGetDiff extends GenericUDTF {

    public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException {
        // 1. 首先检查参数合法性
        if (argOIs.length != 2) {
            throw new UDFArgumentException("只需要两个参数");
        }

        if (argOIs[0].getCategory() != ObjectInspector.Category.PRIMITIVE || argOIs[1].getCategory() != ObjectInspector.Category.PRIMITIVE) {
            throw new UDFArgumentException("只接受基础类型参数");
        }

        // 2. 定义返回值名称和类型
        ArrayList<String> fieldName = new ArrayList<>();
        fieldName.add("error_code");

        ArrayList<ObjectInspector> fieldType = new ArrayList<>();
        fieldType.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldName, fieldType);
    }

    public void process(Object[] objects) throws HiveException {
        try {
            Long newErrorCode = Long.parseLong(objects[0].toString());
            ArrayList<String> newFaultCodeList = parseErrorCode(newErrorCode);

            Long oldErrorCode = Long.parseLong(objects[1].toString());
            ArrayList<String> oldFaultCodeList = parseErrorCode(oldErrorCode);

            // 返回新增的异常值
            for (String newFaultCode : newFaultCodeList) {
                if (!oldFaultCodeList.contains(newFaultCode)) {
                    forward(new String[]{newFaultCode});
                }
            }

        } catch (Exception e) {
            Logger.getLogger("ParseErrorCodeAndGetDiff").warning("解析错误代码异常: " + objects[0] + "  " + objects[1]);
        }

    }

    private ArrayList<String> parseErrorCode(Long code){
        ArrayList<String> list = new ArrayList<>();

        long exp = 0L;
        long value = 0L;
        while (code > value) {
            // 通过比较
            value = Double.valueOf(Math.pow(2D, exp)).longValue();
            long calc = code & value;
            // 1为E1,2为E2,4为E3,8192为E12
            if (calc > 0L) {
                list.add(String.valueOf(exp - 1));
            }
            ++exp;
        }

        // 转化为指数上标的集合
        return list;
    }

    public void close() throws HiveException {
    }
}
```
设置全局函数 `create function compass_dws.parseErrorCodeAndGetDiff as 'com.iotmars.hive.ParseErrorCodeAndGetDiff' using jar 'hdfs://192.168.101.184:9820/user/hive/jars/Hxr_Compass_udf-2.0-SNAPSHOT.jar';`
