---
title: 调优
categories:
- 大数据建模
---
调优最重要的就是知道问题在哪里，所以在执行hive时将日志设置为debug，可以看到更多信息。
```
hive -hiveconf hive.root.logger=DEBUG,console
```

# 一、HDFS调优
## 1.1 core-default.xml：
- hadoop.tmp.dir：
默认值： /tmp
说明： 尽量手动配置这个选项，否则的话都默认存在了里系统的默认临时文件/tmp里。并且手动配置的时候，如果服务器是多磁盘的，每个磁盘都设置一个临时文件目录，这样便于mapreduce或者hdfs等使用的时候提高磁盘IO效率。

- fs.trash.interval：
默认值： 0
说明： 这个是开启hdfs文件删除自动转移到垃圾箱的选项，值为垃圾箱文件清除时间。一般开启这个会比较好，以防错误删除重要文件。单位是分钟。

- io.file.buffer.size：
默认值：4096
说明：SequenceFiles在读写中可以使用的缓存大小，可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。

- io.compression.codecs
说明：设置hadoop支持的压缩模式，使用Lzop才能进行切片，但是需要创建索引，不创建索引不会进行切片效率低下。需要使用代码为文件创建Lzo切片索引。

## 1.2 hdfs-default.xml：
- dfs.blocksize：
默认值：134217728
说明： 这个就是hdfs里一个文件块的大小了，CDH5中默认128M。太大的话会有较少map同时计算，太小的话也浪费可用map个数资源，而且文件太小namenode就浪费内存多。根据需要进行设置。

- dfs.namenode.handler.count：
默认值：10
说明：设定 namenode server threads 的数量，这些 threads 會用 RPC 跟其他的 datanodes 沟通。当 datanodes 数量太多时会发現很容易出現 RPC timeout，解决方法是提升网络速度或提高这个值，但要注意的是 thread 数量多也表示 namenode 消耗的内存也随着增加。
推荐值：namenode需要接受datanode心跳、客户端请求等，根据公式20*math.log(9)，计算9个节点的集群，nn的线程池大小设置为45个线程最好。


<br>
## 1.3 其他
cloudera manager监控页面HDFS大部分机器出现类似告警"存在隐患 : DataNode 有 xxxxxx 个块。 警告阈值：500,000 块。"，cm给出的建议：
   >这是 DataNode 运行状况检查，用于检查 DataNode 是否含有过多的块。如果 DataNode 含有过多的块，可能影响 DataNode 的性能。具有大量块数的 DataNode 将需要较大的 java 堆并且可能遇到较长时间的垃圾回收暂停。另外，大量块数可能表明存在许多小文件。不会为处理许多小文件而优化 HDFS，跨许多小文件进行操作时处理时间可能受影响。
如果只有部分 DataNode 有大量块，运行 HDFS 重新平衡命令可以通过移动 DataNode 之间的数据解决该问题。如果 HDFS 重新平衡命令将群集报告为平衡，没有修复块不平衡，则问题与存在的许多小文件有关。参阅 HDFS 文档了解解决该问题的最佳做法。如果许多小文件不是您的使用案例的关注点，则考虑禁用该运行状况测试。如果所有 DataNode 都有大量块数且该问题与小文件无关，则应添加更多 DataNode。

   **思路：**确认hdfs集群中是否确实存在大量小文件，根据实际需要对小文件进行合并，对于历史数据及时清理归档。

   **获取fsimage信息:**`hdfs dfsadmin -fetchImage  /opt/data`
   **格式化fsimage为可读文本:**`hdfs oiv -i /data/fsimage_0000000000930647029 -o /data/fsimage.csv -p Delimited  -delimiter ","`
   **建立存储fsimage的表:**
   ```
   CREATE TABLE `fsimage_info_csv`(
     `path` string, 
     `replication` int, 
     `modificationtime` string, 
     `accesstime` string, 
     `preferredblocksize` bigint, 
     `blockscount` int, 
     `filesize` bigint, 
     `nsquota` string, 
     `dsquota` string, 
     `permission` string, 
     `username` string, 
     `groupname` string)
   ROW FORMAT SERDE 
     'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
   WITH SERDEPROPERTIES ( 
     'field.delim'=',', 
     'serialization.format'=',') 
   STORED AS INPUTFORMAT 
     'org.apache.hadoop.mapred.TextInputFormat' 
   OUTPUTFORMAT 
     'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
   LOCATION
       'hdfs://nameservice1/user/hive/warehouse/fsimage_info_csv';
   ```
   **加载数据到hive表:**`hdfs dfs -put /data/fsimage.csv /user/hive/warehouse/fsimage_info_csv/`
   **查看文件大小分布:**`hdfs oiv -p FileDistribution  -i fsimage_0000000000930647029 -o fs_distribution`
```
$ cat fs_distribution 
Processed 0 inodes.
Processed 1048576 inodes.
Processed 2097152 inodes.
Processed 3145728 inodes.
Size    NumFiles
0       209746
2097152 2360944
4194304 184952
6291456 121774
8388608 37136
// 省略中间部分
10485760        36906
12582912        51616
14680064        19209
16777216        14617
18874368        7655
20971520        5625
23068672        26746
25165824        112429
27262976        10304
29360128        12315
31457280        11966
33554432        15739
35651584        10180
115425148928    1
totalFiles = 3472422
totalDirectories = 224875
totalBlocks = 3401315
totalSpace = 122170845300822
maxFileSize = 115423398874
```

**逐级目录统计文件数量:**
```
SELECT
    dir_path ,
    COUNT(*) AS small_file_num 
FROM
    (    SELECT
            relative_size,
            dir_path 
        FROM
            (    SELECT
                    (
                    CASE filesize < 4194304 
                        WHEN TRUE 
                        THEN 'small' 
                        ELSE 'large' 
                    END)  AS relative_size,
                    concat('/',split(PATH,'\/')[1], '/',split(PATH,'\/')[2], '/'
                    ,split(PATH,'\/')[3], '/',split(PATH,'\/')[4], '/', split(
                    PATH,'\/')[5]) AS dir_path 
                FROM
                    DEFAULT.fsimage_info_csv 
                WHERE
                    permission LIKE 'd%') t1
        WHERE
            relative_size='small') t2 
GROUP BY
    dir_path 
ORDER BY
    small_file_num
```
**相应数据脱敏后输出如下：**
```
/data/load/201905032130      1
//省略中间部分
/user/hive/warehouse/teset.db/table1  2244
/user/hive/warehouse/teset.db/table2  2244
/user/hive/warehouse/teset.db/table3  2244
/user/hive/warehouse/teset.db/table4  2246
/user/hive/warehouse/teset.db/table5  2246
/user/hive/warehouse/teset.db/table6  2248
/user/hive/warehouse/teset.db/table7  2508
/user/hive/warehouse/teset.db/table8  3427
Time taken: 53.929 seconds, Fetched: 32947 row(s)
```
**小文件处理:**
- 根据涉及目录，反向找到涉及程序，尝试优化避免小文件的产生
- 及时合并归档小文件
- 及时清理历史小文件

https://zhuanlan.zhihu.com/p/269530943

<br>
# 二、Yarn调优
![image.png](调优.assets\9e9c9f43cfab42dfbd58f5b6a8cfc73d.png)

## 2.1 ResourceManager相关
- yarn.resourcemanager.scheduler.class    配置调度器，默认容量调度器
- yarn.resourcemanager.scheduler.client.thread-count   ResourceManager处理调度器请求的线程数量，默认50。设置为节点数*节点线程数的值。

#### 容量调度器多队列创建
**创建思路**
- 按照框架创建：hive/spark/flink 每个框架的任务放入指定队列（企业用的不多）
- 按照业务模块创建：登录注册、购物车、下单、ETL等。

**多队列好处**
- 避免某一个任务把所有资源耗尽
- 实现任务降级使用，特殊时期保证重要任务资源充足

**capacity-scheduler.xml调度器配置文件调优**
默认是容量调度器，大厂中服务器资源多可以使用公平调度器，中小厂使用容量调度器。
1） `yarn.scheduler.capacity.maximum-am-resource-percent=0.5`  集群中可用于运行application master的资源比例上限，默认值为0.1，表示application master占用的队列空间大于10%，就会堵塞，直到appmaster占用小于10%，这通常用于限制并发运行的应用程序数目。当提交多个任务 有一部分任务处于accept状态下的时候 看一下 是不是AM 内存已经达到上限了。
2） `yarn.scheduler.capacity.root.queues=default`  默认队列是default，可以在添加其他队列，如default,hive,flink。
`yarn.scheduler.capacity.root.hive.state=RUNNING`  使定义的hive队列生效
3） `yarn.scheduler.capacity.root.hive.capacity=60` 设置hive队列的资源分配比例为60；
`yarn.scheduler.capacity.root.hive.maximum-capacity=80`  设置hive队列的最大容量为80；
容量调度器的队列资源分配是弹性的，设置为在60-80之间。
4） `yarn.scheduler.capacity.root.hive.acl_submit_applications=*`   访问控制，控制谁可以将任务提交到该队列，*表示所有人。
`yarn.scheduler.capacity.root.hive.acl_administer_queue=*`   访问控制，控制谁可以管理（包括提交和取消）该队列的任务，*表示任何人。

**启动任务时指定队列**
hadoop提交：hadoop jar xxx.jar pi -Dmapreduce.job.queuename=hive 1 1;
sqoop中提交：sqoop import -Dmapreduce.job.queuename=hive --connect xxx ...;
hive中提交：set mapreduce.job.ququqname=hive;
spark中提交：spark-submit --queue hive xxx;
Flink on session中提交：yarn-session.sh -s 2 -jm 1024 -tm 2048 -nm flink-on-yarn -qu flink -d

## 2.2 NodeManager相关
- `yarn.nodemanager.resource.detect-hardware-capabilities`   
是否让yarn自己检测硬件进行配置，默认false；
- `yarn.nodemanager.resource.count-logical-processors-as-cores`
是否将虚拟核数当作CPU核数，默认false ，可以改为true，并在如下配置中设置虚拟核数和物理核数乘数；
- `yarn.nodemanager.resource.pcores-vcores-multiplier `
虚拟核数和物理核数乘数，例如：4核8线程，该参数就应设为2，默认是1；
- `yarn.nodemanager.resource.memory-mb` 
表示该节点上YARN可使用的物理内存总量，默认是8G。即是节点资源充足如内存有128G，但是跑大于8G内存的任务就会失败；所有yarn节点上的`yarn.nodemanager.resource.memory-mb`之和，就是Yarn UI中的Memory Total的值。
- `yarn.nodemanager.resource.cpu-vcores`
表示resourcemanager可以分配给容器的核数，默认-1表示可以分配节点最大线程数；所有yarn节点上的`yarn.nodemanager.resource.cpu-vcores`之和，就是Yarn UI中的VCores Total的值。
- `yarn.nodemanager.pmem-check-enabled`
是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。
- `yarn.nodemanager.vmem-check-enabled`
是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。可以设置为false，不检测虚拟内存；
- `yarn.nodemanager.vmem-pmem-ratio`
虚拟内存和物理内存的比值，默认是2.1，即2比1。

## 2.2 Container容器相关
- `yarn.scheduler.minimum-allocation-mb`
容器最小内存(单个任务可申请的最少物理内存量，默认是1024（MB），如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数)。Yarn UI上的Minimum Allocation的值就是我们配置的值。
- `yarn.scheduler.maximum-allocation-mb`
单个容器最大内存(单个任务可申请的最多物理内存量，默认是8192)。不能超过nodemanager最大内存。Yarn UI上的Maximum Allocation的值就是我们配置的值。
- `yarn.scheduler.minimum-allocation-vcores`
容器最小CPU核数，默认1个。Yarn UI上的Minimum Allocation的值就是我们配置的值。
- `yarn.scheduler.maximum-allocation-vcores`
容器最大CPU核数，默认4个。Yarn UI上的Maximum Allocation的值就是我们配置的值。


**官网yarn所有参数的网址：**[https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)


<br>
## Yarn UI说明

 ![image.png](调优.assets