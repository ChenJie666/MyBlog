---
title: 运维
categories:
- 大数据建模
---
# 一、启动命令
## 1.1 正式
### 启动 hdfs
`sudo -i -u hxr start-dfs.sh`

### 启动 yarn
`sudo -i -u hxr start-yarn.sh`

### 启动timelineserver和jobhistoryserver
`sudo -i -u hxr yarn --daemon start/stop timelineserver`
`sudo -i -u hxr mapred --daemon start/stop historyserver`

### 启动 zookeeper
`sudo -i -u hxr bash -c "cd /opt/module/zookeeper-3.4.10/;bin/zkServer.sh start"`

### 启动 kafka
`sudo -i -u hxr bash -c "cd /opt/module/kafka-2.11/;bin/kafka-server-start.sh -daemon config/server.properties"` 
`sudo -i -u hxr bash -c "cd /opt/module/kafka-2.11/;bin/kafka-server-stop.sh -daemon config/server.properties"`

### 启动 hiveserver2和metastore
`sudo -i -u hxr bash -c " nohup hiveserver2 1>/opt/module/hive-3.1.2/logs/hiveserver2.log 2>&1 &"`
`sudo -i -u hxr bash -c 'nohup /opt/module/hive-3.1.2/bin/hive --service metastore 1>/opt/module/hive-3.1.2/logs/metastore.log 2>&1 &'`


### 启动 flume
**edb_roder(179)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/edb_order_kafka2hdfs.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=edb_order_kafka2hdfs.log -Xms128m -Xmx128m -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`
>@Deprecated
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/edb_order_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=edb_order_kafka2hdfs.log  -Xms128m -Xmx128m -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`

**umeng(180)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/umeng_pv_kafka2hdfs.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=umeng_pv_kafka2hdfs.log -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'`
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a3 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/umeng_uv_kafka2hdfs.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=umeng_uv_kafka2hdfs.log -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36003 1>/dev/null 2>&1 &'`
>@Deprecated
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/youmeng_order_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=youmeng_order_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'`
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a3 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/youmeng_active_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=youmeng_active_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36003 1>/dev/null 2>&1 &'`

**feiyan(181)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_file2kafka.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=feiyan_model_file2kafka.log -Xms256m -Xmx256m -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_kafka2hdfs.job -Dflume.root.logger=INFO,LOGFILE -Dflume.log.file=feiyan_model_kafka2hdfs.log -Xms512m -Xmx512m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'`


### 启动 Azkaban(exec/web)
`bin/start-web.sh`
`bin/shutdown-web.sh`
`bin/start-exec.sh`
`bin/shutdown-exec.sh`
`curl -G "bigdata1:12321/executor?action=activate"`

### 启动 Ranger(admin/usersync)
`ranger-admin start/stop/restart`
`ranger-usersync start/stop/restart`

### 启动飞燕日志客户端
`nohup java -jar hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.jar  1>/opt/jar/hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.log 2>&1 &`

### 启动Flume监控(181)
`nohup /opt/module/flume_exporter-master/flume_exporter-master --metric-file /opt/module/flume_exporter-master/metrics.yml --config-file=/opt/module/flume_exporter-master/config.yml 1>/opt/module/flume_exporter-master/flume_exporter-master.log 2>&1 &`

## 1.2 测试
### 启动 hdfs
`sudo -i -u hdfs start-dfs.sh`

### 启动 yarn
`sudo -i -u yarn start-yarn.sh`

### 启动timelineserver和jobhistoryserver
`sudo -i -u yarn yarn --daemon start/stop timelineserver`
`sudo -i -u mapred mapred --daemon start/stop historyserver`

### 启动 zookeeper
`sudo -i -u hxr bash -c "cd /opt/module/zookeeper-3.5.7/;bin/zkServer.sh start"`

### 启动 kafka
`sudo -i -u hxr bash -c "cd /opt/module/kafka_2.11/;bin/kafka-server-start.sh -daemon config/server.properties"` 
`sudo -i -u hxr bash -c "cd /opt/module/kafka-2.11/;bin/kafka-server-stop.sh -daemon config/server.properties"`

### 启动 hiveserver2和metastore
`sudo -i -u hive bash -c " nohup hiveserver2 1>/opt/module/hive-3.1.2/logs/hiveserver2.log 2>&1 &"`
`sudo -i -u hive bash -c 'nohup /opt/module/hive-3.1.2/bin/hive --service metastore 1>/opt/module/hive-3.1.2/logs/metastore.log 2>&1 &'`


### 启动 flume
**edb_roder(179)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/edb_order_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=edb_order_kafka2hdfs.log  -Xms128m -Xmx128m -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`

**umeng(180)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/umeng_pv_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=umeng_pv_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'`
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a3 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/umeng_uv_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=umeng_uv_kafka2hdfs.log  -Xms64m -Xmx64m -Dflume.monitoring.type=http -Dflume.monitoring.port=36003 1>/dev/null 2>&1 &'`

**feiyan(181)**
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a1 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_file2kafka.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=feiyan_model_file2kafka.log  -Xms256m -Xmx256m  -Dflume.monitoring.type=http -Dflume.monitoring.port=36001 1>/dev/null 2>&1 &'`
`sudo -i -u hxr bash -c 'nohup /opt/module/flume-1.7.0/bin/flume-ng agent -n a2 -c /opt/module/flume-1.7.0/conf -f /opt/module/flume-1.7.0/job/feiyan_model_kafka2hdfs.job  -Dflume.root.logger=INFO,LOGFILE  -Dflume.log.file=feiyan_model_kafka2hdfs.log  -Xms512m -Xmx512m -Dflume.monitoring.type=http -Dflume.monitoring.port=36002 1>/dev/null 2>&1 &'`


### 启动 Azkaban(exec/web)
`bin/start-web.sh`
`bin/shutdown-web.sh`
`bin/start-exec.sh`
`bin/shutdown-exec.sh`
`curl -G "bigdata1:12321/executor?action=activate"`

### 启动 Ranger(admin/usersync)
`ranger-admin start/stop/restart`
`ranger-usersync start/stop/restart`

### 启动飞燕日志客户端
`nohup java -jar hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.jar  1>/opt/jar/hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.log 2>&1 &`

### 启动Flume监控(181)
`nohup /opt/module/flume_exporter-master/flume_exporter-master --metric-file /opt/module/flume_exporter-master/metrics.yml --config-file=/opt/module/flume_exporter-master/config.yml 1>/opt/module/flume_exporter-master/flume_exporter-master.log 2>&1 &`

<br>
## 二、进程
### 179中存在的进程
NameNode DataNode NodeManager
RunJar(Hiveserver2) RunJar(Hivemetastore)
QuorumPeerMain(Zookeeper) Kafka
Application(Flume edb_order_kafka2hdfs.job)
AzkabanExecutorServer
EmbeddedServer(RangerAdmin)  UnixAuthenticationService(RangerUsersync)
```
27172 DataNode
6565 NodeManager
19302 RunJar
15974 RunJar
17865 AzkabanExecutorServer
14860 QuorumPeerMain
15148 Kafka
21005 RunJar
21901 NameNode
16046 Application
1656 UnixAuthenticationService
12412 Jps
12957 RunJar
1662 EmbeddedServer
```

### 180中存在的进程
DataNode ResourceManager NodeManager ApplicationHistoryServer(timelineserver)
QuorumPeerMain(Zookeeper) Kafka
Application(Flume youmeng_active_kafka2hdfs.job/youmeng_order_kafka2hdfs.job)
AzkabanWebServer AzkabanExecutorServer
```
7329 ResourceManager
21062 NodeManager
5511 Application
25361 Application
7177 -- process information unavailable
2575 QuorumPeerMain
32431 DataNode
4179 ApplicationHistoryServer
2838 Kafka
9591 Jps
5943 AzkabanExecutorServer
6135 AzkabanWebServer
25082 Application
```

### 181中存在的进程
DataNode SecondaryNameNode NodeManager JobHistoryServer
QuorumPeerMain Kafka
Application(Flume feiyan_model_file2kafka.job/feiyan_model_kafka2hdfs.job)
AzkabanExecutorServer
hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.jar(feiyan)
```
[root@cos-bigdata-hadoop-03 kafka-2.11]# jps
9572 -- process information unavailable
24327 DataNode
24458 SecondaryNameNode
24555 NodeManager
10028 Jps
23822 Kafka
6414 DAGAppMaster
23536 QuorumPeerMain
2193 Application
6418 JobHistoryServer
9912 AzkabanExecutorServer
1721 hxr-logclient-1.0-SNAPSHOT-jar-with-dependencies.jar
4895 Application
```

<br>
## 三、其他
### flume拦截器


### 自定义函数
```
create function compass_dwd.getDiff as 'com.iotmars.hive.CompareAndGetDiff' using jar 'hdfs://192.168.101.179:9820/user/hive/jars/Hxr_Compass_udf-2.0-SNAPSHOT.jar';
create function compass_dws.parseErrorCodeAndGetDiff as 'com.iotmars.hive.ParseErrorCodeAndGetDiff' using jar 'hdfs://192.168.101.179:9820/user/hive/jars/Hxr_Compass_udf-2.0-SNAPSHOT.jar';
```
