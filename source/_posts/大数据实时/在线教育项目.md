---
title: 在线教育项目
categories:
- 大数据实时
---
# 数仓部分：

hive中的一些配置：

```scala
1)在idea中支持对hive的操作
spark.enableHiveSupport
2)idea中设置上传任务的用户名
System.setProperty("HADOOP_USER_NAME","atguigu")
或在edit configuration中设置-DHADOOP_USER_NAME=atguigu
3）开启动态分区
spark.sql("set hive.exec.dynamic.partition=true")
spark.sql("set hive.exec.dynamic.partition.mode=nonstrict")
4）调大最大分区个数
spark.sql("set hive.exec.dynamic.partition=true")
spark.sql("set hive.exec.dynamic.partition.mode=nonstrict")
spark.sql("set hive.exec.max.dynamic.partitions=100000")
spark.sql("set hive.exec.max.dynamic.partitions.pernode=100000")
spark.sql("set hive.exec.max.created.files=100000")
5）开启压缩
spark.sql("set mapred.output.compress=true")
spark.sql("set hive.exec.compress.output=true")
6）使用lzo压缩
spark.sql("set io.compression.codec.lzo.class=com.hadoop.compression.lzo.LzoCodec")
spark.sql("set mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec")
7）使用snappy压缩
spark.sql("set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec")
spark.sql("set mapreduce.output.fileoutputformat.compress=true")
spark.sql("set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec")
8）如果在hive中跑，可能会有OOM产生，可以设置每个maptask任务开启的虚拟机的内存大小
set mapred.child.java.opts=-Xmx2048M	//最大内存设置为2048M
```



```scala
通过spark任务从hdfs向hive表中插入数据：

def etlMember(spark: SparkSession): Unit ={
    val df1: DataFrame = spark.read.json("hdfs://hadoop102:9000/user/atguigu/ods/member.log")

    val ds: Dataset[Member] = df1.as[Member]

    val etlDS: Dataset[Member] = ds.map(member => {
      member.fullname = member.fullname.take(1) + "*" * (member.fullname.length - 1)
      member.phone = member.phone.take(3) + "*" * 5 + member.phone.takeRight(3)
      member.password = "*" * member.password.length
      member
    })

    etlDS.toDF().createOrReplaceTempView("dwd_member_view")

    val sqlDf: DataFrame = spark.sql("select * from dwd_member_view")

    sqlDf.coalesce(1).write.mode(SaveMode.Overwrite).insertInto("")

  }
```

	insertInto和saveAsTable区别：insertInto会插入到已有的表中，而saveAsTable会创建表再插入，如果表已存在会报错。

	as[...]算子，JSON.parseObject(... , classOf[...])算子都会根据属性名匹配，而insertInto这是按照顺序进行插入，所以要注意属性的顺序。

	采用insertInto进行插入比用sql语句进行插入的好处：因为存在任务存在分区，所以sql执行会产生许多小文件。而insertInto算子执行前先用coalesce进行合并成一个分区然后再写入，则只会产生一个文件。



## spark API写sql用到的算子：

#### 逻辑算子：

union算子：拼接两个dataSet
groupByKey：spark 2.0新增的算子，返回值为（key，Iterator（样例类对象））KeyValueGroupedDataset
mapGroups：KeyValueGroupedDataset类中有mapGroups算子，功能类似map
mapValues：KeyValueGroupedDataset类中有mapValues算子，功能类似mapValue



时间：from_unixtime(时间戳，格式)  转换为日期
unix_timestamp(字符串，格式)  转换为时间戳



withColumn() 新增一列字段并可以指定值
drop() 删除一列字段
.withColumnRenamed()重命名字段



#### 表的缓存和释放：

cache()或persist()建立缓存
spark.catalog.cacheTable("表名")

unpersist()释放缓存
spark.catalog.uncacheTabel("表名")



broadcast()将df表广播到各个executor



df.printSchema()  可以展示表的结构









cdh中spark默认为公平调度器；apache版本spark默认为容量调度器

sqoop：
优点：基于mapred的多节点工作模式默认开启4个mr，可以根据数据量调整。
缺点：不能导列式存储数据。
datax：
优点：可以导入列式存储数据，并行度在channel中设置
缺点：只能单节点工作，对机器性能消耗大。



spark2提交数据时应该写成spark2-submit



insert ignore 

preSql：truncate table







spark向hive插入数据时，为什么会报OOM？
set mapred.child.java.opts=xxx； 调整每个maptask启动的虚拟机的内存（旧）
mapred.map.child.java.opts和mapred.reduce.child.java.opts(默认值为-Xmx200m) （新）

spark向hive查询和插入数据，使用mapreduce跑的还是spark跑的？
用到了coalesce，所以应该是spark跑的？？？那mapred.child.java.opts是spark任务的虚拟机内存吗？？



```scala
scala中求四舍五入的方法（保留1位小数）：
BigDecimal(jsonObject.getDouble("score")).setScale(1, BigDecimal.RoundingMode.HALF_UP) 



需求4：基于宽表统计各试卷平均耗时、平均分
// TODO 聚合函数需要用到agg、avg、cast、as等算子
    import org.apache.spark.sql.functions._
    val result: DataFrame = spark.sql("select paperviewid,paperviewname,score,spendtime,dt,dn from `dws`.`dws_user_paper_detail`")
      .where(s"dt=${time}").groupBy("paperviewid", "paperviewname", "dt", "dn")
      .agg(avg("score").cast("decimal(4,1)").as("avgscore")
        , avg("spendtime").cast("decimal(10,1)").as("avgspendtime"))
      .select("paperviewid", "paperviewname", "avgscore", "avgspendtime", "dt", "dn")

    result.coalesce(1).write.mode(SaveMode.Overwrite).insertInto("ads.ads_paper_avgtimeandscore")



需求6：按试卷分组统计每份试卷的前三用户详情
// TODO 开窗函数需要用到withColumn、dense_rank、over、Window.partitionBy、orderBy等算子
    import org.apache.spark.sql.functions._
    val result: DataFrame = spark.sql("select userid,paperviewid,paperviewname ,chaptername ,pointname ," +
      "sitecoursename ,coursename ,majorname ,shortname ,papername ,score,dt,dn from `dws`.`dws_user_paper_detail`")
      .where(s"dt=${time}")
      .withColumn("rk", dense_rank().over(Window.partitionBy("dn", "paperviewid").orderBy(desc("score"))))
      .where("rk<=3").select("userid", "paperviewid", "paperviewname", "chaptername", "pointname",
      "sitecoursename", "coursename", "majorname", "shortname", "papername", "score", "rk", "dt", "dn")

    result.coalesce(1).write.mode(SaveMode.Overwrite).insertInto("ads.ads_top3_userdetail")


需求8：统计各试卷各分段的用户id，分段有0-20,20-40,40-60，60-80,80-100
// TODO case-when 需要用到的算子withColumn、when、col、between等
    import org.apache.spark.sql.functions._
    val result: DataFrame = spark.sql("select paperviewid,paperviewname,score,userid,dt,dn from `dws`.`dws_user_paper_detail`")
      .where(s"dt=${time}").withColumn("score_segment",
      when(col("score").between(0, 20), "0-20").
        when(col("score") > 20 && col("score") <= 40, "20-40").
        when(col("score") > 40 && col("score") <= 60, "40-60").
        when(col("score") > 60 && col("score") <= 80, "60-80").
        when(col("score") > 80 && col("score") <= 100, "80-100")
    ).groupBy("paperviewid", "paperviewname", "score_segment", "dt", "dn")
      .agg(concat_ws(",", collect_list(col("userid").cast("string"))).as("userids"))
      .select("paperviewid", "paperviewname", "score_segment", "userids", "dt", "dn")

    result.coalesce(1).write.mode(SaveMode.Overwrite).insertInto("ads.ads_paper_scoresegment_user")

```





## 我的datax数据导入文件和脚本：

知识点：

①可以在文本中设置变量${time}  ,  提交的时候可以指定变量的值bin/datax.py job/education_online/hdfs2mysql.json -p "-Dtime=20190722"

②xargs  -i   echo  {}    	可以通过设置-i，用｛｝指定参数的位置

```json
文件名为hdfs2mysql.json

{
    "job": {
        "setting": {
            "speed": {
                "channel": 3
            }
        },
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "path": "/user/hive/warehouse/ads.db/ads_paper_avgtimeandscore/dt=${dt}/dn=${dn}/*",
                        "defaultFS": "hdfs://hadoop102:9000",
                        "column": [
                               {
                                "index": 0,             //解析hdfs文件
                                "type": "string"
                               },
                               {
                                "index": 1,
                                "type": "string"
                               },
                               {
                                "index": 2,
                                "type": "string"
                               },
                               {
                               "index":3,
                               "type":"string"
                               },
                               {
                               "type":"string",
                               "value":"${dt}"
                               },
                               {
                               "type":"string",
                               "value":"${dn}"
                               }
                        ],
                        "fileType": "text",
                        "encoding": "UTF-8",
                        "fieldDelimiter": "	"
                    }

                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "writeMode": "insert ignore",
                        "username": "root",
                        "password": "abc123",
                        "column": [
                            "paperviewid",
                            "paperviewname",
                            "avgscore",
                            "avgspendtime",
                            "dt",
                            "dn"
                        ],
                        "session": [
                        	"set session sql_mode='ANSI'"
                        ],
                        "preSql": [
                            "delete from test"
                        ],
                        "connection": [
                            {
                                "jdbcUrl": "jdbc:mysql://hadoop102:3306/education_online",
                                "table": [
                                    "ads_paper_avgtimeandscore"
                                ]
                            }
                        ]
                    }
                }
            }
        ]
    }
}
```

脚本文件：

```shell
#!/bin/bash
dt=`date -d '-1 day' +%Y%m%d`

if [ -n "$1" ]
then
    dt=$1
fi


#读取dt分区中的dn分区文件，每个dn文件用datax进行插入。需要给定dt参数，dn参数通过
hadoop dfs -ls /user/hive/warehouse/ads.db/ads_paper_avgtimeandscore/dt=$dt | awk -F = '{print $3}' | xargs -i /opt/module/datax/bin/datax.py /opt/module/datax/job/education_online/hdfs2mysql.json -p "-Ddt=$dt -Ddn={}"
#xargs可以通过-i，用｛｝来指定出入参数的位置
```





## spark参数的调优：

github上有spark的各种范例，可以查找不会写的代码：
<https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples>

structure streaming的例子:
<https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/sql/streaming>



官网的spark sql参数：
<http://spark.apache.org/docs/latest/sql-performance-tuning.html>



调优主要从①并行度②shuffle③缓存  三个方面调优

#### 设置分区数：

一般任务个数即partitions数量为core核数的2-3倍为最佳，根据core数对参数进行调整
spark.sql.shuffle.partitions	200

#### 设置自动广播变量，避免shuffle：

spark.sql.autoBroadcastJoinThreshold	10485760 （10M）

```scala
//导入broadcast类，将小表广播，在join时会进行mapjoin，避免了shuffle过程
import org.apache.spark.sql.functions.broadcast
broadcast(spark.table("src")).join(spark.table("records"), "key").show()
```

![UI_spark_sql](F:/Typora/图片/UI_spark_sql.png)

	如图所示为启动join任务后在UI界面的sql中的dag图，可以看到每张表的数据大小，将小表作为广播表进行广播。还未进行broadcast优化的时候用的是sortMergeJoin，有shuffle过程。



2.1.1版本的spark，可以设置默认的广播join表大小（默认10M），如果小于该值会自动进行广播优化。否则只能自己添加broadcast进行BroadcastJoin。



```scala
//原式join
dwdMember.join(dwdMemberRegtype, Seq("uid", "dn"), "left_outer")
.join(dwdBaseAd, Seq("ad_id", "dn"), "left_outer")
//将表进行广播后join，需要导包
import org.apache.spark.sql.functions.broadcast
dwdMember.join(broadcast(dwdMemberRegtype), Seq("uid", "dn"), "left_outer")
      .join(broadcast(dwdBaseAd), Seq("ad_id", "dn"), "left_outer")
```

![broadhashjoin](F:/Typora/图片/broadhashjoin.png)

	可见其他表广播给大表后，大表直接在map端进行BroadcastHashJoin，没有经过shuffle过程，且因为将任务进行operator chain，所以shuffle任务没有了，现在的任务量会比原任务量少shuffle个任务。



#### 设置缓存：

注意点：
①**rdd的默认缓存级别是MEMORY_ONLY；df的默认缓存级别是MEMORY_AND_DISK。**

②rdd进行缓存时，有action算子，才会在sparkUI上显示缓存的stage。而df不需要action算子也能缓存。

![df_cache](F:/Typora/图片/df_cache.png)

③df和ds默认使用了kryo序列化，不需要再进行注册。也是spark API的优点之一。而使用rdd进行缓存，默认不使用kryo，会比df和ds缓存大很多。kryo缺点是消耗大量cpu性能。





spark.serializer	org.apache.spark.serializer.JavaSerializer   默认为java序列化。
因为kryo序列化需要将序列化的类进行注册，并不是所有的类都支持注册。

```
//注册过程，在spark的配置中设置：
val conf = new SparkConf().setMaster(...).setAppName(...)
conf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer")
conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2]))

//使用时也需要指定缓存级别为MEMORY_AND_DISK_SER等序列化存储等级
```



DataFrame join两种join方式：
df1.join（df2，df1（字段名）===df2（字段名））条件字段可以不同，且不会去重
df1.join（df2，Seq("字段名")）需要两个df中都有该字段，且会去重，只留下一个。



df没有reducebykey算子，只有reduce算子。可以用groupbykey算子分组，在用mapGroups算子或mapValues算子进行聚合。







# 实时部分



yarn application kill  命令可以kill调sparkstreaming任务。

## kafka的exactly once：

我们之前从kafka读取数据是用的低版本，将在zookeeper中维护offset，spark1.3之前用，现在不用：

```
<groupId>org.apache.spark</groupId>
<artifactId>spark-streaming-kafka-0-8_2.11</artifactId>
&lt;!&ndash;            <scope>provided</scope>&ndash;&gt;
<version>2.1.1</version>

```

现在用高版本（包含了kafka-client包），在kafka中维护offset：

```
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-streaming-kafka-0-10_2.11</artifactId>
    <version>${spark.version}</version>	//高版本的spark会自动cache，产生skiped stage
</dependency>

```

```scala
//kafka消费代码

val map = Map[String, Object](
      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> "hadoop102:9092,hadoop103:9092,hadoop104:9092",
      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> "earliest",
      ConsumerConfig.GROUP_ID_CONFIG -> "register_topic6",
      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer],
      ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> (true: java.lang.Boolean)
    )


    val kafkaDS: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](Array("register_topic"), map))

/*
 LocationStrategies.PreferBrokers()  优先分配到存在  kafka broker 的机器上
 LocationStrategies.PreferConsistent() 一致性的方式均匀分配分区任务到所有 executor 上
 LocationStrategies.PreferFixed		数据倾斜时手动指定分区
 */

```

低版本kafka容错需要checkpoint和wal，已过时不用考虑；高版本kafka一致性的官方文档链接如下：
<http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html>



	updateStateByKey算子需要设置checkpoint路径，然后自动将state数据存储到该路径下；因为存储文件个数由分区数决定，每次存储都会产生文件，堆积了大量小文件；所以一般不存储在hdfs上，大数据下也不存储在redis中，而是存储在mysql和hbase中，每次存储都会覆盖上一次的数据。

	reduceByKeyAndWindow算子如果用到checkpoint，那么可以将历史数据累加；如果没有用到checkpoint，那么只对窗口中的数据进行操作。（参数数量不同）

 

```scala
//updateStateByKey算子：
val countDS: DStream[(String, Int)] = objDS.map(obj => (obj.stagename, 1))
      .updateStateByKey((values: Seq[Int], state: Option[Int]) => {
      val currentCount: Int = values.reduceLeft(_ + _)
      val stateCount: Int = state.getOrElse(0)
      Some(currentCount + stateCount)
    }
    )
//reduceByKeyAndWindow算子
    val windowDS: DStream[(String, Int)] = objDS.map(obj => (obj.stagename,1)).reduceByKeyAndWindow((a:Int,b:Int) => a+b,Minutes(1),Seconds(6))


```





可以查看任务的UI界面![UI界面](F:/Typora/图片/UI界面.png)





kafka的exactly once语义的实现
①设置offset自动提交为false，手动维护偏移量
②在读取数据时获取维护的偏移量
③业务处理完成后手动提交偏移量（提交到本地数据库中或kafka中）
	但是如果偏移量提交时出错，也会导致数据的重复消费，但是大数据环境下一般不考虑事务；如果业务和提交偏移量要形成事务，那么业务数据就需要聚合到driver端，再进行事务处理。但是有些数据库不支持事务，如hbase等，mysql等关系型数据库支持事务。



## 生产中的spark文件配置：

```scala
val conf: SparkConf = new SparkConf().setAppName("conversionrate")
	conf.setMaster("local[*]")//TODO 打jar包，以yarn模式运行时需要注释掉
    conf.set("spark.streaming.stopGracefullyOnShutdown", "true") //TODO 设置优雅的关闭,yarn application -kill 时不会丢数据
    conf.set("spark.streaming.kafka.maxRatePerPartition", "10") //TODO 设置每个分区每批次读取的数据个数,和kafka中的分区数相乘为总个数
    conf.set("spark.streaming.backpressure.enable","true")  //TODO 设置背压，如果延迟过高，减少拉取的数据
    //   conf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer") //TODO 设置序列化方式为kryo序列化
    //   conf.registerKryoClasses()  //TODO 可以注册需要kryo序列化的类
    
      val ssc = new StreamingContext(conf, Seconds(5))
//    ssc.checkpoint("")  //设置checkpoint路径，新版本已舍弃
```



## 生产中进行sql操作的流程：

#### ①创建Druid连接池类，获取连接对象

DruidDataSourceFactory.createDataSource(props)

```java
public class DataSourceUtil implements Serializable {

    public static DataSource dataSource;

    /**
     * 创建德鲁伊连接池
     */
    static{
        try {
            Properties props = new Properties();
            props.setProperty("url", ConfigurationManager.getProperty("jdbc.url"));  //TODO 从resources目录下.properties文件中获取url的值
            props.setProperty("username", ConfigurationManager.getProperty("jdbc.user"));//TODO 获取用户名
            props.setProperty("password", ConfigurationManager.getProperty("jdbc.password"));//TODO 获取密码
            props.setProperty("initialSize", "5");//设置初始化大小
            props.setProperty("maxActive", "10");//最大连接
            props.setProperty("mixIdle", "5");//最小连接

            props.setProperty("maxWait", "60000"); //等待时长
            props.setProperty("timeBetweenEvictionRunsMillis", "2000");//配置多久进行一次检测,检测需要关闭的连接 单位毫秒
            props.setProperty("minEvictableIdleTimeMillis", "600000");//配置连接在连接池中最小生存时间 单位毫秒
            props.setProperty("maxEvictableIdleTimeMillis", "900000"); //配置连接在连接池中最大生存时间 单位毫秒
            props.setProperty("validationQuery", "select 1");
            props.setProperty("testWhileIdle", "true");
            props.setProperty("testOnBorrow", "false");
            props.setProperty("testOnReturn", "false");
            props.setProperty("keepAlive", "true");
            props.setProperty("phyMaxUseCount", "100000");

            dataSource = DruidDataSourceFactory.createDataSource(props);
        }catch(Exception e){
            e.printStackTrace();
        }
    }

    /**
     * 从连接池中获取连接
     * @return
     * @throws SQLException
     */
    public static Connection getConnection() throws SQLException {
        return dataSource.getConnection();
    }

    /**
     * 关闭资源
     * @param resultSet
     * @param preparedStatement
     * @param connection
     */
    public static void closeResource(ResultSet resultSet, PreparedStatement preparedStatement, Connection connection){
        closeResultSet(resultSet);
        closePreparedStatement(preparedStatement);
        closeConnection(connection);
    }

    private static void closeConnection(Connection connection) {
        try {
            connection.close();
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }

    private static void closePreparedStatement(PreparedStatement preparedStatement) {
        try {
            preparedStatement.close();
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }

    private static void closeResultSet(ResultSet resultSet) {
        try {
            resultSet.close();
        } catch (SQLException e) {
            e.printStackTrace();
        }
    }
}

//此处还需读取resource目录下.properties文件的属性的方法
InputStream resourceAsStream = ConfigurationManager.class.getClassLoader().getResourceAsStream("druidpool.properties")//获取类加载器，加载resources目录下的指定文件作为输入流
prop.load(resourceAsStream);    //TODO 将输入流中的kv值通过load方法写入到prop中
```

②创建更新和查询的方法类

```scala
object SqlProxy {

  var preparedStatement: PreparedStatement = _
  var resultSet: ResultSet = _

  /**
    * 将connection，sql语句和参数传入，在方法内进行对mysql的操作，返回更新的条数
    *
    * @param conn
    * @param sql
    * @param params
    */
  def executeUpdate(conn: Connection, sql: String, params: Array[Any]): Int = {
    var rtn = 0
    try {
      val prepareStatement: PreparedStatement = conn.prepareStatement(sql)
      if (prepareStatement != null && params.length > 0) {
        for (i <- params.indices) {
          val param = params(i)
          prepareStatement.setObject(i, param)
        }
      }
      rtn = prepareStatement.executeUpdate()
    } catch {
      case e: Exception => e.printStackTrace()
    }
    rtn
  }

  /**
    * 执行查询任务，并提供 回调函数。传入实现特质的类对象，在查询结束时调用实现的回调方法
    *
    * @param conn
    * @param sql
    * @param params
    * @return
    */
  def executeQuery(conn: Connection, sql: String, params: Array[Any], queryCallback: QueryCallback): Unit = {
    try {
      val prepareStatement: PreparedStatement = conn.prepareStatement(sql)
      if (prepareStatement != null && params.length > 0)
        for (i <- params.indices) {
          val param = params(i)
          prepareStatement.setObject(i, params)
        }
      resultSet = prepareStatement.executeQuery()
      queryCallback.process(resultSet)
    } catch {
      case e: Exception => e.printStackTrace()
    }
  }

  /**
    * 关闭资源
    * @param conn
    * @param resultSet
    * @param preparedStatement
    */
  def shutdown(conn: Connection, resultSet: ResultSet, preparedStatement: PreparedStatement): Unit = {
    DataSourceUtil.closeResource(resultSet,preparedStatement,conn)
  }

}

/**
  * 具有回调功能的特质
  */
trait QueryCallback {
  def process(resultSet: ResultSet)
}
```







```scala
//TODO 如果表的primary key或unique key冲突，执行后面的语句；如果不冲突，执行前面的语句
insert  into  qz_point_history(userid,courseid,pointid,questionids,createtime,updatetime)  values(?,?,?,?,?,?)  on  duplicate  key  update  questionids=? , updatetime=?   

//TODO 如果primary key或unique key冲突，则会删除该条数据并插入新数据；如果不冲突，直接插入新数据
replace into `offset_manager` (groupid,topic,`partition`,untilOffset) values(?,?,?,?)

replace into和on duplcate key update都是只有在primary key或者unique key冲突的时候才会执行。如果数据存在，replace into则会将原有数据删除，再进行插入操作，这样就会有一种情况，如果某些字段有默认值，但是replace into语句的字段不完整，则会设置成默认值。而on duplicate key update则是执行update后面的语句。
```







疑问：为什么会报java.lang.AbstractMethodError？？

 val map = new mutable.HashMap[String, LearnModel]()  什么用？

如何控制每秒钟处理100条数据？

	sparkstream读取kafka是每个excutor进行读取吗？ConsumerRecord类不能用java序列化，所以用print算子打印时会序列化传输到driver端，就会报无法序列化异常。所以要先将ConsumerRecord按分区转化为可序列化的对象传输，或在参数中设置序列化方式为kryo，并注册ConsumerRecord类。或按分区打印ConsumerRecord，不涉及序列化传输。



```
ConversionRateStreaming中的reduceByKey后的结果会直接在本地的executor中直接计算吗，还是需要重新分配executor？
```







```
sc.broadcast(Array(1, 2, 3))//广播变量
ssc.sparkContext.addFile(this.getClass.getResource("/ip2region.db").getPath)//广播文件
```





val nf = NumberFormat.getPercentInstance
nf.format(数字) 获得该数字的百分数

DateFormat.ofPattern()



hadoop dfs -text 文件名  |head		查看解码后的文本的前几个
hadoop dfs -cat 				查看未解码的文本



	kafka的offset只是为了容灾而存在，每次开启任务才会读取；而运行时应该时有一个临时指针，标记拉取的位置。



```
this.getClass.getSimpleName //获取本类类名
this.getClass.getResource("/ip2region.db").getPath //获取classpath路径下的文件的全路径名
ConfigurationManager.class.getClassLoader().getResourceAsStream("/course_learn.log")//将classpath文件读成流
```
