---
title: 项目知识笔记
categories:
- 大数据实时
---
系统变量：/etc/profile
用户变量：~/.bash_profile | ~/.bash_login | ~/.profile

login 和 non-login shell
login方式加载顺序：etc/profile -> ~/.bash_profile | ~/.bash_login | ~/.profile(加载时，按照上述顺序进行加载，只要加载到一个文件就停止加载)
non-login方式加载顺序：不会加载 etc/profile，只会加载~/.bashrc



常用端口号：

6379：Redis的端口号

3306：Mysql端口号



9000：NN的端口

50070：NN的web端口

50090：2NN的端口

8088：resourcemanager的web端口

8032：resourcemanager（jobtracker）的服务端口

19888：历史服务器web端口



2181：zookeeper的服务端口

2888和3888：内部通讯端口和选举端口

8485：journalnode默认端口



41414：flume监控端口



9092：kafka监控端口

8086：kafka monitoer的web端口

9000：kafka manager的web端口，默认是9000，与namenode端口冲突，bin/kafka-manager  -Dhttp.port=9090

10000：hive2端口



16010：hbase的web端口号

16000：hbase的master的通讯端口

16020：regionserver的端口号

16030：regionserver的web端口号



11000：oozie的web端口号



8443：azkaban的web端口

8081：azkaban的通讯端口



21000：impala的端口

25010：impala的日志web端口

9083：hive元数据仓库metastore的端口号



8080：kettlemaster节点
8081：kettleslave1节点
8082：kettleslave2节点



9300：elasticsearch官方客户端连接、内部通讯端口
9200：elasticsearch集群、控制台和http访问端口
5601：kibana服务端口



1521：orical端口号
27017：mongodb端口号

8080：tomcat端口

7180：cdm服务端口	 hadoop102:7180cdm集群



hadoop102:50070		namenode
hadoop103:8088		resourcemanager
hadoop104:19888		history
hadoop102:16010		hbase

hadoop102：11000  		oozie
https://hadoop102:8443	azkaban

hadoop102:7180		cdm集群



配置文件位置
/etc/hosts			hosts ： 
/etc/profile 		profile : 
/etc/selinux/config		setenforce(临时更改指令：sudo setenforce 0)
/etc/sysconfig/network-scripts/ifcfg-eth0	网络配置
/etc/udev/rules.d/7-persistent-net.rules 网卡配置
/etc/sysconfig/network	修改主机名
/etc/sudoers		用户权限

/etc/ntp.conf		ntp时间同步的配置文件
/etc/sysconfig/ntpd		设置系统时间与硬件时间同步

/etc/selinux/config		安全系统配置文件disable ，可以用指令sudo setenforce 0使安全防护临时失效（使用ganglia需要将其关闭）



hadoop配置（分布式）：①下载并配置jdk和hadoop路径到/etc/profile②配置ssh（ssh-keygen -t rsa和ssh-copy-id $hostname）③配置8个配置文件并分发（core-site.xml配置NN的url和存储路径，hdfs-site.xml配置副本数和2nn，mapred-site.xml配置1.运行在yarn上2.历史服务器通讯地址3.历史服务器web地址，yarn-site.xml配置1.在mr中使用自带的shuffle 2.resourcemanager节点 3.历史服务器聚合功能）④集群时间同步：1.安装ntp2.在/etc/ntp.conf文件中授权同网段的机器可以查询和同步时间，不使用其他互联网上时间，添加server 127.127.1.0和fudge 127.127.1.0 stratum 10  3.硬件时间与系统时间同步 /etc/sysconfig/ntpd文件SYNC_HWCLOCK=yes



zookeeper配置（分布式）：①zk根目录创建zkData文件夹，新建文件myid写入本机id号②配置zoo.cfg：修改dataDir路径到zkData文件夹，添加server.id=hadoopxxx:2888:3888



flume配置（非分布式）：配置flume-env.sh中export JAVA_HOME=/opt/module/jdk1.8.0_144



kafka配置（分布式）：配置server.properties中的broker编号、删除topic功能、运行日志存放位置、kafka集群地址



sqoop配置（非分布式）：①sqoop-env.sh配置hadoop、hive、hbase和zookeeper的目录，因为sqoop作用是大数据集群的导入导出。②将mysql-connector的jar包导入到lib目录下，因为sqoop需要连接mysql。



mysql安装：①安装mysql服务端并启动服务②安装客户端，用生成的密码登陆然后修改密码，并修改mysql库中user表的信息，flush privileges

hive配置（借由hdfs实现分布式）：①配置hive-env.sh文件，指明hadoop路径和hive配置文件路径。②mysql驱动jar包复制到hive/lib目录下，在hive-site.xml文件中配置mysql。④可以配置表头文件和mysql库的编码⑤hive-log4j.properties中修改log的存放位置。

tez引擎配置：①在hive-env.sh文件中添加tez环境变量配置和依赖包环境变量配置②在hive/conf下创建一个tez-site.xml文件，配置tez③将tez文件夹整个上传到hdfs。④如果因为虚拟内存溢出导致kill调container，可以在yarn.site.xml中添加property禁关闭虚拟内存检查。⑤mapred-site.xml中设置map和reduce任务内存配置。

防止metastore中的数据丢失，可以给mysql设置高可用。



azkaban安装（非分布式）：①在mysql中source azkaban自带的建表脚本，创建azkaban数据库和表②生成密钥库（证书和公私钥）③时间需要同步④配置web中的azkaban.properties，增加azkaban-users.xml中的管理员用户⑤配置executor中的azkaban.properties文件



hbase配置：①hbase-env.sh中配置JAVA_HOME，jdk8可以permsize的配置注释掉，将hbase管理zk设为false②regionservers文件中配置regionserver所在的机器③hbase-site.xml中添加hbase和zookeeper的相关信息

③可以将hadoop的配置文件软连接到hbase的conf中，但是配置了hadoop_home可以省略该步骤。④分发hbase



kylin配置①需要hadoop集群、hive和habase②需要在环境变量中配置HADOOP_HOME、HIVE_HOME和HBASE_HOME,kylin会自动读取环境变量。③启动hdfs、历史服务器、yarn和zookeeper，bin/kylin.sh start启动kylin





1.沉默用户：筛选出只启动了一次的用户，然后判断启动日期是否是一周前！！

3.流失用户：可以查询出每个mid_id的最大登录时间，判断是否是一周前！！

4.连续三周登陆统计三周内所有的登陆者，以周进行group by，计算mid_id人数，count等于三的是连续三周登录的



SKU=Stock Keeping Unit（库存量基本单位）。现在已经被引申为产品统一编号的简称，每种产品均对应有唯一的SKU号。

 SPU(Standard Product Unit)：是商品信息聚合的最小单位，是一组可复用、易检索的标准化信息集合。



实体表：存储实体对象的表。

维度表：字段的解释表

事实表：一条数据对应一个事件，每个事件带有可度量属性。

事务型事实表：数据不可变

周期型事实表：数据可变（如订单状态会改变）



每日全量：每天的数据都导入到数仓中。针对数据量较小，一般实体表和纬度表。

每日增量：导入每天新增的数据。针对数据量大且只有新增数据，一般事务型事实表采用。

每日新增及变化量：包括了新增量和变化量

拉链表：导入每日的变化量。数据第一次导入数仓的时候，会将变化字段放入拉链表，并添加上start和end字段（开链和闭链时间 ）记录变化时间。



主键作用是唯一标志一行

 使用范式的根本目的是：

  1）减少数据冗余，尽量让每个数据只出现一次。

  2）保证数据一致性

三范式：①字段的原子性  ②不存在对主键的部分函数依赖③不存在传递函数依赖



关系模型：完全遵循三范式，没有数据冗余，保证数据一致性，但是大量的join操作消耗性能。

维度模型：部分遵循三范式，数据冗余，但是查询性能高。



纬度建模三种模型：

	星型模型：事实表的纬度表没有纬度表，性能更好

	雪花模型：事实表的纬度表也有纬度表，更靠近三范式，冗余少

	星座模型：多个事实表共用纬度表，多个星型和雪花模型组成的模型



hadoop checknative：检查hadoop支持的压缩格式

sqoop查看mysql中的数据库：bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306 --username root --password abc123



	ods层到dwd层进行建模，从dwd层到dws层进行轻微聚合，得到宽表。ads层进行计算时可以直接从dws层拿数据，减少join。



![1562855754525](../AppData/Roaming/Typora/typora-user-images/1562855754525.png)

	图中的$do_date需要加单引号，因为在sql中所有字符串都需要加引号，$do_date得到的是一个字符串，所以需要加引号。

	![捕获2](捕获2.PNG)![捕获1](捕获1.PNG)

	而对于函数，不需要加引号。如上两图所示。



	建表时tblproperties（“parquet.compression"="snappy"）可以指定表的数据的压缩格式为snappy。snappy不支持切片，所以每个大于128mb的文件不切片，单个maptask执行，不会并行执行。



在hive中创建临时表(with 临时表名 as 表，每张表之间用逗号连接)

	with

	tmp_order as (select ... from ...) ,
	tmp_payment as (select ... from ...),
	...





	问题：ods层进行insert into操作，每次因为运行一个reduce，所以都会新建一个文件。这样会造成大量小文件存在，在maptask的时候，会自动对小文件进行合并；但是namenode会①产生大量索引，造成寻址慢。同时占用namenode的内存空间。②浪费了block的空间 ③每个block与namenode通讯，消耗资源。

	解决：每隔一段时间对该表进行全表查询然后再插入回原表，然后将小文件删除，就只剩一个大文件了。





sqoop导出脚本

#/bin/bash
db='gmall'

export_data(){
/opt/module/sqoop-2.5.0/bin/sqoop export \
--connect "jdbc:mysql://hadoop102:3306/$db?useUnicode=true&characterEncoding=utf-8"
--username root
--password abc123
--table $1 \
--num-mapper 1 \
--export-dir /warehouse/$db/ads/$1 \
--input-fields-terminated-by '	' \
--update-mode allowinsert \
--update-key 'tm_id,category1_id,stat_mn,stat_date' \
--input-null-string '\N'
--input-null-non-string '\N'

}

	sqoop导出参数：--update-mode allowinsert/updateonly    允许更新和插入/只允许更新    --update-key   指定更新的字段。

	hive中的空值就是'\N'，mysql中空值是null,从mysql中导入null的数据如果不替换，就会以字符串null的形式存储在hive中。

	hive中的空数据为'\N'，指定--input-null-string '\\N'    字符串类型的\N的到mysql中转成null。  --input-null-non-string  '\\N'   非字符串类型的\N到mysql中也转成null。

	从mysql中导入到hive中时指定  --null-string '\\N'    --null-non-string  '\\N'





keytool -keystore keystore -list查看生成的密钥库keystore。



	1.azkaban的job文件中可以传参，用${变量名}就可以，在web中可以设置该变量的值；如果不需要该变量，则将该变量值设为空，但是不能不设置，会报错。

	2.调用脚本时，需要在前面加上sh或bash，因为用户不同，没有执行权限。



	mysql中发生增删改查，操作会记录在mysql的日志文件中。通过监控日志文件获取信息。



![丢数据](丢数据.PNG)

	left join on ... 不会丢失左表的数据，但是用where过滤会导致丢数据。所以把where的判断条件放到on里面。这样左表和右表只有id相同且结束时间满足条件的情况下才会join，左表的数据全都保存下来。



下载maven依赖包时出现问题，可以删除*lastupdated.properties文件，重新下载依赖包。



掌握kylin纬度表

掌握kylin的定时调用



mr-jobhistory-daemon.sh start historyserver

curl 是发送http请求的指令。



base 64位加密网上搜加密



hbase是按字典序进行存储，所以kylin查询是可以将？？？ 一般一个regionserver配17G-40G内存



kylin查询的三个方法：通过网页、linux命令和jdbc

kylin进程是runjar



11:20 kylin在hive上构建大宽表？？？



unzip  ... -d 路径



hive中comment乱码需要配置字符集



b+树写入较慢，读取快，通过b+树建立索引；LSM通过顺序写提高了写入速度，但是是多个有序小块，读取较慢。可以通过bloom filter和定期合并来提高读取效率。



如果kafka和zk非正常关闭导致zk中有kafka的非正常数据，可以将 zk根目录下的brokers的ids删除。



	1.Kylin是分布式引擎，提供了**sql查询**接口，多维分析能力，**亚秒内**查询**超大数据集**的表。2.需要进行**预计算**，有layer和inmem两种计算方式，都是基于mapreduce。如果纬度为n，则有2^n-1个cuboid，构成了一个cube。3.可以从hdfs，hive，kafka和rdbms中查询数据，将cube build engine构建后的key Value Data值放在HBase中存储。然后通过REST Server和Query Engine在将sql语句进行翻译后在hbase中查询。**支持多重查询方式**，可以通过web app/rest api（即http请求），BI Tools（可视化工具）、JDBC/ODBC(java和c语言的驱动接口)和zepplin进行sql查询。4.旧版本的Kylin仅支持文件的预计算，所以无法对流数据进行实时处理（新版本支持流式处理，和druid类似），但是亚秒级的查询速度可以提供实时查询。5.一般仅支持所选维度和度量，直接从hbase中查询结果。6.需要依赖zookeeper，默认使用mapreduce计算，也可以用spark引擎计算。

	

	impala也是自己的基于内存的计算引擎。

	1.presto是分布式查询引擎，提供了sql接口，提供秒级查询。2.存在一个coordinator和多个worker，查询任务提交给coordinator，然后分发给worker，每个worker赋值执行任务和处理数据，可以连接不同的数据源（如hive、redis、kafka），**跨数据源联表查询**3.自带计算引擎，**基于内存**计算，加载一部分，计算后再清内存，所以要避免联表查询，可以先聚合成宽表。impala查询性能好，但是数据源单一。4.presto对ORC格式支持较好，impala对parquet支持较好。5.join操作时大表放前面，presto会将大表进行切分到多个worker中，然后broadcast右表到每个worker中进行join；如果两张大表，则采用hashjoin，将较小的表的连接列取哈希值然后通过哈希算法放到bucket中，另一张表通过哈希算法进行外键匹配。所以小表放在左边，减少bucket的内存占用。而hive最好将小表放在左边（hashjoin）；



	1.druid是列式，分布式，**实时**分析引擎（对流数据进行实时分析），提供毫秒级查询2.基于预计算，有自己的存储系统。2.延迟为亚秒级，可以查询为预计算的数据，延迟秒级。3.支持流式数据和批量数据查询。可以从kafka中读取流，进行实时查询。4.自愈，自平衡，易操作。4.数据结果采用bitmap压缩算法。基于LSM进行存储，逻辑上是一张时间有序的大表。物理上按时间范围切割成segment数据块（相当于分区），每个数据块中采用bitmap压缩格式压缩（数据纵向切割）。



druid和hbase都是基于LSM（log struct merge）进行存储的，将每条数据顺序写入，按时间进行排序后，达到一定大小写出到磁盘。然后通过merge每个数据块和bloom filter提高查询效率。







获取redis连接池和客户端（spark项目）：

```scala
def getJedisClient: Jedis = {
    if(jedisPool==null){
//      println("开辟一个连接池")
      val config = PropertiesUtil.load("config.properties")
      val host = config.getProperty("redis.host")
      val port = config.getProperty("redis.port")

      val jedisPoolConfig = new JedisPoolConfig()
      jedisPoolConfig.setMaxTotal(100)  //最大连接数
      jedisPoolConfig.setMaxIdle(20)   //最大空闲
      jedisPoolConfig.setMinIdle(20)     //最小空闲
      jedisPoolConfig.setBlockWhenExhausted(true)  //忙碌时是否等待
      jedisPoolConfig.setMaxWaitMillis(500)//忙碌时等待时长 毫秒
      jedisPoolConfig.setTestOnBorrow(true) //每次获得连接的进行测试

      jedisPool=new JedisPool(jedisPoolConfig,host,port.toInt)
    }
//    println(s"jedisPool.getNumActive = ${jedisPool.getNumActive}")
 //   println("获得一个连接")
    jedisPool.getResource
  }
```

kafka 0.10版本之后的连接方式：

```scala
val kafkaParam = Map(
    "bootstrap.servers" -> broker_list,//用于初始化链接到集群的地址
    "key.deserializer" -> classOf[StringDeserializer],
    "value.deserializer" -> classOf[StringDeserializer],
    //用于标识这个消费者属于哪个消费团体
    "group.id" -> "gmall_consumer_group",
    //如果没有初始化偏移量或者当前的偏移量不存在任何服务器上，可以使用这个配置属性
    //可以使用这个配置，latest自动重置偏移量为最新的偏移量
    "auto.offset.reset" -> "latest",
    //如果是true，则这个消费者的偏移量会在后台自动提交,但是kafka宕机容易丢失数据
    //如果是false，会需要手动维护kafka偏移量
    "enable.auto.commit" -> (true: java.lang.Boolean)
  )

dStream：InputDStream[ConsumerRecord[String,String]] = KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent,ConsumerStrategies.Subscribe[String,String](Array(topic),kafkaParam))
```



日期工具：
org.apache.commons.lang.time	DateUtils.addDays(date,-1)获取昨天的日期



从resources文件夹中读取properties文件

```scala
static {
    try {
      InputStream inputStream = ConfigurationManager.class.getClassLoader()
          .getResourceAsStream("comerce.properties");
      prop.load(inputStream);
    } catch (Exception e) {
      e.printStackTrace();
    }
  }
```



创建druid连接池

```scala
public static DataSource dataSource = null;

    static {
        try {
            Properties props = new Properties();
            props.setProperty("url", ConfigurationManager.getProperty("jdbc.url"));
            props.setProperty("username", ConfigurationManager.getProperty("jdbc.user"));
            props.setProperty("password", ConfigurationManager.getProperty("jdbc.password"));
            props.setProperty("initialSize", "5"); //初始化大小
            props.setProperty("maxActive", "10"); //最大连接
            props.setProperty("minIdle", "5");  //最小连接
            props.setProperty("maxWait", "60000"); //等待时长
            props.setProperty("timeBetweenEvictionRunsMillis", "2000");//配置多久进行一次检测,检测需要关闭的连接 单位毫秒
            props.setProperty("minEvictableIdleTimeMillis", "600000");//配置连接在连接池中最小生存时间 单位毫秒
            props.setProperty("maxEvictableIdleTimeMillis", "900000"); //配置连接在连接池中最大生存时间 单位毫秒
            props.setProperty("validationQuery", "select 1");
            props.setProperty("testWhileIdle", "true");
            props.setProperty("testOnBorrow", "false");
            props.setProperty("testOnReturn", "false");
            props.setProperty("keepAlive", "true");
            props.setProperty("phyMaxUseCount", "100000");
//            props.setProperty("driverClassName", "com.mysql.jdbc.Driver");
            dataSource = DruidDataSourceFactory.createDataSource(props);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
```



从本地文件中读取内容：

```scala
val source: BufferedSource = io.Source.fromFile("D:\MyWork\UserBehaviorAnalyze\HotItemsAnalysis\src\main\resources\UserBehavior.csv")

val lineItr: Iterator[String] = source.getLines()
```





从Kafka中读取数据

```scala
SparkStreaming：
    val inputDS: InputDStream[ConsumerRecord[String, String]] = KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](Array(topic), kafkaParam))


Flink：
val dataStream: DataStream[String] = env.addSource(new FlinkKafkaConsumer[String]("kafka1",new SimpleStringSchema(),prop))
```
