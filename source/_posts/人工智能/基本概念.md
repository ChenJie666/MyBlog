---
title: 基本概念
categories:
- 人工智能
---
![image.png](基本概念.assets637bb3acd1c426ab5fe7c0299f42e59.png)

# 一、概念
## 1.1 模型评估
- 训练集和测试集
   - 训练集：输入到模型中对模型进行训练的数据集合
   - 测试集：模型训练完成后测试训练效果的数据集合
- 损失函数和经验风险
   - 损失函数：损失函数用来衡量模型预测误差的大小 L(Y,f(X))。 0-1损失函数、平方损失函数、绝对损失函数、对数损失函数
   - 经验风险：模型f(X)关于训练数据集的平均损失，记作Remp。这一策略认为，经验损失最小的模型就是最优模型。

<br>
## 1.2 模型选择
- 过拟合和欠拟合
   - 过拟合：把训练数据学习的太彻底，以至于把噪声数据的特征也学习到了，特征集过大，导致模型泛化能力太差，不能很好的识别数据。
   - 欠拟合：模型没有很好地捕捉到数据的特征，特征集过小，导致模型不能很好的拟合数据。本质是对数据的特征学习不够。

   当模型复杂度增大时，训练误差会逐渐减小并趋向于0，而测试误差会先减小，达到最小值后再增大。当模型复杂度过大时，就会发生过拟合，所以模型复杂度应当适当。![image.png](基本概念.assets09ce65a98644ea19725ac194d869823.png)

- 正则化和交叉验证
   - 正则化：结构风险最小化SRM，在ERM基础上为了防止过拟合而提出来的策略。在经验风险上加上表示模型复杂度的正则化项(regularizer)或叫惩罚项。正则化项一般是模型复杂度的单调增函数，即模型越复杂，正则化值越大。
   - 交叉验证：样本数据不足时，可以重复利用数据交叉验证。简单交叉验证、S折交叉验证、留一交叉验证。

<br>
## 1.3 二类分类问题
TP：预测为正类且预测正确
TN：预测为负类切预测正确
FP：预测为正类且预测错误
FN：预测为负类且预测错误

精准率(precision)：所有预测为正类的数据中，预测正确的比例。 P=TP/(TP+FP)
召回率(recall)：所有实际为正类，被正确预测的比例。R=TP/(TP+FN)

F1：为了让一个评价指标中，既能体现查准率又能体现召回率。F1=2*精准率*召回率/(精准率+召回率)

<br>
## 1.4 回归问题
- 分类
   - 按照输入变量个数：一元回归和多元回归
   - 按照模型类型：线性回归好非线性回归
- 回归学习的损失函数：平方损失函数

- 模型求解算法
   - 最小二乘法：令偏导数为0求出最低点
   - 梯度下降算法：损失函数是系数的函数，如果系数沿着损失函数的负梯度方向变化，此时损失函数减少最快，能够以最快速度下降到极小值。![image.png](基本概念.assets\46564b5804d54d09be2c8b538c72dee5.png)
![其中i为样本号，j为参数w的下标](基本概念.assets5f011dcd4af4fe28b67a6e14f788146.png)

梯度下降不一定能够找到全局最优解，有可能是一个局部最优解，如果损失函数是凸函数，梯度下降法得到的解一定是全局最优解。[【深度学习概念理解】深度学习中的epochs，batch_size，iterations，learning rate，momentum理解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/437894581)


   - 牛顿法和拟牛顿法![image.png](基本概念.assets\19f44cf6474449ddad7d7ab94e14d671.png)

<br>
**梯度下降法**
求解步骤
1. 定义代价函数
2. 选择起始点
3. 计算梯度
4. 按学习率前进
5. 重复步骤3和4，找到最低点


**激活函数**
神经网络想实现万能逼近(通用近似定理)，需要引入非线性的激活函数，如果没有激活函数，再多的神经元也不会增加模型的复杂性，只会是一个超平面。即神经网络的复杂性主要来源于激活函数。
深度神经网络中，每一层的神经元个数可以理解为对样本数据的升维操作，在更高维度对数据进行区分。

常用的运算操作



<br>
# 二、学习模型
- 监督学习
   - 回归模型
      - 线性回归

   - 分类模型
      - k近邻(kNN)
      - 决策树
      - 逻辑斯蒂回归

- 无监督学习
   - 聚类
      - k均值(k-means)
   - 降维
