---
title: 遇到的问题
categories:
- 权限认证
---
### 问题一
- 现象：服务端192.168.101.174使用kinit admin进行认证，可以成功；但是三个客户端192.168.101.179/180/181都会报错如下：
   ```
   kinit: Cannot contact any KDC for realm 'IOTMARS.COM' while getting initial credentials
   ```
- 原因：KDC所在的节点开启了防火墙，导致88端口无法被访问
- 解决：最简单的就是关闭防火墙。

<br>
### 问题二
- 现象：Hadoop整合Kerberos，启动hdfs。访问Namenode Web，发现没有任何Datanode注册到Namenode。随后查看Namenode日志报错如下。
```
2021-09-29 10:57:48,414 WARN org.apache.hadoop.ipc.Client: Couldn't setup connection for dn/bigdata1@IOTMARS.COM to bigdata1/192.168.101.179:9820
org.apache.hadoop.ipc.RemoteException(javax.security.sasl.SaslException): GSS initiate failed
```
- 原因：Datanode无法连接到Namenode的端口。我用的是jdk1.8.0_144，查看相关资料发现是因为jce的问题。jce版本问题，需要网上下载并进行替换。
- 解决：
1、jce下载地址：https://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html
2、将解压得到的local_policy.jar和US_export_policy.jar拷贝到$JAVA_HOME/jre/lib/security目录下面
3、重启HDFS服务
启动NodeManager时同样会遇到注册失败的问题，解决方法相同。

<br>
### 问题三
- 现象：启动namenode正常，启动datanode时报错无法注册到namenode
```
org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server:cos-bigdata-test-hadoop-01/192.168.101.184:9820
```
- 原因：执行命令`netstat -nap | grep 9820`，发现本机监控的地址为**回环地址127.0.0.1**，而不是192.168.101.184，导致其他节点无法进行通讯。
- 解决：修改host文件，在第一行添加  `[本机ip]  [hostname]`
```
192.168.101.186   cos-bigdata-test-hadoop-03
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.101.184 cos-bigdata-test-hadoop-01 bigdata1
192.168.101.185 cos-bigdata-test-hadoop-02 bigdata2
192.168.101.186 cos-bigdata-test-hadoop-03 bigdata3
```
如果进程启动正常，但是一直无法注册到namenode或resourcemanager，查看端口发现状态是time_wait，但是查看日志没有报错。这种情况有可能是hosts文件配置出现了问题。


<br>
### 问题四
- 现象：hadoop配置kerberos后，无法启动namenode，datanode等进程。
- 原因：查看log文件后发现是配置的**_HOST**通配符解析有误。
- 解决：_Host文件不是读取的本机的hostname，不同的进程有不同的规则，如下
1. 启动Datanode时，会调用KerberosUtil.java中的getLocalHostName()方法
   ```
     /* Return fqdn of the current host */
     static String getLocalHostName() throws UnknownHostException {
       return InetAddress.getLocalHost().getCanonicalHostName();
     }
   ```
   可以发现使用的是java.net包中的getCanonicalHostName()方法获取全限定主机名。需要在/etc/hosts中的第一行添加  
   ```
   [本机ip] [hostname]
   ```
   各个节点启动时会读取这个hostname并赋值给_HOST。

2. 启动SecondaryNameNode时，会读取配置中的值
   ```
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>cos-bigdata-test-hadoop-03:9868</value>
    </property>
   ```
   启动SecondaryNameNode就会将_HOST设置为cos-bigdata-test-hadoop-03。

3. 启动NameNode时，会读取配置中的值
   ```
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://cos-bigdata-test-hadoop-01:9820</value>
    </property>
   ```
   启动NameNode就会将_HOST设置为cos-bigdata-test-hadoop-01。

4. 启动ResouceManager时，会读取配置中的值
   ```
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>cos-bigdata-test-hadoop-02</value>
    </property>
   ```
   启动ResouceManager就会将_HOST设置为cos-bigdata-test-hadoop-02。

5. 启动jobhistoryserver时，会读取配置中的值
   ```
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>cos-bigdata-test-hadoop-03:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>cos-bigdata-test-hadoop-03:19888</value>
    </property>
   ```
   启动ResouceManager就会将_HOST设置为cos-bigdata-test-hadoop-03。

6. 启动hiveserver2时


<br>
### 问题五
- 现象：本机使用火狐浏览器访问需要Kerberos认证的Namenode Web，出现错误
   ```
   2021-10-15 09:57:18,297 WARN org.apache.hadoop.security.authentication.server.AuthenticationFilter: Authentication exceon: No valid credentials provided (Mechanism level: Failed to find any Kerberos credentails)
   ```
- 原因：通过页面查询hadoop目录时，会先验证携带的凭证是否有效，然后向客户端的凭证中写入新的凭证名称，新的凭证名称为 `HTTP/节点名@REALM`，该主机名需要与在hdfs-site中配置的HTTP凭证相同。如果出现节点名不一致的情况，就无法获取该凭证，也就不能访问Web。
![image.png](遇到的问题.assets\37794c07343944f494282ccf9eb05322.png)

- 解决：修改hosts文件，将访问的页面的ip对应的节点名称改为与配置的主体相同的名称。
```
192.168.101.184	cos-bigdata-test-hadoop-01 bigdata1
192.168.101.185	cos-bigdata-test-hadoop-02 bigdata2
192.168.101.186	cos-bigdata-test-hadoop-03 bigdata3
```
然后清除火狐浏览器的Cookie和网站数据后，重新访问Web。



<br>
### 问题六
- 现象：使用命令`hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 1 1`执行任务时出错，报错如下
```
[2021-10-14 17:16:43.462]Application application_1634202938998_0001 initialization failed (exitCode=255) with output: main : command provided 0
main : run as user is hxr
main : requested yarn user is hxr
User hxr not found

2021-10-14 17:16:55,615 INFO mapreduce.Job:  map 100% reduce 100%
2021-10-14 17:16:56,633 INFO mapreduce.Job: Job job_1634202938998_0001 failed with state FAILED due to: Task failed task_1634202938998_0001_m_000000
Job failed as tasks failed. failedMaps:1 failedReduces:0 killedMaps:0 killedReduces: 0

2021-10-14 17:16:56,724 INFO mapreduce.Job: Counters: 13
	Job Counters 
		Failed map tasks=4
		Killed reduce tasks=1
		Launched map tasks=4
		Other local map tasks=3
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=8940
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=4470
		Total vcore-milliseconds taken by all map tasks=4470
		Total megabyte-milliseconds taken by all map tasks=4577280
	Map-Reduce Framework
		CPU time spent (ms)=0
		Physical memory (bytes) snapshot=0
		Virtual memory (bytes) snapshot=0
Job job_1634202938998_0001 failed!
```
- 原因：原因可能如下
1. kerberos认证的用户会通过core-site.xml中匹配的规则映射到主机上的用户，主机上的用户必须存在。我这里出现问题的原因就是只有提交的节点上有该用户，但是分布式执行的其他节点上没有该用户。
2. 该用户的id号小于1000或在container-executor.cfg中ban了该用户。

- 解决：
1. 在其他节点添加该用户；
2. 修改container-executor.cfg配置。

<br>
### 问题七
- 现象：启动SecondaryNameNode后，查看日志发现报错如下
   ```
   2021-10-15 10:52:08,802 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint
   javax.net.ssl.SSLHandshakeException: Error while authenticating with endpoint: https://cos-bigdata-test-hadoop-01:9871/imagetransfer?getimage=1&txid=4419&storageInfo=-64:2014770126:1634089984720:CID-b3e0e3d0-2f3d-49f2-a41c-5725d56b89b5&bootstrapstandby=false
   ...
   Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
   ```
- 原因：Https配置时，使用的是keytool生成的证书，导致2nn请求https://cos-bigdata-test-hadoop-01:9871时，证书不可信。
   ```
   [root@cos-bigdata-test-hadoop-03 namesecondary]# curl https://bigdata1:9871
   curl: (60) Issuer certificate is invalid.
   More details here: http://curl.haxx.se/docs/sslcerts.html
   
   curl performs SSL certificate verification by default, using a "bundle"
     of Certificate Authority (CA) public keys (CA certs). If the default
     bundle file isn't adequate, you can specify an alternate file
     using the --cacert option.
    If this HTTPS server uses a certificate signed by a CA represented in
     the bundle, the certificate verification probably failed due to a
     problem with the certificate (it might be expired, or the name might
     not match the domain name in the URL).
    If you'd like to turn off curl's verification of the certificate, use
     the -k (or --insecure) option.
   ```
- 解决：将自签证书导入JDK的信任域中
   将证书添加到jdk的信任库中，jdk的信任库文件是cacerts。
   ```
   keytool -import -v -trustcacerts -alias cos-bigdata-test-hadoop-01 -file /etc/security/keytab/cos-bigdata-test-hadoop-01.crt -keystore $JAVA_HOME/jre/lib/security/cacerts -storepass changeit
   ```

<br>
### 问题八
- 现象：配置ranger hive插件后，hiveserver2拉取ranger admin中的配置一直失败，报错如下
```
2021-10-16T14:49:15,936  WARN [Thread-8] client.RangerAdminRESTClient: Error getting policies. secureMode=true, user=hive/cos-bigdata-test-hadoop-01@IOTMARS.COM (auth:KERBEROS), response={"httpStatusCode":401,"statusCode":0}, serviceName=hivetest
```
- 原因：在hive插件的配置文件中，policymgr_external_url属性设置为ip，而不是主机名。
- 解决：将policymgr_external_url设置为主机名 `POLICY_MGR_URL=http://cos-bigdata-test-hadoop-01:6080`

<br>
### 问题九
- 现象：配置ranger hive和hdfs插件后，都报错如下：
```
2021-10-18 09:54:07,514 ERROR org.apache.ranger.plugin.util.PolicyRefresher: PolicyRefresher(serviceName=hdfstest): failed to refresh policies. Will continue to use last known version of policies (-1)
com.sun.jersey.api.client.ClientHandlerException: java.lang.RuntimeException: java.lang.NullPointerException
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:155)
	at com.sun.jersey.api.client.Client.handle(Client.java:652)
	at com.sun.jersey.api.client.WebResource.handle(WebResource.java:682)
	at com.sun.jersey.api.client.WebResource.access$200(WebResource.java:74)
	at com.sun.jersey.api.client.WebResource$Builder.get(WebResource.java:509)
	at org.apache.ranger.admin.client.RangerAdminRESTClient$3.run(RangerAdminRESTClient.java:134)
	at org.apache.ranger.admin.client.RangerAdminRESTClient$3.run(RangerAdminRESTClient.java:126)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:360)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709)
	at org.apache.ranger.admin.client.RangerAdminRESTClient.getServicePoliciesIfUpdated(RangerAdminRESTClient.java:137)
	at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicyfromPolicyAdmin(PolicyRefresher.java:251)
	at org.apache.ranger.plugin.util.PolicyRefresher.loadPolicy(PolicyRefresher.java:191)
	at org.apache.ranger.plugin.util.PolicyRefresher.run(PolicyRefresher.java:161)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1488)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474)
	at sun.net.www.protocol.http.HttpURLConnection.getHeaderField(HttpURLConnection.java:3018)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:489)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler._invoke(URLConnectionClientHandler.java:253)
	at com.sun.jersey.client.urlconnection.URLConnectionClientHandler.handle(URLConnectionClientHandler.java:153)
	... 13 more
Caused by: java.lang.NullPointerException
	at java.util.Base64$Encoder.encode(Base64.java:261)
	at java.util.Base64$Encoder.encodeToString(Base64.java:315)
	at sun.net.www.protocol.http.NegotiateAuthentication.setHeaders(NegotiateAuthentication.java:208)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1731)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480)
	... 15 more
```
- 原因：
- 解决：


<br>
### 问题十
- 现象：Hiveserver2配置Kerberos后，启动报错
```
2021-09-30T09:50:25,230 ERROR [main] server.HiveServer2: Error starting priviledge synchonizer: 
java.lang.NullPointerException: null
	at org.apache.hive.service.server.HiveServer2.startPrivilegeSynchonizer(HiveServer2.java:985) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:726) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1037) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.3.jar:?]

2021-09-30T09:51:25,453  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 3, will retry in 60000ms
org.apache.hive.service.ServiceException: java.lang.NullPointerException
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:729) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1037) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.3.jar:?]
Caused by: java.lang.NullPointerException
	at org.apache.hive.service.server.HiveServer2.startPrivilegeSynchonizer(HiveServer2.java:985) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:726) ~[hive-service-3.1.2.jar:3.1.2]
	... 10 more
```
- 原因：
- 解决：


<br>
### 问题十一
- 问题：执行`./enable-hive-plugin.sh`启动ranger hive插件时，报错如下
```
Unable to store password in non-plain text format. Error: [SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/htrace/core/Tracer$Builder
	at org.apache.hadoop.fs.FsTracer.get(FsTracer.java:42)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3407)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:158)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3474)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3442)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.initFileSystem(JavaKeyStoreProvider.java:89)
	at org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider.<init>(AbstractJavaKeyStoreProvider.java:85)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:49)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:41)
	at org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory.createProvider(JavaKeyStoreProvider.java:100)
	at org.apache.hadoop.security.alias.CredentialProviderFactory.getProviders(CredentialProviderFactory.java:73)
	at org.apache.ranger.credentialapi.CredentialReader.getDecryptedString(CredentialReader.java:74)
	at org.apache.ranger.credentialapi.buildks.createCredential(buildks.java:87)
	at org.apache.ranger.credentialapi.buildks.main(buildks.java:41)
Caused by: java.lang.ClassNotFoundException: org.apache.htrace.core.Tracer$Builder
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 16 more]
Exiting plugin installation
```
- 原因：找不到htrace-core4.jar包中的类，原因是${HADOOP_HOME}/share/hadoop/common/lib中的htrace-core4-4.1.0-incubating.jar版本过低。
- 解决：下载htrace-core4-4.2.0-incubating.jar替换htrace-core4-4.1.0-incubating.jar包。


### 问题十二
- 问题：Hadoop集群配置Kerberos并配置Ranger hive插件后，启动hiveserver2获取不到在Ranger admin中配置的策略，报错如下
```
2021-10-20T17:15:02,245  WARN [PolicyRefresher(serviceName=hivetest)-33] client.RangerAdminRESTClient: Error getting Roles. secureMode=true, user=hive/cos-bigdata-test-hadoop-01@IOTMARS.COM (auth:KERBEROS), response={"httpStatusCode":401,"statusCode":401,"msgDesc":"Authentication Failed"}, serviceName=hivetest
2021-10-20T17:15:02,250  WARN [PolicyRefresher(serviceName=hivetest)-33] client.RangerAdminRESTClient: Error getting policies. secureMode=true, user=hive/cos-bigdata-test-hadoop-01@IOTMARS.COM (auth:KERBEROS), response={"httpStatusCode":401,"statusCode":401,"msgDesc":"Authentication Failed"}, serviceName=hivetest
```
- 原因：创建ranger:ranger用户启动的ranger各个进程，但是配置的spnego.service.keytab文件的用户是root:hadoop且权限是660，ranger用户没有操作权限。
- 解决：将ranger用户另外添加到hadoop组 `usermod -a -G hadoop ranger`


### 问题十三
- 问题：hive配置了Kerberos和Ranger hive插件后，启动hiveserver2后查看hive.log日志，报错如下：
```
2021-10-20T23:58:27,406 ERROR [main] server.HiveServer2: Unable to create HiveServer2 namespace: hiveserver2 on ZooKeeper
org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:130) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:740) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[curator-client-2.13.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:454) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[curator-framework-2.12.0.jar:?]
	at org.apache.hive.service.server.HiveServer2.startZookeeperClient(HiveServer2.java:490) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startPrivilegeSynchonizer(HiveServer2.java:1002) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:726) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1037) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.3.jar:?]
2021-10-20T23:58:27,406 ERROR [main] server.HiveServer2: Error starting priviledge synchonizer: 
org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:130) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:740) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[curator-client-2.13.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:454) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[curator-framework-2.12.0.jar:?]
	at org.apache.hive.service.server.HiveServer2.startZookeeperClient(HiveServer2.java:490) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startPrivilegeSynchonizer(HiveServer2.java:1002) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:726) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1037) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.3.jar:?]
2021-10-20T23:58:27,407  INFO [main] server.HiveServer2: Shutting down HiveServer2
2021-10-20T23:58:27,407  INFO [main] service.AbstractService: Service:ThriftBinaryCLIService is stopped.
2021-10-20T23:58:27,407  INFO [main] service.AbstractService: Service:OperationManager is stopped.
2021-10-20T23:58:27,407  INFO [main] service.AbstractService: Service:SessionManager is stopped.
2021-10-20T23:58:27,407  INFO [main] thrift.ThriftCLIService: Thrift server has stopped
2021-10-20T23:58:27,408  INFO [main] service.AbstractService: Service:CLIService is stopped.
2021-10-20T23:58:27,409  INFO [main] metastore.HiveMetaStore: 0: Done cleaning up thread local RawStore
2021-10-20T23:58:27,409  INFO [main] HiveMetaStore.audit: ugi=hive/cos-bigdata-test-hadoop-01@IOTMARS.COip=unknown-ip-addr	cmd=Done cleaning up thread local RawStore	
2021-10-20T23:58:27,409  INFO [main] service.AbstractService: Service:HiveServer2 is stopped.
2021-10-20T23:58:27,409  INFO [main] server.HiveServer2: Web UI has stopped
2021-10-20T23:58:27,409  INFO [main] server.HiveServer2: Stopping/Disconnecting tez sessions.
2021-10-20T23:58:27,409  INFO [main] session.SparkSessionManagerImpl: Closing the session manager.
2021-10-20T23:58:27,410  WARN [main] server.HiveServer2: Error starting HiveServer2 on attempt 8, will retry in 60000ms
org.apache.hive.service.ServiceException: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:729) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:1037) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.access$1600(HiveServer2.java:140) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2$StartOptionExecutor.execute(HiveServer2.java:1305) [hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:1149) [hive-service-3.1.2.jar:3.1.2]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_144]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_144]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_144]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_144]
	at org.apache.hadoop.util.RunJar.run(RunJar.java:318) [hadoop-common-3.1.3.jar:?]
	at org.apache.hadoop.util.RunJar.main(RunJar.java:232) [hadoop-common-3.1.3.jar:?]
Caused by: org.apache.zookeeper.KeeperException$AuthFailedException: KeeperErrorCode = AuthFailed for /hiveserver2
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:130) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:54) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538) ~[zookeeper-3.5.7.jar:3.5.7]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:740) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:723) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109) ~[curator-client-2.13.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:720) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:484) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:474) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:454) ~[curator-framework-2.12.0.jar:?]
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44) ~[curator-framework-2.12.0.jar:?]
	at org.apache.hive.service.server.HiveServer2.startZookeeperClient(HiveServer2.java:490) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.startPrivilegeSynchonizer(HiveServer2.java:1002) ~[hive-service-3.1.2.jar:3.1.2]
	at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:726) ~[hive-service-3.1.2.jar:3.1.2]
	... 10 more
```
错误会导致hiveserver2无法的10000端口关闭导致无法访问。
- 原因：ranger-2.2.0配置了hive插件后，会配置hiveserver2连接zookeeper来实现HA高可用，但是连接zookeeper时会有权限问题。
- 解决：1. 配置不连接zookeeper；2. 解决权限问题。


### 问题十四
- 现象：Kerberos集群中，用户认证为hdfs或chenjie用户，启动hive并执行任务后报错，错误信息如下
```
Failed to execute spark task, with exception 'org.apache.hadoop.hive.ql.metadata.HiveException(Failed to create Spark client for Spark session af52c6c3-0317-4b0f-bac4-63f37b2bfcfe)'
FAILED: Execution Error, return code 30041 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. Failed to create Spark client for Spark session af52c6c3-0317-4b0f-bac4-63f37b2bfcfe
```

- 原因：在配置中，禁止了hdfs用户连接，所以不能使用hdfs用户提交任务。具体看resourcemanager的日志，报错如下
```
Failing this attempt.Diagnostics: [2021-11-22 13:40:52.007]Application application_1637559452985_0002 initialization failed (exitCode=255) with output: main : command provided 0
main : run as user is hdfs
main : requested yarn user is hdfs
Requested user hdfs is banned
```
而chenjie用户在服务器节点中不存在，这样的用户提交任务同样会导致任务失败。
```
 Failing this attempt.Diagnostics: [2021-11-22 13:59:28.807]Application application_1637559452985_0005 initialization failed (exitCode=255) with output: main : command provided 0
main : run as user is chenjie
main : requested yarn user is chenjie
User chenjie not found 
```
>在一个非安全的集群里，Hadoop会把计算用的container分发到集群的所有节点上，并且用已经存在的账号或替代账号例如yarn或mapred用户来启动这些container。但是在一个安全集群里，container必须使用你提交的用户名来分发和启动，所以，这个错误是由于你仅仅添加了KDC里面的chenjie用户这个pricipal，而没有在所有节点建立chenjie账户。所以container的执行器无法启动AM或者Mapper或Reducer。

- 解决：第一个错误只需要在container-executor.cfg配置文件的配置banned.users=hdfs,yarn,mapred中去掉对应的用户即可。第二个错误只需要在所有节点中**新增该用户到hadoop组**即可。
